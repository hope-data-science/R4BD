[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "实战大数据：基于R语言",
    "section": "",
    "text": "前言\n本教程主要介绍如何使用R语言来进行高性能计算，从而应对大数据时代给我们带来的各种挑战。这本书面向的是已经具有一定R语言基础的读者，在面对海量观测构成的数据集时，如何从容地像往常一样对数据进行丰富的分析与建模。为了对R基础较为薄弱的读者也友好，本书不会使用过分深刻晦涩的材料，力求深入浅出。书会结合当前最先进的R语言工具包（包括但不限于data.table、duckdb、arrow、sparklyr、Rcpp、future），系统地介绍以下几个部分的内容：\n\n大数据基本概念\nR语言编程入门\n数据处理效能的衡量\n快速读写：大数据的导入与导出\n快速整理：基于data.table的数据处理工具\n快速绘图：大数据可视化工具\n快速建模：高性能机器学习工具\n化整为零：对文件进行批处理\n跨语言召唤术：在R中调用其他编程工具\n时间换空间：大数据流式计算\n空间换时间：大数据并行计算\n从内存到外存：用数据库管理数据\n从本地到集群：大数据分布式计算\n\n本书所需要用到的R包可以使用以下代码一次性进行安装：\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  tidyfst,microbenchmark,pryr,bench,profvis,rio,scattermore,tidyverse,hexbin,\n  ggridges,GGally,mlr3verse,FSelectorRcpp,praznik,ranger,kknn,fs,archive,openxlsx2,\n  Rcpp,DBI,RSQLite,futureverse,duckdb,dbplyr,arrow,sparklyr\n)\n\ninstall.packages(\"polars\", repos = \"https://community.r-multiverse.org\")\ninstall.packages(\n  'tidypolars', \n  repos = c('https://etiennebacher.r-universe.dev', getOption(\"repos\"))\n)",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "大数据基本概念.html",
    "href": "大数据基本概念.html",
    "title": "1  大数据基本概念",
    "section": "",
    "text": "1.1 大数据的概念\n大数据（Big Data）是指那些因其大小、复杂性或增长速度而难以使用传统数据处理方法有效管理和处理的数据集，其关键特征包括三个，常被简称为“3V”：\n后来随着大数据概念的扩展，人们逐渐为其又补充了其他特性，包括：\n以上特点统称大数据的5V特性（见图1.1）。\nFigure 1.1: 大数据的5V特性",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>大数据基本概念</span>"
    ]
  },
  {
    "objectID": "大数据基本概念.html#大数据的概念",
    "href": "大数据基本概念.html#大数据的概念",
    "title": "1  大数据基本概念",
    "section": "",
    "text": "体量（Volume）：大数据的最显著特征是数据量巨大，通常以TB、PB甚至更大的单位来衡量。\n速度（Velocity）：数据的生成速度非常快，常常需要实时或几乎实时的处理能力，以便快速捕捉、分析和响应。\n多样性（Variety）：大数据包括各种类型的数据，如结构化数据（例如数据库中的表格数据）、半结构化数据（如XML数据）、非结构化数据（如文本、视频和图片）。\n\n\n\n真实性（Veracity）：由于数据来源多样，数据的质量和准确性也各不相同，这要求在处理数据前进行验证和清洗。\n价值（Value）：大数据的价值在于能够从中提取有用的信息和洞察，帮助做出决策。然而，大数据本身并不自动等同于“有价值的数据”，必须通过适当的分析才能发现其中的价值。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>大数据基本概念</span>"
    ]
  },
  {
    "objectID": "大数据基本概念.html#大数据的价值",
    "href": "大数据基本概念.html#大数据的价值",
    "title": "1  大数据基本概念",
    "section": "1.2 大数据的价值",
    "text": "1.2 大数据的价值\n引入大数据的概念是为了应对和利用在现代信息技术快速发展和数字化转型过程中生成的海量数据，具有多种价值（如图1.2）。其关键原因包括：\n\n应对信息爆炸：随着互联网、社交媒体、智能设备等的普及，每天产生的数据量呈指数级增长。传统的数据处理工具和技术已无法有效管理和分析这些海量的数据。大数据技术应运而生，提供了新的方法和工具来存储、处理和分析这些大规模数据集。\n洞察和决策支持：大数据技术能够帮助组织从大量的结构化和非结构化数据中提取有价值的信息和洞察，支持更加准确和高效的决策制定。这在商业竞争、政府管理、公共安全、健康医疗等多个领域尤为重要。\n增强竞争力：在商业环境中，利用大数据技术可以帮助企业更好地了解市场趋势、消费者行为和运营效率，从而优化产品和服务，提高客户满意度，增强企业的市场竞争力。\n推动创新：大数据不仅可以改进现有的产品和服务，还可以激发新的商业模式和创新思路。例如，通过数据挖掘和机器学习技术，企业可以开发出新的推荐系统、智能化服务和个性化解决方案。\n社会效益：大数据技术在社会科学、环境监测、灾害预防和公共健康等领域的应用也显示了其在推动社会进步和提高人类福祉方面的巨大潜力。\n\n总的来说，大数据的提出是为了更好地利用在数字化时代产生的庞大数据资源，提高数据处理的能力和效率，支持科学研究和商业决策，促进经济发展和社会进步。\n\n\n\n\n\n\n\n\nFigure 1.2: 大数据分析的价值",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>大数据基本概念</span>"
    ]
  },
  {
    "objectID": "大数据基本概念.html#大数据的技术",
    "href": "大数据基本概念.html#大数据的技术",
    "title": "1  大数据基本概念",
    "section": "1.3 大数据的技术",
    "text": "1.3 大数据的技术\n大数据技术是指一系列方法、工具和框架，用于有效处理和分析在传统数据库管理工具无法有效处理的大量、高速、多样化的数据集。常用的大数据技术包括大规模并行处理、分布式文件系统、云计算平台、互联网、可扩展的存储系统等（如图1.3，这些技术能够从巨大的数据集中提取有用的洞察和信息，从而支持决策制定和业务优化。以下是几个主要方面：\n\n数据存储：大数据技术提供了专门的存储解决方案，用于存储大规模数据。例如，Hadoop Distributed File System (HDFS) 是为处理大量数据而设计的分布式文件系统，能够存储不同类型的数据，并支持高吞吐量的数据访问。\n数据处理：为了处理和分析大数据，科学家开发了多种计算模型和框架。例如，Apache Hadoop 的 MapReduce 允许对大量数据进行并行处理；而Apache Spark 则提供了一个快速的、通用的、基于内存的数据处理平台，能够执行批处理和实时数据处理任务。\n数据分析：大数据技术还包括复杂的数据分析工具，如R、Python等，这些工具可以处理高速流动的大量数据，支持机器学习、预测分析、数据挖掘和统计分析等高级分析功能。\n实时处理：技术如 Apache Storm 和 Apache Flink 支持对实时数据流进行处理，使得企业可以即时分析和响应数据。这对于需要快速决策的应用场景（如金融市场分析、网络安全和实时广告投放）至关重要。\n数据可视化：大数据技术还涉及数据可视化工具，这些工具可以帮助用户更容易地理解数据分析结果。通过图表、图形和仪表板等形式，数据可视化使复杂数据变得易于接受和理解。\n数据安全与隐私：随着数据量的增加，数据安全和隐私保护也成为大数据技术的重要组成部分。这包括确保数据的完整性、保密性以及遵守相关的法律和规章制度。\n\n大数据技术正成为企业和组织不断寻求的解决方案，以便更好地理解和利用日益增长的数据资源，推动业务成长和创新。\n\n\n\n\n\n\n\n\nFigure 1.3: 大数据技术实例",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>大数据基本概念</span>"
    ]
  },
  {
    "objectID": "大数据基本概念.html#大数据之于你我",
    "href": "大数据基本概念.html#大数据之于你我",
    "title": "1  大数据基本概念",
    "section": "1.4 大数据之于你我",
    "text": "1.4 大数据之于你我\n尽管在上文中我们探讨了很多广义上的大数据概念，但是在实际的学习、工作和生活中，我们所碰到的大数据问题往往是属于狭义的大数据范畴。举一些简单的例子：\n\n在参加建模大赛中，有一份大小为9G的数据集，但是当前你手里只有一个内存（RAM）为8G的个人笔记本电脑。在没有其他计算资源的情况下，如何合理地发挥计算机的性能，让其能够对该数据集进行基本的数据分析？\n你是企业中的业务员，负责对各个部门的劳动绩效进行统计，并生成Excel报表给各部门领导进行参考。各个部门提供的数据都是规整的，如何使用脚本让整个过程自动化完成？\n你在量化金融公司担任建模分析师，需要对不同品类贵金属期货进行回归分析。每一种品类数据量都非常大，如何才能够利用并行框架多快好省地建立模型？\n\n在上述例子中，如果不是因为数据规模的问题，那么我们使用常规工具都可以很快获得结果。但是由于对数据处理的时空限制（在时间上，处理速度需要更快；在空间上，需要能够容纳大数据），导致本来非常普通的数据分析问题变得非常困难。这就是本书要聚焦的问题，即如何基于数据分析基本原理和相关实践工具，来突破大数据分析的时空限制。书中将以R语言作为基本工具，对大数据高效分析和高性能计算进行介绍，让读者对相关的概念和方法进行掌握。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>大数据基本概念</span>"
    ]
  },
  {
    "objectID": "大数据基本概念.html#小结",
    "href": "大数据基本概念.html#小结",
    "title": "1  大数据基本概念",
    "section": "1.5 小结",
    "text": "1.5 小结\n本章主要对大数据的基本概念进行介绍，在宏观上把握大数据是什么、大数据有什么价值、当前大数据都有什么技术，最后从自身角度出发，辨明我们在生活、工作和学习中可能碰到的大数据问题，进而对往后的章节进行学习。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>大数据基本概念</span>"
    ]
  },
  {
    "objectID": "R语言编程入门.html",
    "href": "R语言编程入门.html",
    "title": "2  R语言编程入门",
    "section": "",
    "text": "2.1 基本数据类型\nR语言是一门面向数据的语言，因此用户必须熟悉R体系中的基本数据类型。R中常用数据类型包括数值型、逻辑型、字符型和因子型四种，下面我们会一一进行介绍。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程入门</span>"
    ]
  },
  {
    "objectID": "R语言编程入门.html#基本数据类型",
    "href": "R语言编程入门.html#基本数据类型",
    "title": "2  R语言编程入门",
    "section": "",
    "text": "2.1.1 数值型\n数值型，顾名思义就是数字，如20220124。我们可以用class函数来查看数据的类型。\n\nclass(20220124)\n\n[1] \"numeric\"\n\n\n我们可看到，这是一个数值型。其实数值还有更深层的分类，就是整数型和双精度型，可以用来表示整数和正整数。如果要表示整数，一般在数字后面加入“L”。我们可以用typeof函数来看到细分的数据差别。\n\ntypeof(20220124L) #整数型\n\n[1] \"integer\"\n\ntypeof(2022.0124) #双精度型\n\n[1] \"double\"\n\n\nR中还有一种数据类型叫做复合型，可以表示数学中的虚数，表示方法如下：\n\nclass(1 + 1i)\n\n[1] \"complex\"\n\n\n由于在文本挖掘中不常用，这里不展开介绍这种数据类型。\n\n\n2.1.2 逻辑型\n逻辑型的数据，一般是指非黑即白的两种：真（TRUE）与假（FALSE）。\n\nclass(FALSE)\n\n[1] \"logical\"\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\nTRUE和FALSE都是R中的保留字符，它们还可以分别简写为T和F。\n\nclass(F)\n\n[1] \"logical\"\n\nclass(T)\n\n[1] \"logical\"\n\n\n值得注意的是，R中表示缺失值的保留字NA也是逻辑型数据。\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\n\n2.1.3 字符型\n字符型就是字符串，在文本挖掘中所有文本格式的数据都属于这种类型，如“R语言”、“数据分析”。\n\nclass(\"R语言\")\n\n[1] \"character\"\n\nclass(\"数据分析\")\n\n[1] \"character\"\n\n\n\n\n2.1.4 因子型\n因子型是R中独特的数据结构，它代表了字符与数字的映射关系，可以表示离散型的数据。\n\na = factor(\"Ding\") # 构造一个因子型数据赋值给a\nclass(a)\n\n[1] \"factor\"\n\n\n使用level函数可以看到因子变量的等级。\n\nlevels(a)\n\n[1] \"Ding\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程入门</span>"
    ]
  },
  {
    "objectID": "R语言编程入门.html#常用数据结构",
    "href": "R语言编程入门.html#常用数据结构",
    "title": "2  R语言编程入门",
    "section": "2.2 常用数据结构",
    "text": "2.2 常用数据结构\n在实际应用中，数据总是以一定的形式组织起来，在R中也有对应的数据结构来表达这些组织形式。在本节中，我们将会对R语言中常用的数据结构（向量、矩阵、列表、数据框）进行介绍。\n\n2.2.1 向量\n在上一节中，我们提到了数据类型，包括数值型、字符型、因子型等。这些具有相同数据类型的多个数据单位组合到一起，可以构成一个向量（Vector）。在R中，我们可以利用c函数来构造一个向量。\n\nnum_vec = c(1,3,5,7)\nchar_vec = c(\"A\",\"B\",\"C\")\nlogl_vec = c(T,F,T)\n\nnum_vec\n\n[1] 1 3 5 7\n\nchar_vec\n\n[1] \"A\" \"B\" \"C\"\n\nlogl_vec\n\n[1]  TRUE FALSE  TRUE\n\n\n我们可以用is.vector来判断变量是否是一个向量。\n\nis.vector(num_vec)\n\n[1] TRUE\n\n\n\n\n2.2.2 矩阵\n矩阵（Matrix）的本质是一个二维数组，具有行和列两个维度。在R中，可以使用matrix函数来构造一个矩阵。比如，我们构造一个名为mdat的矩阵，其中行名称为row1和row2，列名称为C.1、C.2和C.3。需要注意的是，行列的名称其实是可以缺省的。\n\nmdat &lt;- matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3, byrow = TRUE,\n               dimnames = list(c(\"row1\", \"row2\"),\n                               c(\"C.1\", \"C.2\", \"C.3\")))\nmdat\n\n     C.1 C.2 C.3\nrow1   1   2   3\nrow2  11  12  13\n\n\n我们可以使用is.matrix函数来判断数据是否是一个矩阵。\n\nis.matrix(mdat)\n\n[1] TRUE\n\n\n\n\n2.2.3 列表\n列表（List）是R中最为灵活的数据结构，它就像一列火车，每个车厢中可以放任意类型的数据。下面我们举个例子：\n\na_list = list(T,1,\"hello\")\na_list\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] \"hello\"\n\n\n使用is.list函数可以判断一个数据是否是一个列表。\n\nis.list(a_list)\n\n[1] TRUE\n\n\n\n\n2.2.4 数据框\n数据框（Data Frame）是R中重要的数据结构，能够表达传统数据库中的二维表结构。它是一种特殊的列表，它每一列是一个向量（具有数据类型同质性），每一行是一个列表（单个样本可以有不同数据类型的属性）。一般而言，数据框一定会有列名称来描述属性，而行名称则可有可无，因为行名称可以新增一列来进行表示。在R中，可以使用data.frame函数来构建一个数据框。\n\ndf = data.frame(a = 1:3, b = 4:6)\ndf\n\n  a b\n1 1 4\n2 2 5\n3 3 6\n\n\n可以使用names函数来获得其列名称。\n\nnames(df)\n\n[1] \"a\" \"b\"\n\n\n如果想要获知它的维度（它有几行几列），可以使用dim函数。\n\ndim(df)\n\n[1] 3 2\n\n\n与之前类似，我们可以用is.data.frame函数来判断一个数据是否是数据框结构。\n\nis.data.frame(df)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程入门</span>"
    ]
  },
  {
    "objectID": "R语言编程入门.html#编程知识介绍",
    "href": "R语言编程入门.html#编程知识介绍",
    "title": "2  R语言编程入门",
    "section": "2.3 编程知识介绍",
    "text": "2.3 编程知识介绍\nR语言与C语言等其他编程语言相似，有自身的一套编程体系。尽管这个体系非常庞杂，但是对于入门者而言只需要掌握其中一些核心的内容就可以完成大部分简单的数据操作和计算。本节将会针对R语言编程的部分核心内容进行简要介绍，从而让初学者快速掌握一些基本概念。\n\n2.3.1 赋值\n在前面的介绍中，我们已经用到了赋值操作。赋值就是把计算好的结果赋予一个变量的过程。在R中，可以使用等号（“=”）或箭头（“&lt;-”和“-&gt;”）来对变量进行赋值。\n\na = 1\n# 等价于\na &lt;- 1\n# 等价于\n1 -&gt; a\n\n尽管在R中可以灵活地使用以上三种方法进行赋值，但是有时候作为项目管理，应该统一编程风格。比如有的规范中建议在所有函数定义的时候用“=”，而在数值保存的时候使用“&lt;-”。而日常使用中，因为编写代码总是有从左到右的习惯，则可以灵活地使用“-&gt;”来进行赋值。 在R中，还可以使用assign函数来为一个变量名进行赋值，也就是说我们上面的复制，还可以这么写：\n\nassign(\"a\",1)\n\n这里，“a”是一个字符，它代表了变量的名称。\n\n\n2.3.2 函数\n函数式编程是R的一大特色，在R中我们无时无刻不在调用函数来实现不同的算法。比如，我们想要求得一个数值型向量的均值，我们可以使用R内置的mean函数。\n\na = c(1,2,3)\nmean(a)\n\n[1] 2\n\n\nsum函数则可以求得数值型向量的总和。\n\nsum(a)\n\n[1] 6\n\n\n在日常工作中，我们常常需要自定义函数来完成特定的任务，比如我们想利用勾股定理来求直角三角形斜边的长度。下面，我们尝试构造一个名为“get_length”的函数来完成这个计算。\n\nget_length = function(a,b){\n  sqrt(a^2 + b^2)  #sqrt是R内置的开方函数\n}\n\nget_length(3,4)\n\n[1] 5\n\n\n\n\n2.3.3 强制类型转换\n我们已经知道R中的基本数据类型，这些数据类型在一定条件下，可以进行相互转化。举一个例子，在R中认为，逻辑型的函数，TRUE是1，FALSE是0，通过强制类型转换，我们能够看到这个关系。\n\nas.numeric(TRUE)\n\n[1] 1\n\nas.numeric(FALSE)\n\n[1] 0\n\n\n这里，我们用as.numeric把逻辑型数据转化为了数值型。在这个步骤中，操作时可逆的，我们可以把数值1和0重新转化为逻辑型数据。\n\nas.logical(1)\n\n[1] TRUE\n\nas.logical(0)\n\n[1] FALSE\n\n\n逻辑型和数值型的数据都可以转化为字符型数据。\n\nas.character(TRUE)\n\n[1] \"TRUE\"\n\nas.character(1990)\n\n[1] \"1990\"\n\n\n在R中，可以使用以“as.”为前缀的函数，对数据进行强制类型转换。\n\n\n2.3.4 条件判断\n在R中经常要使用条件判断来实现分支结构，如果满足某一条件就执行A操作，否则执行B操作，if语句和else语句可以轻松地实现这个过程。\n\na = 3\nif(a &gt; 3) print(\"a大于3\") else print(\"a不大于3\")\n\n[1] \"a不大于3\"\n\n\n我们还可以用ifelse语句直接实现这个结构：\n\nifelse(a &gt; 3,\"a大于3\",\"a不大于3\")\n\n[1] \"a不大于3\"\n\n\n\n\n2.3.5 循环操作\n在批处理过程中，我们往往需要利用循环来对数据做遍历，以计算所有的情况。在R中使用循环非常灵活，比如我们要打印1到10的所有正整数，我们可以利用for循环来编码。\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n如果我们要利用条件判断，那么可以使用while语句来实现上面的操作。\n\ni = 1\nwhile(i &lt;= 10){\n  print(i)\n  i = i+1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n需要注意的是，在上面的操作中，我们给i定义了初始值1，并在每一步运算结束后加上了1来推进遍历操作。 最后，还要介绍repeat语句，它相当于“while(1)”，也就是没有遇到终止操作break，它就会一直运行下去。下面，我们尝试利用repeat语句来实现以上操作。\n\ni = 1\nrepeat{\n  print(i)\n  i = i+1\n  if(i &gt; 10) break #如果i大于10，结束循环\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程入门</span>"
    ]
  },
  {
    "objectID": "R语言编程入门.html#小结",
    "href": "R语言编程入门.html#小结",
    "title": "2  R语言编程入门",
    "section": "2.4 小结",
    "text": "2.4 小结\n本章介绍了R语言中最基础的编程概念。要开展数据分析，就需要了解R语言中最基本的数据类型和数据结构，因此本章首先对此进行了讲解介绍。随后，我们还引入了一些最基础的编程概念，包括赋值、函数、循环等。掌握这些基本概念后，在后面的实践中将会慢慢体会如何利用这些知识来解决各种实际问题。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程入门</span>"
    ]
  },
  {
    "objectID": "数据处理效能的衡量.html",
    "href": "数据处理效能的衡量.html",
    "title": "3  数据处理效能的衡量",
    "section": "",
    "text": "3.1 时间衡量\n任意特定操作、函数或一组代码的执行都需要一定的时间，在保证代码能够正确工作前提下，应该对R代码进行优化，使其执行得更快。在R中，对代码的时间进行衡量非常重要，因为这个步骤可以获知代码中运行缓慢和低效的部分，从而识别瓶颈，然后进行代码优化，进而提高程序的性能。因此，如何在R中对代码进行计时，对开发者来说是一项非常重要的技能。\n在R中，能够实现计时的工具非常多，本书不会穷举所有的方法，而是对最常见而实用的方法进行介绍。这里我们统一使用Sys.sleep来构造测试代码，Sys.sleep是系统里面用于暂停制定时间的函数，可以让脚本运行暂停一定的秒数。首先，如果想要对某一段代码运行的时间进行衡量，可以使用tidyfst包的pst函数：\nlibrary(tidyfst)\n\nThank you for using tidyfst!\n\n\nTo acknowledge our work, please cite the package:\n\n\nHuang et al., (2020). tidyfst: Tidy Verbs for Fast Data Manipulation. Journal of Open Source Software, 5(52), 2388, https://doi.org/10.21105/joss.02388\n\npst({\n  Sys.sleep(0.5) # 暂停0.5秒的时间\n})\n\n[1] \"# Finished in 0.510s elapsed (0.000s cpu)\"\n在上面给出的结果中，首先给出的是实际系统运行时间（Wall-Clock Time），随后在括号内给出的时间是CPU运行时间（CPU Time）。CPU运行时间指的是CPU实际用于处理某个程序的时间。这个时间只计算CPU在执行这个特定程序或进程上的工作时间，不包括系统处理其他任务的时间（如输入/输出操作、磁盘操作或网络通信）的时间。实际系统运行时间是从程序开始到程序结束所经过的总时间，就像是用墙上的时钟来度量时间一样。这个时间包括了所有的等待和延迟时间，比如CPU切换到其他任务、数据从硬盘加载、网络延迟等。因此，这个时间通常会比CPU运行时间长，因为它包括了程序执行中所有可能的等待时间。\n有的时候，我们认为仅仅利用一次运行的计时结果不够稳定，可以增加运行的次数。在这种情况下，使用microbenchmark包的microbenchmark函数。比如我们想要让代码重复5次，可以这样操作：\nlibrary(microbenchmark)\nmicrobenchmark(Sys.sleep(0.1), # 暂停0.1秒的时间\n               times = 5)  # 重复运行5次\n\nUnit: milliseconds\n           expr      min       lq     mean   median       uq     max neval\n Sys.sleep(0.1) 108.5984 108.8506 109.6036 109.2285 109.5447 111.796     5\n得到的结果中，“Unit”部分声明了本次代码执行时间衡量的时间单位（“milliseconds”代表时间单位为微秒），expr代表执行的代码，neval代表代码执行的次数，而mean和median分别代表多次执行中时间的平均值和中位数，min和max则给出了执行时间的最小值和最大值。 此外，microbenchmark函数还可以比较不同代码的运行时间长短，实现方法如下：\nmicrobenchmark(Sys.sleep(0.1), # 暂停0.1秒的时间\n               Sys.sleep(0.2), # 暂停0.2秒的时间\n               times = 5) # 各自重复运行5次\n\nUnit: milliseconds\n           expr      min       lq     mean   median       uq      max neval\n Sys.sleep(0.1) 107.9187 108.5331 108.7826 108.5972 109.3027 109.5615     5\n Sys.sleep(0.2) 201.7077 202.2759 205.4304 204.4330 204.9903 213.7449     5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据处理效能的衡量</span>"
    ]
  },
  {
    "objectID": "数据处理效能的衡量.html#空间衡量",
    "href": "数据处理效能的衡量.html#空间衡量",
    "title": "3  数据处理效能的衡量",
    "section": "3.2 空间衡量",
    "text": "3.2 空间衡量\n一般来说，R将所有对象存储在计算机的物理内存（RAM，Random-access Memory）中进行操作。如果我们的计算机的物理内存不足以处理一些工作，那么就需要使用一些新的方法来对其进行处理。因此，了解计算环境的可用内存限制对我们而言至关重要。在使用R时值得注意的第一件事情是，您的计算机实际上有多少物理内存。通常情况下，我们可以通过查看操作系统的设置来了解这一点。举例来说，如果我们有一个内存为8GB的笔记本电脑，那么R可用的RAM量将远远小于此值，即上限是无法达到8G的。如果我们计划在这台笔记本上读入一个占用16GB内存的对象，那么我们需要换一台内存更大的计算机才有可能实现。\n在R中，pryr包提供了一系列函数来对R中的内存使用情况进行分析。首先，我们可以使用object_size函数来测度某一个对象占用了多少内存：\n\nlibrary(pryr)\n\n\nAttaching package: 'pryr'\n\n\nThe following object is masked from 'package:tidyfst':\n\n    object_size\n\na = rnorm(1e5) # 生成10万个服从正态分布的随机数，赋值给a\nobject_size(a) # 查看变量a占据了多少内存\n\n800.05 kB\n\n\n如果我们想要看整个R当前所占的总内存数量，那么可以使用mem_used函数：\n\nmem_used()\n\n53.5 MB\n\n\n有的时候，我们需要知道经过某一步操作之后，我们R占用内存的变化是多少，可以使用mem_change函数来完成这一步操作：\n\nmem_change(rm(a)) # 把a变量移除后，R所占内存数量的变化\n\n-800 kB\n\n\n注意，如果结果带有负号，说明R所占用内存减少了，否则就是增加了。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据处理效能的衡量</span>"
    ]
  },
  {
    "objectID": "数据处理效能的衡量.html#综合衡量",
    "href": "数据处理效能的衡量.html#综合衡量",
    "title": "3  数据处理效能的衡量",
    "section": "3.3 综合衡量",
    "text": "3.3 综合衡量\n在上面的章节中，我们分别讲述了如何在R中对一些操作完成的时间和占用的内存进行测量。实际上，这两个步骤可以同时完成。使用bench包的mark函数可以实现这一过程，比如我们要对一段代码运行的时间和空间花销进行测度，可以这样操作：\n\nlibrary(bench)\nmark(a = rnorm(1e5))\n\n# A tibble: 1 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 a            2.55ms   2.81ms      349.     781KB     4.08\n\n\n上面的代码会对表达式执行若干次，然后进行效能的衡量。在返回结果中，median代表若干次迭代中时间花销的中位数，mem_alloc代表在运行表达式时，R分配的内存总量；而itr/sec则告诉我们每秒可以对该表达式执行多少次。 mark函数不仅可以对一个表达式的效能进行测量，还可以对多个表达式的效能进行比较。一般来说，默认情况下要求不同表达式的返回值必须是一致的，但是我们可以通过把check参数设置为FALSE来避免这一默认设置。实现方法如下：\n\nmark(\n  a = rnorm(1e5),\n  b = rnorm(1e5 + 1),\n  c = rnorm(2e5),\n  check = FALSE\n)\n\n# A tibble: 3 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 a            2.54ms    2.8ms      345.   781.3KB     6.24\n2 b            2.55ms   2.79ms      346.   781.3KB     4.09\n3 c            5.11ms   5.61ms      175.    1.53MB     6.39\n\n\n以上代码利用mark函数对比了3个表达式的效能。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据处理效能的衡量</span>"
    ]
  },
  {
    "objectID": "数据处理效能的衡量.html#小结",
    "href": "数据处理效能的衡量.html#小结",
    "title": "3  数据处理效能的衡量",
    "section": "3.4 小结",
    "text": "3.4 小结\n在本章中，我们解释了什么是R语言中执行代码的效能，并给出一系列方法来对执行R代码时的时间和空间花销进行衡量。拥有这些工具，我们就可以较为精准地对R代码进行评估，看看其是否高效地完成了我们所需要的功能。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据处理效能的衡量</span>"
    ]
  },
  {
    "objectID": "快速读写：大数据的导入与导出.html",
    "href": "快速读写：大数据的导入与导出.html",
    "title": "4  快速读写：大数据的导入与导出",
    "section": "",
    "text": "4.1 数据导入导出所需要考虑的因素\n在对大数据进行导出和导入的时候，在保证数据不会在导入导出过程中被损坏的前提下，需要重点考虑的因素有三个：1、读写速度；2、内存占用；3、文件格式通用性。一般来说，我们希望数据的读写速度快，内存占用小。除了从效能方面进行考虑以外，有时候还需要考虑的因素是数据是否能够跨平台复用，比如从R语言中导出的数据格式，是否能够用Excel或者Python也能够打开。如果答案是肯定的，那么使用不同分析工具的成员就能够更好地协作完成项目。在本章中，我们将以R语言为例，描述如何对表格进行高效的读写。案例主要使用rio包进行实现（六边形标志符见图4.1），因为它集成了R语言生态中众多的数据I/O工具，满足了我们对数据高效读写的大部分需求。\nFigure 4.1: rio包的六边形标志符",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>快速读写：大数据的导入与导出</span>"
    ]
  },
  {
    "objectID": "快速读写：大数据的导入与导出.html#速度至上",
    "href": "快速读写：大数据的导入与导出.html#速度至上",
    "title": "4  快速读写：大数据的导入与导出",
    "section": "4.2 速度至上",
    "text": "4.2 速度至上\n我们不妨试想一个场景，如果我们在计算机中刚生成了一份珍贵的数据，它非常大。咱们突然被告知，实验室大楼将会在半小时后停电，我们现在需要把数据从R环境中尽快地写出来保存好。在这种条件下，写出数据的速度越快，对我们越有利，这时候我们应该如何操作呢？\n我们不妨来尝试一下，首先我们生成一份数据。\n\nlibrary(pacman)\np_load(pryr,rio)\nnr_of_rows &lt;- 5e7\n\ndf &lt;- data.frame(\n    Logical = sample(c(TRUE, FALSE, NA), prob = c(0.85, 0.1, 0.05), nr_of_rows, replace = TRUE),\n    Integer = sample(1L:100L, nr_of_rows, replace = TRUE),\n    Real = sample(sample(1:10000, 20) / 100, nr_of_rows, replace = TRUE),\n    Factor = as.factor(sample(labels(UScitiesD), nr_of_rows, replace = TRUE))\n  )\n\nobject_size(df)\n\n## 1.00 GB\n\n在上面的操作中，我们随机生成了一份变量名为df的数据框，包含5千万条数据，4列，大致占用内存1 GB。导出什么文件格式才能让速度最快地读出呢？这是我们下面的探索的问题。在R语言生态中，当前导出数据框最快的方法有data.table包的fwrite函数、fst包的write_fst函数、arrow包的write_feather函数和write_parquet函数，以及qs包的qsave函数。下面我们将对这些方案进行比较，看看哪一种方案数据的写出速度最快。\n\np_load(bench)\n\n# 测试函数的定义\ntest_write_speed = function(){\n  \n  # 创建临时文件\n  tempfile(fileext = \".csv\") -&gt; csv_file\n  tempfile(fileext = \".fst\") -&gt; fst_file\n  tempfile(fileext = \".feather\") -&gt; feather_file\n  tempfile(fileext = \".parquet\") -&gt; parquet_file\n  tempfile(fileext = \".qs\") -&gt; qs_file\n  \n  # 声明在函数结束的时候删掉临时文件\n  on.exit(file.remove(csv_file,fst_file,feather_file,parquet_file,qs_file))\n  \n  # 测速\n  mark(\n    export(df,csv_file),\n    export(df,fst_file),\n    export(df,feather_file),\n    export(df,parquet_file),\n    export(df,qs_file),\n    check = F\n  )\n}\n\nres = test_write_speed()\n\nres %&gt;% \n  select(expression,median)\n\n# expression    median\n# export(df, csv_file)    1.53s\n# export(df, fst_file)  193.82ms\n# export(df, feather_file)    1.95s\n# export(df, parquet_file)    3.22s\n# export(df, qs_file)     4.93s\n\n结果表示，当导出为fst格式的文件时，导出速度最快，观察中位数发现，只需要193.82微秒（即不到0.2秒的时间）就完成了数据导出。事实上，这还没有考虑到压缩比率的问题，这些函数还可以设置参数，来对压缩算法和压缩比例进行调整，从而达到更快的导出速度。这个过程留待读者进行进一步尝试。\n除了写出速度以外，读入速度也非常重要。试想另一个场景，我们现在需要对一份数据进行高频读入（多个组员都需要用，而且每一天都需要读入该数据），那么如果能够减少读入的时间，就可以大大提高效率。在这个场景下，应该把文件存成什么格式合适呢？\n这里我们还是以上一份数据为例，测试当它存为不同格式的文件时，读入的时间大概是多少。试验代码如下：\n\n# 测试函数的定义\ntest_read_speed = function(){\n  \n  # 创建临时文件\n  tempfile(fileext = \".csv\") -&gt; csv_file\n  tempfile(fileext = \".fst\") -&gt; fst_file\n  tempfile(fileext = \".feather\") -&gt; feather_file\n  tempfile(fileext = \".parquet\") -&gt; parquet_file\n  tempfile(fileext = \".qs\") -&gt; qs_file\n  \n  # 声明在函数结束的时候删掉临时文件\n  on.exit(file.remove(csv_file,fst_file,feather_file,parquet_file,qs_file))\n  \n  # 写出文件为不同文件格式\n  export(df,csv_file)\n  export(df,fst_file)\n  export(df,feather_file)\n  export(df,parquet_file)\n  export(df,qs_file)\n  \n  # 测速\n  mark(\n    import(csv_file),\n    import(fst_file),\n    import(feather_file),\n    import(parquet_file),\n    import(qs_file),\n    check = F\n  )\n}\n\nres2 = test_read_speed()\n\n# 展示结果\nres2 %&gt;% \n  arrange(median) %&gt;% \n  select(expression,median)\n\n# expression    median\n# import(fst_file)  565.43ms\n# import(feather_file)  770.19ms\n# import(parquet_file)    1.13s\n# import(qs_file)     1.66s\n# import(csv_file)    2.47s\n\n结果显示，从读入时间来看，依然是fst格式最快，不超过0.6秒。 综合来说，如果需要对一个数据框进行最快的读写，只考虑速度快慢的情况下，应该优先考虑把数据保存为fst格式，并使用fst包的read_fst和write_fst函数进行读取和写出（本例中，rio包直接对这些函数进行了调用）。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>快速读写：大数据的导入与导出</span>"
    ]
  },
  {
    "objectID": "快速读写：大数据的导入与导出.html#极限压缩",
    "href": "快速读写：大数据的导入与导出.html#极限压缩",
    "title": "4  快速读写：大数据的导入与导出",
    "section": "4.3 极限压缩",
    "text": "4.3 极限压缩\n现在，让我们来设想另一个场景。你有幸进入一个研究小组开展关于某一个专题的大数据研究，平时数据都在超级计算机上，可以自由进行探索分析。这份数据你需要经常使用，而且处于安全性的考虑你无法把它放到云计算平台进行分析，你只能把数据存储到硬盘上。那么究竟使用什么方法才能够让硬盘承载最多的数据呢？\n在前面一个章节中，我们探讨了如何用最快的速度来读写一份表格文件。那么在本节，我们将会探讨如何才能把数据表的体积压缩到最小，从而提高存储的效率。下面我们依然生成一份数据来进行测试：\n\nlibrary(pacman)\np_load(pryr,rio,tidyverse)\nnr_of_rows &lt;- 5e7\n\ndf &lt;- data.frame(\n    Logical = sample(c(TRUE, FALSE, NA), prob = c(0.85, 0.1, 0.05), nr_of_rows, replace = TRUE),\n    Integer = sample(1L:100L, nr_of_rows, replace = TRUE),\n    Real = sample(sample(1:10000, 20) / 100, nr_of_rows, replace = TRUE),\n    Factor = as.factor(sample(labels(UScitiesD), nr_of_rows, replace = TRUE))\n  )\n\nobject_size(df)\n\n## 1.00 GB\n\n我们将利用不同的方式导出上面这份数据，然后对比文件的体积大小，从而确定最佳的导出方式。我们要测试的导出文件格式包括：1、rds：rds是R语言专用的二进制文件格式，用于保存单个R对象，支持高效的读写和压缩；2、qs：qs是一个快速、压缩率高的R对象序列化格式，适合高性能的数据存储和读取；3、fst：fst是一个高效的R数据框保存格式，支持快速的读写和高压缩率；4、Feather：Feather是一种跨平台的二进制文件格式，基于Apache Arrow，专为高效读写和数据交换设计；5、Parquet：Parquet是一种列式存储格式，广泛应用于大数据处理，具有出色的压缩和查询性能；6、csv：csv是一种简单的文本文件格式，用于存储表格数据，每行一条记录，字段间以逗号分隔，易于阅读和编写，但压缩效果较差。比较代码如下：\n\ntest_size = function(){\n  # 创建临时文件\n  tempfile(fileext = \".rds\") -&gt; rds_file\n  tempfile(fileext = \".csv\") -&gt; csv_file\n  tempfile(fileext = \".fst\") -&gt; fst_file\n  tempfile(fileext = \".feather\") -&gt; feather_file\n  tempfile(fileext = \".parquet\") -&gt; parquet_file\n  tempfile(fileext = \".qs\") -&gt; qs_file\n  \n  # 声明在函数结束的时候删掉临时文件\n  on.exit(file.remove(rds_file,csv_file,fst_file,feather_file,parquet_file,qs_file))\n  \n  export(df,rds_file,compress = \"xz\") # 使用xz压缩方法，保证高压缩比率\n  export(df,csv_file)\n  export(df,fst_file,compress = 100) # 设置压缩比例为100\n  export(df,feather_file,compression = \"zstd\") # 设置使用zstd压缩算法\n  export(df,parquet_file,compression = \"zstd\") # 设置使用zstd压缩算法\n  export(df,qs_file,preset = \"high\") # 设置高压缩率\n  \n  file_sizes &lt;- data.frame(\n    Format = c(\"qs\", \"fst\", \"rds\", \"feather\", \"parquet\", \"csv\"),\n    Size = c(\n      file.size(qs_file),\n      file.size(fst_file),\n      file.size(rds_file),\n      file.size(feather_file),\n      file.size(parquet_file),\n      file.size(qs_file)\n    )\n  )\n}\n\n\ntest_size() -&gt; res\nres %&gt;% \n  arrange(Size) %&gt;% \n  mutate(Size = Size / 1024^2) %&gt;% \n  mutate(Size = str_c(round(Size),\" MB\"))\n\n# Format    Size\n# parquet   97 MB\n# rds   98 MB\n# fst   132 MB\n# feather   150 MB\n# qs    240 MB\n# csv   240 MB\n\n我们在上述操作中，尽量使用压缩比率较高的方案，这样的话有时候可能会需要更长的写出时间。从结果来看，导出为Parquet格式的时候，获得的文件最小，仅为97 MB；其次是R的原生方法，存储为rds文件的时候文件大小为98 MB；而csv和qs格式的文件相对来说体积最大，均为240 MB。因此可以认为，当使用zstd算法（全称为Zstandard，是一种由Facebook开发的快速、无损的数据压缩算法）进行压缩时，把我们的目标文件保存为Parquet文件格式具有最好的压缩比率。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>快速读写：大数据的导入与导出</span>"
    ]
  },
  {
    "objectID": "快速读写：大数据的导入与导出.html#通用交流",
    "href": "快速读写：大数据的导入与导出.html#通用交流",
    "title": "4  快速读写：大数据的导入与导出",
    "section": "4.4 通用交流",
    "text": "4.4 通用交流\n在我们的书中，主要使用R语言作为大数据分析的工具。R语言既支持多种数据格式的读入，也支持多种数据格式的写出。在写出的时候，需要注意写出文件的用途。如果仅仅是保存下来以便后续使用，那么写出什么格式都可以，因为R语言写出的格式基本都可以再用R环境读入。但是如果需要把写出的数据交给其他不会使用R语言的成员进行协作，那么就必须确保写出的数据格式能够有效地被伙伴所利用。\n举个例子，如果在一个数据团队中，只有你懂得如何使用R语言，其他成员只会使用Excel。那么你在使用R语言处理完数据后，得到的结果要给其他成员查阅的话，一般需要生成xlsx或csv格式，这样才能够确保其他伙伴能够在Excel中打开并进行再次加工。\n我们在前面两个章节中使用了R语言生态中的一些大数据方案，需要明确的是，如果使用qs::qsave和saveRDS函数所写出的文件，只有R环境才能再次进行读取，使用其他软件则一般没有通用接口可以读入。而我们在“速度至上”章节中发现的最快格式fst，当前也只是在R中比较流行，这种格式在其他软件平台（如Python）还没有接口能够直接读入。\n在这个背景下，如果生成的数据结果不是特别大，可以优先数据接口的通用性。R语言的rio包支持导出多种格式的数据文件，包括而不限于：1、xls/xlsx（Excel软件通用格式）；2、sas/sas7bdat（SAS软件通用格式）；3、sav/spss/zsav（SPSS软件通用格式）；4、mat/matlab（Matlab软件通用格式）。此外，rio还可以直接导出压缩文件等其他多种格式，详细的支持文件格式列表可以查阅官方链接：https://cran.r-project.org/web/packages/rio/vignettes/rio.html。\n如果导出的数据非常大，那么应该优先选择的数据格式是Apache Arrow所定义的数据格式，即Parquet和Feather。Parquet 和 Feather 是两种用于存储数据的文件格式，它们在设计和用途上有所不同，各自适用于不同的场景和需求。下面我们简单对两种文件格式进行介绍：（1）Parquet：Parquet 文件格式被设计用于最大化存储空间的利用率，采用了先进的压缩和编码技术。它非常适合在存储大量数据时尽量减少磁盘使用空间。Parquet 文件通常比较小，因为它使用了列式存储和高效的压缩策略。然而，读取 Parquet 文件需要相对复杂的解码过程，并且数据不能直接操作，而是需要以大块进行解码。因此，Parquet 文件适合于长期存储和归档目的，即使在未来几年也能被广泛支持的系统读取。（2）Feather：Feather 文件格式最初是为了在 Arrow 文件格式开发之前，简化存储 Arrow 格式的一部分数据而设计的。现在，“Feather version 2” 实际上就是 Arrow IPC 文件格式。Feather 文件格式保留了 Feather 名称和 API 以确保向后兼容性。与 Parquet 相比，Feather 文件更注重数据的直接读写和处理效率。Feather 文件格式中的数据与内存中的数据表示相同，因此读取 Feather 文件时无需解码，可以直接进行访问，从而提高了读写速度和操作效率。一言以蔽之，Parquet 适合长期存储和归档，而 Feather 则更适用于数据的直接读写和操作，特别是在计算任务中的实时数据处理。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>快速读写：大数据的导入与导出</span>"
    ]
  },
  {
    "objectID": "快速读写：大数据的导入与导出.html#小结",
    "href": "快速读写：大数据的导入与导出.html#小结",
    "title": "4  快速读写：大数据的导入与导出",
    "section": "4.5 小结",
    "text": "4.5 小结\n本章聚焦于大数据的读写性能，介绍了大数据读写中需要考虑的三个要素：（1）读写速度；（2）内存占用；（3）文件格式通用性。在R平台中进行测试，发现读写速度最快的文件格式是fst，而存储效率最高的是Parquet格式，在考虑通用交流的时候则需靠考虑团队成员能够读取什么格式的文件。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>快速读写：大数据的导入与导出</span>"
    ]
  },
  {
    "objectID": "快速整理：基于data.table的数据处理工具.html",
    "href": "快速整理：基于data.table的数据处理工具.html",
    "title": "5  快速整理：基于data.table的数据处理工具",
    "section": "",
    "text": "5.1 数据整理的基本操作模式\n对数据处理基本范式的探索最早可以追溯到1970年，当时在IBM工作的牛津大学数学家Edgar F. Codd首次提出了“关系模型”，并具体给出应该遵循的基本准则）。其后，陈品山博士在1976年提出了实体关系模型（Entity-Relationship Model），运用真实世界中事物与关系的观念，来解释数据库中抽象的数据架构。最初对于这些数据处理的实现，主要是由SQL语言来实现的，它是1974年由Boyce和Chamberlin提出的一种介于关系代数与关系演算之间的结构化查询语言，是一个通用的、功能极强的关系型数据库语言。SQL对业界的影响是极其深远的，各大软件公司都有支持SQL语言的数据库产品，比如甲骨文的Oracle和微软的SQL Server。需要明确的是，数据操作的基本范式不依赖于任何软件平台，它是一个概念模型，可以在各种软件工具中得以实现，因此在本部分我们会暂时脱离软件工具来介绍数据处理的基本范式。在介绍之前，我们目前需要有一个概念是，现在有一张二维表格，需要对表格的数据进行数据处理。基本处理方法包括创建、删除、检索、插入、排序、过滤、汇总、分组和连接，下面我们会对这些基本的数据操",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>快速整理：基于data.table的数据处理工具</span>"
    ]
  },
  {
    "objectID": "快速整理：基于data.table的数据处理工具.html#数据整理的基本操作模式",
    "href": "快速整理：基于data.table的数据处理工具.html#数据整理的基本操作模式",
    "title": "5  快速整理：基于data.table的数据处理工具",
    "section": "",
    "text": "5.1.1 创建\n创建的概念非常简单，就是从无到有建立一张二维表。重要的是，我们需要二维表是由什么构成的：1.行；2.列。其中，每一列可以称之为属性或者特征，在一些数据库系统中，我们创建表格的时候是需要对每一列的属性进行定义的。比如我们创建“性别”列的时候，如果里面只有“男”和“女”两种类型的数据，我们一般需要把它定义为字符型。这在不同的数据库系统中不一样，但是我们创建的时候应该对列的名称进行定义。每一行则代表一个记录，也就是在现实世界中的一个实例，比如一个人、一个商品或者是一个城市。\n\n\n5.1.2 删除\n删除就是把已经存在的表格，在环境中删除掉。\n\n\n5.1.3 检索\n这里讲的检索，包括两种，即列检索与行检索。列检索，即对数据表中的列进行选择。选择列可以有很多规则，有的时候我们可以选择特定列，比如我们就像看学生期末语文成绩是多少；有的时候可以选择连续的列，比如我们要看第1到第10列的内容；有的时候可以按照规则选择列，比如我们想要检索列名称以_id作为后缀的列。行检索，则是根据行号对记录进行切片筛选，比如选取地100到200行的记录。如果需要按照条件来对行进行筛选，我们称之为过滤，这在后面会专门提到。\n\n\n5.1.4 插入\n插入就是给表格插入一行，本质上是给总体加入一条记录。举个例子就是，如果老师有全班同学的点名册，现在有一个新的同学加入，那么这个点名册就需要再加一个同学。同时，我们也可以加入一列。比如一年级的同学只需要学习语文、数学和英语，如果到了二年级需要加一门生物课，那么就需要加入新的一列来记录学生的生物成绩。\n\n\n5.1.5 排序\n排序的概念就是，当我们碰上数值型数据的时候，我们可以让这些记录按照升序或者降序排列（升序就是从小到大排列，降序就是从大到小排列）。比如乱序的1,3,5,2,4，经过升序排序可以变成1,2,3,4,5。我们知道，表格可以有很多列，排序的时候需要指定按照哪一个列排序。比如学生有语文、数学和英语成绩，我们只能够按照一种成绩排序，否则会乱成一团。不过事实上可以按照多列排序，但是需要有一定顺序，比如我们可以用学生的成绩先用语文成绩排序，然后再按照数学成绩排序。本来，有的同学本来语文成绩是相同的，因此他们的顺序是随机的；现在，如果学生的语文成绩相同，那么就会按照数学成绩的多少来进行排列。\n\n\n5.1.6 过滤\n过滤就是按照一定的规则来筛选数据。举例说明，我们有全班同学的成绩，但是我们可以按照性别筛选出男同学的成绩；我们也可以按照数学成绩是否达到60分，来筛选出数学不及格同学的成绩。\n\n\n5.1.7 汇总\n汇总，就是要用较少的信息来表征较多的信息。举个例子，我们现在有全班同学的身高，如果我们对这个身高计算平均值，就完成了一个汇总。我们原来的数据可能是五十多名学生的身高，现在我们只用一个平均值就可以代表总体身高的平均水平，用较少的信息表征了较多的信息。汇总的方法可以有很多种，除了求均值，我们还可以求中位数、最大值、最小值等等。\n\n\n5.1.8 分组\n分组就是按照一定的规则给数据表分类，然后按照类别分别进行操作。举例说明，如果我们现在有一个班的学生，我们想知道男同学的语文成绩和女同学的语文成绩，这时候就要根据性别对表格进行分组。分组的功能是很强大的，比如我们有12个班级，要得到每个班级成绩最好的前三名同学，就可以用分组操作进行实现。\n\n\n5.1.9 连接\n连接就是根据多个表都包含的共同信息，对多个表格进行合并的过程。连接分为左连接、右连接、全连接、内连接等。比如我们现在知道三个学生小明、小光和小红，有两张表A和B，分别包含了他们的体育和美术成绩。其中，小红缺考体育，小光缺考美术。两张表格信息如图5.1所示。\n\n\n\n\n\n\n\n\nFigure 5.1: 连接表格示意图\n\n\n\n\n\n现在我们需要通过一定的方式把这两张表格连接起来。下面以此为例，把表格A作为左表，表格B作为右表，分别演示如何进行内连接、左连接、右连接和全连接。\n\n内连接：又称为自然连接，只有两个表格中都包含的信息才会被保留。在我们例子中，只有两个表格都出现的同学小明，才会在合并的表格出现，所得结果见图5.2。\n\n\n\n\n\n\n\n\n\nFigure 5.2: 内连接所得结果\n\n\n\n\n\n\n左连接：只有左边的（第一个出现的）表格的信息会予以完全的保留，右边的表格只有能够匹配左表的信息的内容才会得以保留，结果见图5.3。如果左表存在的信息而右表不存在，会自动填充缺失值。\n\n\n\n\n\n\n\n\n\nFigure 5.3: 左连接所得结果\n\n\n\n\n\n\n右连接：即左连接的逆运算，结果见图5.4。\n\n\n\n\n\n\n\n\n\nFigure 5.4: 右连接所得结果\n\n\n\n\n\n\n全连接：左右表格的信息都会予以保留，无信息处会自动填充缺失值，结果见图5.5。\n\n\n\n\n\n\n\n\n\nFigure 5.5: 全连接所得结果",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>快速整理：基于data.table的数据处理工具</span>"
    ]
  },
  {
    "objectID": "快速整理：基于data.table的数据处理工具.html#data.table数据处理",
    "href": "快速整理：基于data.table的数据处理工具.html#data.table数据处理",
    "title": "5  快速整理：基于data.table的数据处理工具",
    "section": "5.2 data.table数据处理",
    "text": "5.2 data.table数据处理\n\n5.2.1 data.table简介\ndata.table是R语言中的高性能数据处理包，旨在提供比基础数据框（data.frame）更简洁的语法和更丰富的功能。它不仅在数据操作速度上具有显著优势，而且对内存的使用也更加经济，适合处理大规模数据集。data.table的主要功能包括快速的文件读写，通过fread和fwrite函数可以高效地读取和写入csv文件。同时，工具包的底层并行化功能使许多常见操作能够利用多个CPU线程，进一步提升数据处理效率。对于大规模数据集，data.table可以实现快速且可扩展的聚合操作，例如能够在内存中处理100GB的数据（前提是计算机的内存需要大于100GB）。\ndata.table具有强大的数据处理能力，比如在连接操作方面，data.table提供了丰富的选项，包括有序连接、重叠区间连接、非等值连接、基于连接的汇总和更新等。这些功能使得复杂数据操作变得更加简便高效。此外，data.table支持基于引用的快速列操作，允许用户快速地添加、更新或删除列，而无需复制数据。这对于大数据集尤为重要，因为它避免了不必要的内存消耗和时间开销。数据重塑功能也是data.table的一大亮点，通过dcast和melt函数，可以轻松实现数据列表的长宽转换。另一个显著优势是，data.table几乎没有外部依赖，除了基础R之外，无需其他包。这简化了生产环境中的部署和维护工作。同时，data.table兼容旧版本的R，确保了在不同系统和环境中的稳定性和一致性。\n总之，data.table以其高效的性能、简洁的语法和丰富的功能，成为R语言中处理和分析大规模数据的理想工具。它不仅提高了数据处理的速度和效率，还通过稳定的API和强大的社区支持，为用户提供了一个强大而可靠的解决方案。\n\n\n5.2.2 基本操作实现\n本部分将介绍如何使用data.table包来实现前一个小节提出的数据整理基本操作模式。尽管很多用户反馈data.table的语法结构非常难学，但其实熟悉了语法逻辑之后还是可以轻松掌握。首先我们将载入data.table包。\n\nlibrary(data.table)\n\n\n5.2.2.1 创建\ndata.table包中设置了一种称之为data.table的数据结构，data.table是data.frame的增强模式，它具备的特征如下：\n\n行号使用冒号（:）打印，以便在视觉上将行号与第一列分开。\n当记录个数超过n行时（默认n等于100），会自动显示前五行和后五行，不会像数据框一样无限输出。这个可以使用options(datatable.print.nrows = n)来设定n是多少，同时可以用getOption(\"datatable.print.nrows\")来对n进行查询\ndata.table从来不使用行名称。\n\n在我们的演示案例中，我们认为显示前2列和后2列就足够了，因此利用options函数进行设置：\n\noptions(datatable.print.topn = 2)\n\n要在R环境中创建data.table格式的表格，有三种形式：1.内部创建；2.强制转化；3.外部读入。我们会分别进行介绍。\n\n5.2.2.1.1 内部创建\n创建data.table其实与创建data.frame的语法完全一样。事实上，所有data.table都继承data.frame的所有属性。下面我们创建一个基本的data.table。\n\nDT = data.table(\n  ID = c(\"b\",\"b\",\"b\",\"a\",\"a\",\"c\"),\n  a = 1:6,\n  b = 7:12,\n  c = 13:18\n)\nDT\n\n       ID     a     b     c\n   &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1:      b     1     7    13\n2:      b     2     8    14\n3:      b     3     9    15\n4:      a     4    10    16\n5:      a     5    11    17\n6:      c     6    12    18\n\n\n让我们看看它的数据结构：\n\nstr(DT)\n\nClasses 'data.table' and 'data.frame':  6 obs. of  4 variables:\n $ ID: chr  \"b\" \"b\" \"b\" \"a\" ...\n $ a : int  1 2 3 4 5 6\n $ b : int  7 8 9 10 11 12\n $ c : int  13 14 15 16 17 18\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n可以发现，它既是一个data.table，也是一个data.frame。\n\n\n5.2.2.1.2 强制转化\n我们可以把已经有的数据框、矩阵和列表转化为data.table格式。转化函数有两个，一个是as.data.table函数，一个是setDT函数。前者与as.data.frame是一样的，能够随意自由转换成别的格式。setDT实现的功能是原位转化，转化之后不需要赋值，原始的变量直接变成了data.table。这里我们对这两个函数分别进行演示。 首先，我们的案例主要用到iris数据集，因此我们要把它转化为data.table格式，存放在iris.dt变量中。\n\nas.data.table(iris) -&gt; iris.dt   \n\niris.dt\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1         3.5          1.4         0.2    setosa\n  2:          4.9         3.0          1.4         0.2    setosa\n ---                                                            \n149:          6.2         3.4          5.4         2.3 virginica\n150:          5.9         3.0          5.1         1.8 virginica\n\n\n然后，我们再来尝试setDT函数。需要明确的是，如果用setDT函数就不需要额外进行赋值，返回结果会自动赋值给原来的变量。我们会用mtcars数据集来举例，因为基本包内置数据集是不能随便更改的，因此我们先赋值给a。\n\nmtcars -&gt; a\nstr(a)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n现在，让我们把a变量变成data.table格式。\n\nsetDT(a)\nstr(a)\n\nClasses 'data.table' and 'data.frame':  32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n现在，我们虽然没有再次把结果赋值给a，但是a已经变为data.table的格式了。\n\n\n5.2.2.1.3 外部读入\nfread也许是是目前R语言中读取csv格式文件最快的函数，关于它的各种高级特性，可以在官网https://github.com/Rdatatable/data.table/wiki/Convenience-features-of-fread中进行了解。如果希望知道它有什么个性化的参数设置，可以用?fread进行查询。事实上它的使用是非常简便的，直接放入文件路径即可读取任意csv文件，并返回一个data.table格式的变量。 在我们的例子中，先读出一个变量到根目录下名为temp的文件夹中。data.table的fwrite函数一样非常有名，它是写出csv格式数据最快的函数。\n\nfwrite(iris.dt,\"temp/iris.csv\")\n\n接下来，我们重新读入。\n\n#用fread读入文件，赋值给iris.1变量\nfread(\"temp/iris.csv\") -&gt; iris.1 \n\n#查看iris.1的数据类型\nclass(iris.1) \n\n[1] \"data.table\" \"data.frame\"\n\n\n操作实在太简便了，不过iris数据集太小了，大家看不到它的威力。如果条件允许的读者，可以拿非常大的csv进行读写尝试，使用方法是一样的，加速效果非常明显。\n\n\n\n5.2.2.2 删除\n表格删除在所有R环境中基本都是一样的，都是使用rm函数。这里我们删除掉所有的变量，但是留下iris.dt变量做演示：\n\nrm(list=setdiff(ls(), \"iris.dt\"))\n\n此外，我们每次都在temp文件夹中写出文件，但是并没有删除它。其实我们会尝试利用R软件来管理文件夹中的文件，因此我们会对之前在temp文件夹中创建的文件进行删除。我们还记得，之前写出文件的文件名是“iris.csv”，我们用基本包的file.exists函数看看这个文件是否还在temp文件夹中。\n\nfile.exists(\"temp/iris.csv\")\n\n[1] TRUE\n\n\n返回了一个逻辑值TRUE，说明这个文件还在temp文件夹中，下面我们用file.remove函数把它删除掉。\n\nfile.remove(\"temp/iris.csv\")\n\n[1] TRUE\n\n\n返回值证明我们已经成功删除掉了，让我们再看它是否存在。\n\nfile.exists(\"temp/iris.csv\")\n\n[1] FALSE\n\n\n现在我们在temp文件夹中已经找不到这个文件了。\n\n\n5.2.2.3 检索\n检索是最基本的操作，但是我们需要明确的是，检索返回的一般还是一个data.table。我们需要统一返回的格式，这样有利于规范我们的数据处理范式。\n\n5.2.2.3.1 行检索\n因为data.table永远不会使用行名称，因此对行的检索只能够通过序号，也就是告诉程序我们要检索第几行。操作基本与基本包的data.frame类似，但是我们要永远记住，data.table的最基本格式是DT[i,j,by]。其中，i控制行，j控制列，by控制分组。要进行行检索，只要对i进行控制即可。尽管在语法上，data.table允许缺省其他逗号，直接对行进行检索（即DT[i]）。但是这里不建议这么做，而是倡导永远把所有逗号补全（即DT[i,,]）。自由诚可贵，规范价更高，只有规范的风格才能让我们的代码走得更长远。下面我们来举例子熟悉一下操作，比如我们要在选取数据框的第2行，可以这样操作：\n\niris.dt[2,,]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.9           3          1.4         0.2  setosa\n\n\n如果要选取第2-5行，可以这样操作：\n\niris.dt[2:5,,]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.9         3.0          1.4         0.2  setosa\n2:          4.7         3.2          1.3         0.2  setosa\n3:          4.6         3.1          1.5         0.2  setosa\n4:          5.0         3.6          1.4         0.2  setosa\n\n\n如果需要选取第3、5、9行，可以这样操作：\n\niris.dt[c(3,5,9),,]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.7         3.2          1.3         0.2  setosa\n2:          5.0         3.6          1.4         0.2  setosa\n3:          4.4         2.9          1.4         0.2  setosa\n\n\n去除第2到4行，可以这样操作：\n\niris.dt[-(2:4),,]  #等价于iris.dt[!2:4,,]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1         3.5          1.4         0.2    setosa\n  2:          5.0         3.6          1.4         0.2    setosa\n ---                                                            \n146:          6.2         3.4          5.4         2.3 virginica\n147:          5.9         3.0          5.1         1.8 virginica\n\n\n我们可以看到，如果需要选择多行，就需要使用向量来操作。\n\n\n5.2.2.3.2 列检索\n列检索与基本包有相似之处，但并不完全相同。我们可以根据序号和列名称来检索数据表的列。先介绍用序号来选择，因为它与数据框的操作基本是一样的，但是我们不能忘记其经典的DT[i,j,by]格式。比如我们要选取其中的第二列，可以这样操作：\n\niris.dt[,2,]\n\n     Sepal.Width\n           &lt;num&gt;\n  1:         3.5\n  2:         3.0\n ---            \n149:         3.4\n150:         3.0\n\n\n选取第2-4列，可以这样操作：\n\niris.dt[,2:4,]\n\n     Sepal.Width Petal.Length Petal.Width\n           &lt;num&gt;        &lt;num&gt;       &lt;num&gt;\n  1:         3.5          1.4         0.2\n  2:         3.0          1.4         0.2\n ---                                     \n149:         3.4          5.4         2.3\n150:         3.0          5.1         1.8\n\n\n选取第1、3、5列，可以这样操作：\n\niris.dt[,c(1,3,5),]\n\n     Sepal.Length Petal.Length   Species\n            &lt;num&gt;        &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1          1.4    setosa\n  2:          4.9          1.4    setosa\n ---                                    \n149:          6.2          5.4 virginica\n150:          5.9          5.1 virginica\n\n\n去除第2到3列，可以这样操作：\n\niris.dt[,-(2:3),]  #等价于iris.dt[,!2:3,]\n\n     Sepal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1         0.2    setosa\n  2:          4.9         0.2    setosa\n ---                                   \n149:          6.2         2.3 virginica\n150:          5.9         1.8 virginica\n\n\n现在，我们尝试利用列的名称对表格的列进行检索。尽管我们可以把列名称直接放在DT[i,j,by]中的j里面，即输入iris.dt[,Sepal.Length,]。但是这样会返回一个向量，而不是data.table，因此我们不会用这个方法。要返回data.table格式，需要在查询列名称的时候输入.()格式。 比如我们取出Sepal.Length列：\n\niris.dt[,.(Sepal.Length),]\n\n     Sepal.Length\n            &lt;num&gt;\n  1:          5.1\n  2:          4.9\n ---             \n149:          6.2\n150:          5.9\n\n\n如果要取出多列，直接加上其他列名称即可，中间用逗号分隔：\n\niris.dt[,.(Sepal.Length,Sepal.Width,Species),]\n\n     Sepal.Length Sepal.Width   Species\n            &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1         3.5    setosa\n  2:          4.9         3.0    setosa\n ---                                   \n149:          6.2         3.4 virginica\n150:          5.9         3.0 virginica\n\n\n其实，行列检索是可以同时检索的。比如我们需要提取第3到5行的第1到3列，可以这样进行编程：\n\niris.dt[3:5,1:3,]\n\n   Sepal.Length Sepal.Width Petal.Length\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:          4.7         3.2          1.3\n2:          4.6         3.1          1.5\n3:          5.0         3.6          1.4\n\n\n\n\n\n5.2.2.4 插入\n因为data.table本质上还是一个data.frame，因此如果要按照行列进行合并，操作与基本包是完全一致的，依然是用rbind和cbind函数。不过data.table构造一个新列，是需要知道如何操作的。因为它跟我们之前的认识并不一致，需要用到:=进行赋值。比方说，我们要给iris.dt增加一个常数列，名称为new.column，所有数字均为1，可以这样操作：\n\niris.dt[,new.column := 1,]\n\niris.dt\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species new.column\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;      &lt;num&gt;\n  1:          5.1         3.5          1.4         0.2    setosa          1\n  2:          4.9         3.0          1.4         0.2    setosa          1\n ---                                                                       \n149:          6.2         3.4          5.4         2.3 virginica          1\n150:          5.9         3.0          5.1         1.8 virginica          1\n\n\n在data.table中增加新列，是会直接进行更新（而不需要进行赋值）。也就是说，iris.dt在插入列的那一刻，它就不是它自己了，而是加入了一列的它。如果想要删除这一列，需要把空值NULL赋值给这一列。\n\niris.dt[,new.column := NULL,]\n\niris.dt\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          5.1         3.5          1.4         0.2    setosa\n  2:          4.9         3.0          1.4         0.2    setosa\n ---                                                            \n149:          6.2         3.4          5.4         2.3 virginica\n150:          5.9         3.0          5.1         1.8 virginica\n\n\n这样它就还原为我们最初的表格了。 很多data.table爱好者认为这是一个很优良的特性，觉得省事儿了，可以少写一个赋值语句。这其实要客观地看待，因为很多时候我们还希望重复利用原来的表格，但是这样赋值之后我们原来的表格就永远地发生了变化，原来的表格就不在了。在这种情况下，我们就可以先通过赋值做一个备份，然后使用备份进行添加列的操作，那么原始表格的数据也得以保留。\n\n\n5.2.2.5 排序\ndata.table的排序基本与data.frame相似，就是对变量进行排序，这需要用order函数。不过data.table数据格式已经进行优化，我们不需要每次都用$来进行取值。 举个例子，如果我们要根据Sepal.Length进行升序排列，可以这样进行操作：\n\niris.dt[order(Sepal.Length),,]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          4.3         3.0          1.1         0.1    setosa\n  2:          4.4         2.9          1.4         0.2    setosa\n ---                                                            \n149:          7.7         3.0          6.1         2.3 virginica\n150:          7.9         3.8          6.4         2.0 virginica\n\n\n降序排列加入负号即可：\n\niris.dt[order(-Sepal.Length),,]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          7.9         3.8          6.4         2.0 virginica\n  2:          7.7         3.8          6.7         2.2 virginica\n ---                                                            \n149:          4.4         3.2          1.3         0.2    setosa\n150:          4.3         3.0          1.1         0.1    setosa\n\n\n多个变量排序也与基本包一样，比如需要先对Sepal.Length进行升序排列，再对Sepal.Width进行降序排列，那么可以这样操作：\n\niris.dt[order(Sepal.Length,-Sepal.Width),,]\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          4.3         3.0          1.1         0.1    setosa\n  2:          4.4         3.2          1.3         0.2    setosa\n ---                                                            \n149:          7.7         2.6          6.9         2.3 virginica\n150:          7.9         3.8          6.4         2.0 virginica\n\n\n\n\n5.2.2.6 过滤\n在data.table进行过滤，主要是靠DT[i,j,by]中的i来进行的，也就是把条件放在i中即可。比如，我们要选择Sepal.Length等于5.1的记录：\n\niris.dt[Sepal.Length == 5.1,,]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;     &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2     setosa\n2:          5.1         3.5          1.4         0.3     setosa\n3:          5.1         3.8          1.5         0.3     setosa\n4:          5.1         3.7          1.5         0.4     setosa\n5:          5.1         3.3          1.7         0.5     setosa\n6:          5.1         3.4          1.5         0.2     setosa\n7:          5.1         3.8          1.9         0.4     setosa\n8:          5.1         3.8          1.6         0.2     setosa\n9:          5.1         2.5          3.0         1.1 versicolor\n\n\n可以通过且（&）、或（|）、非（!）来进行条件控制，这一点跟基本包data.frame是一样的。比如，我们要选择Sepal.Length等于5.1且Sepal.Width大于3.5的记录，可以这样操作：\n\niris.dt[Sepal.Length == 5.1 & Sepal.Width &gt; 3.5,,]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.8          1.5         0.3  setosa\n2:          5.1         3.7          1.5         0.4  setosa\n3:          5.1         3.8          1.9         0.4  setosa\n4:          5.1         3.8          1.6         0.2  setosa\n\n\n\n\n5.2.2.7 汇总\n我们还记得，汇总就是把一个向量变为一个数值。在data.table中汇总需要控制DT[i,j,by]中的j。比如说我们要得到Sepal.Length的均值：\n\niris.dt[,mean(Sepal.Length),]\n\n[1] 5.843333\n\n\n如果想要对多个列进行求均值的操作，就需要用到.SDcols和.SD这些指定的特殊符号了。比如我们现在要求Sepal.Length和Sepal.Width的均值，需要这么操作：\n\niris.dt[,lapply(.SD, mean, na.rm=TRUE),\n        .SDcols = c(\"Sepal.Length\",\"Sepal.Width\")]\n\n   Sepal.Length Sepal.Width\n          &lt;num&gt;       &lt;num&gt;\n1:     5.843333    3.057333\n\n\n这个操作中，第一步是利用.SDcols来指定我们想要操作的列是哪些，.SD则是指数据的子集，它是数据根据分组返回的若干列。尽管我们没有分组，但是这里还是必须用lapply这个函数才能够正确完成操作。最后，我们还在最后设置了na.rm=TRUE来保证忽略缺失值。不过其实我们的数据中并没有缺失值，写出来只是为了告诉大家，能够用这种方法来对函数传递额外的参数。 因此我们知道，列名称是在.SDcols中进行选择的，所以也可以根据列名称进行筛选。比如我们要选列名称以“Width”结尾的变量的均值，可以利用基本包中的endsWith函数：\n\niris.dt[,lapply(.SD, mean, na.rm=TRUE),\n        .SDcols = endsWith(names(iris.dt),\"Width\")]\n\n   Sepal.Width Petal.Width\n         &lt;num&gt;       &lt;num&gt;\n1:    3.057333    1.199333\n\n\n如果想要得到所有数值型变量的均值，需要得到数值型变量的变量名，然后放入.SDcols参数中即可。实现方法可以参考以下代码：\n\n#sapply按照列来做函数操作，求得列向量数据类型\nsapply(iris.dt,class) -&gt; a    \n\n#把数据类型为数值型的列名称提取出来\nnames(a[a==\"numeric\"]) -&gt; numeric.names   \n\n# 对数值型列进行求均值，忽略缺失值\niris.dt[,lapply(.SD, mean, na.rm=TRUE),\n        .SDcols = numeric.names]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;\n1:     5.843333    3.057333        3.758    1.199333\n\n\n不过其实还有另一种捷径可以完成这一步操作，就是直接使用is.numeric函数，实现方法如下：\n\niris.dt[,lapply(.SD, mean, na.rm=TRUE),\n        .SDcols = is.numeric]\n\n\n\n5.2.2.8 分组\ndata.table中分组是通过控制DT[i,j,by]中的by来实现的，by可以接收一个你需要进行分组的变量。这里建议尽量在变量中加入.()符号，这样可以保证结果返回另一个data.table。不过在只有一个变量的时候，其实是可以缺省的。下面举个例子，我们要根据Species分组，然后对Sepal.Length求平均值。这与我们之前的汇总操作一样，只是这次增设了一个by参数而已。\n\niris.dt[,mean(Sepal.Length),by = Species]\n\n      Species    V1\n       &lt;fctr&gt; &lt;num&gt;\n1:     setosa 5.006\n2: versicolor 5.936\n3:  virginica 6.588\n\n\n不过这样的话，我们的列名称不够直观，我们可以使用.SDcols来控制我们需要处理的列。\n\niris.dt[,lapply(.SD, mean, na.rm=TRUE), \n        by = Species,\n        .SDcols = \"Sepal.Length\"]\n\n      Species Sepal.Length\n       &lt;fctr&gt;        &lt;num&gt;\n1:     setosa        5.006\n2: versicolor        5.936\n3:  virginica        6.588\n\n\n对多个列进行操作也大同小异，操作方法如下：\n\niris.dt[,lapply(.SD, mean, na.rm=TRUE), \n        by = Species,\n        .SDcols = c(\"Sepal.Length\",\"Sepal.Width\")]\n\n      Species Sepal.Length Sepal.Width\n       &lt;fctr&gt;        &lt;num&gt;       &lt;num&gt;\n1:     setosa        5.006       3.428\n2: versicolor        5.936       2.770\n3:  virginica        6.588       2.974\n\n\n事实上，.SDcols可以放在DT[i,j,by]中任意位置。但是这里建议永远把这个部分放在最后，不要破坏传统的DT[i,j,by]结构，这样可以提高代码的可读性。\n\n\n5.2.2.9 连接\n在data.table中使用连接有以下特点：\n\n如果两个表格都设置了主键，那么会优先基于主键进行连接。如果没有的话，跳到下一步。\n如果只有第一个表格设置了主键，那么会优先基于第一个表格的主键进行连接。如果没有设置，跳到下一步。\n基于两个表格拥有的共同列名称，对这些列进行连接。 如果需要自定义，请直接使用by.x和by.y对两个表格需要连接的列进行设置。那么会直接跳过上面三个步骤进行自定义连接。\n\n在data.table里面使用连接，可以使用merge函数。下面我们会对如何连接进行演示，首先构建顾客交易数据表：\n\ndf1 = data.table(CustomerId = c(1:6), Product = c(rep(\"Oven\", 3), rep(\"Television\", 3)))\ndf1\n\n   CustomerId    Product\n        &lt;int&gt;     &lt;char&gt;\n1:          1       Oven\n2:          2       Oven\n3:          3       Oven\n4:          4 Television\n5:          5 Television\n6:          6 Television\n\n\n再构建顾客地址数据表：\n\ndf2 = data.table(CustomerId = c(2, 4, 6), State = c(rep(\"California\", 2), rep(\"Texas\", 1)))\ndf2\n\n   CustomerId      State\n        &lt;num&gt;     &lt;char&gt;\n1:          2 California\n2:          4 California\n3:          6      Texas\n\n\n首先我们尝试进行内连接：\n\ndf &lt;- merge(x=df1,y=df2,by=\"CustomerId\")\ndf\n\nKey: &lt;CustomerId&gt;\n   CustomerId    Product      State\n        &lt;int&gt;     &lt;char&gt;     &lt;char&gt;\n1:          2       Oven California\n2:          4 Television California\n3:          6 Television      Texas\n\n\n这里建议大家尽量设置by参数，这样能够声明我们是根据哪一个列进行合并的。 左连接可以通过设置all.x = T来实现：\n\ndf&lt;-merge(x=df1,y=df2,by=\"CustomerId\",all.x=T)\ndf\n\nKey: &lt;CustomerId&gt;\n   CustomerId    Product      State\n        &lt;int&gt;     &lt;char&gt;     &lt;char&gt;\n1:          1       Oven       &lt;NA&gt;\n2:          2       Oven California\n3:          3       Oven       &lt;NA&gt;\n4:          4 Television California\n5:          5 Television       &lt;NA&gt;\n6:          6 Television      Texas\n\n\n右连接则可以通过设置all.y=T进行实现：\n\ndf&lt;-merge(x=df1,y=df2,by=\"CustomerId\",all.y=T)\ndf\n\nKey: &lt;CustomerId&gt;\n   CustomerId    Product      State\n        &lt;int&gt;     &lt;char&gt;     &lt;char&gt;\n1:          2       Oven California\n2:          4 Television California\n3:          6 Television      Texas\n\n\n如果要实现全连接，则设置all=T：\n\ndf&lt;-merge(x=df1,y=df2,by=\"CustomerId\",all=T)\ndf\n\nKey: &lt;CustomerId&gt;\n   CustomerId    Product      State\n        &lt;int&gt;     &lt;char&gt;     &lt;char&gt;\n1:          1       Oven       &lt;NA&gt;\n2:          2       Oven California\n3:          3       Oven       &lt;NA&gt;\n4:          4 Television California\n5:          5 Television       &lt;NA&gt;\n6:          6 Television      Texas\n\n\n如果需要连接的列在两个表格中的列名称不一样，就要使用by.x和by.y来设置两个表格中用来连接的列。 下面举例说明，首先构造两个表格。\n\ncopy(iris.dt)[1:3,1:3,][,id := 1:3,][] -&gt; dt1\ncopy(iris.dt)[1:3,3:5,][,id := 1:3,][] -&gt; dt2\n\n上面的代码，使用了连锁管道操作（[][]），其实就是得到的data.table继续用[i,j,by]来进行操作。我们首先用copy函数得到iris.dt的一份拷贝，取了表格第1到3行。然后两个表格分别取了1到3列和3到5列。最后，我们给他们都加上了id标号，然后最后加上[]把它们取出来（这就是:=带来的副作用，它直接在原来的地方修改，但是原来的地方我们又没有采用变量来引用，所以我们是无法找到的，因此[]是必不可少的）。下面我们看看两个表格的内容：\n\ndt1\n\n   Sepal.Length Sepal.Width Petal.Length    id\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:          5.1         3.5          1.4     1\n2:          4.9         3.0          1.4     2\n3:          4.7         3.2          1.3     3\n\ndt2\n\n   Petal.Length Petal.Width Species    id\n          &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt; &lt;int&gt;\n1:          1.4         0.2  setosa     1\n2:          1.4         0.2  setosa     2\n3:          1.3         0.2  setosa     3\n\n\n我们可以看到，两个表格中都有Petal.Length和id两列，我们把第二个表格dt2的的这两列重新命名为“a”和“b”。\n\nsetnames(dt2,\"Petal.Length\",\"a\") #把原来的Petal.Width列命名为a\nsetnames(dt2,\"id\",\"b\")           #把原来的id列命名为b\ndt2\n\n       a Petal.Width Species     b\n   &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt; &lt;int&gt;\n1:   1.4         0.2  setosa     1\n2:   1.4         0.2  setosa     2\n3:   1.3         0.2  setosa     3\n\n\n现在让我们使用内连接来合并dt1和dt2。\n\nmerge(dt1,dt2,by.x = c(\"Petal.Length\",\"id\"),by.y = c(\"a\",\"b\")) -&gt; dt\ndt\n\nKey: &lt;Petal.Length, id&gt;\n   Petal.Length    id Sepal.Length Sepal.Width Petal.Width Species\n          &lt;num&gt; &lt;int&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          1.3     3          4.7         3.2         0.2  setosa\n2:          1.4     1          5.1         3.5         0.2  setosa\n3:          1.4     2          4.9         3.0         0.2  setosa\n\n\n这样就成功完成连接了。只要by.x和by.y中的变量能够一一对应，就能够完成基于不同列名称的连接。\n\n\n\n5.2.3 高级特性介绍\n除了基本操作的实现以外，data.table包还提供了很多高级的特性，这使得该包在内存管理和高速检索等任务中具有明显的优势。尽管高级特性带来了更多的便捷，但是这也要求使用者对这些特性具有更加深入的理解。下面让我们来对这些特性进行探讨。\n\n5.2.3.1 原位更新\n在data.table包中，set家族（即以set开头的data.table函数）和:=可以对数据框进行原位修改，不需要再进行赋值。下面我们举个例子，还是用iris数据集，但是我们把它先赋值给iris2变量。\n\nlibrary(data.table)\niris -&gt; iris2\n\n现在，我们要把iris2的表格转化为data.table格式，正常来说，我们需要使用as.data.table函数：\n\n# 观察iris2的数据结构\nclass(iris2)\n\n[1] \"data.frame\"\n\n# 进行转化\nas.data.table(iris2) -&gt; iris.dt\n\n# 观察iris.dt的数据结构\nclass(iris.dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\n这样我们就把data.frame格式的iris2转化为data.table格式，并赋值给iris.dt。事实上我们还可以直接使用setDT函数，这时候，我们不需要再次进行赋值。\n\nclass(iris2)\n\n[1] \"data.frame\"\n\nsetDT(iris2)\nclass(iris2)\n\n[1] \"data.table\" \"data.frame\"\n\n\n这时候我们发现，iris2本身已经转化为一个data.table。 :=是一种赋值的符号，它的特点是会在原始的表格中进行赋值，不需要赋值给新的变量。下面我们用刚刚创建的iris.dt做例子。我们给1到3行增加一列常数列，所有数值均为0，赋值给zero。\n\niris.dt[1:3,zero := 0,]\niris.dt\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species  zero\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt; &lt;num&gt;\n  1:          5.1         3.5          1.4         0.2    setosa     0\n  2:          4.9         3.0          1.4         0.2    setosa     0\n ---                                                                  \n149:          6.2         3.4          5.4         2.3 virginica    NA\n150:          5.9         3.0          5.1         1.8 virginica    NA\n\n\n除了前3行外，其他zero的值都是NA。 :=的用法有以下特点：1.它只是用于更新列，没有任何的返回值（需要返回值的时候，需要多加一个[]）；2.可以指定条件进行更新；3.更新后不需要赋值，原始表格永远发生了改变。 我们可以给zero列赋值为NULL（空值），从而删除这一列。\n\niris.dt[,zero := NULL,]\n\n如果不希望改变原来的表格，就需要先把变量存在其他地方，或者使用copy函数。其实，为了不要改变原始的iris表格，我才在最开始就把iris赋值给iris2。下面演示copy的用法： 先观察iris2的1到3行：\n\niris2[1:3]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n2:          4.9         3.0          1.4         0.2  setosa\n3:          4.7         3.2          1.3         0.2  setosa\n\n\n对iris2表格进行拷贝，然后增加一列zero的常数列，数值都是0，最后取它的1-3行进行观察。\n\ncopy(iris2)[,zero := 0,][1:3]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  zero\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt; &lt;num&gt;\n1:          5.1         3.5          1.4         0.2  setosa     0\n2:          4.9         3.0          1.4         0.2  setosa     0\n3:          4.7         3.2          1.3         0.2  setosa     0\n\n\n尽管:=总是在原位进行更新，但是我们是对iris2的拷贝进行更新，iris2本身没有发生改变。让我们再看iris2的前3行：\n\niris2[1:3]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n2:          4.9         3.0          1.4         0.2  setosa\n3:          4.7         3.2          1.3         0.2  setosa\n\n\n还是没有zero这一列，因此可以看到使用copy函数可以在不改变原始表格的条件下构造新的data.table。\n\n\n5.2.3.2 长宽数据转换\nmelt和dcast两个函数首次出现于reshape2包，这个包最主要的功能就是完成长宽数据转换。但是在data.table包中，对这两个函数进行了优化。因此尽量避免在使用data.table包的时候加载reshape2包，否则很可能会造成歧义（也就是在使用这两个函数的时候，我们不知道究竟用的是reshape2的还是data.table的）。 data.table对reshape2的melt与dcast的功能升级，主要是针对多个转换同时进行的简化。但是我们在介绍这个特性之前，先介绍单个转化的完成。首先我们构造一个示例数据集：\n\ns2 &lt;- \"family_id age_mother dob_child1 dob_child2 dob_child3 gender_child1 gender_child2 gender_child3\n1         30 1998-11-26 2000-01-29         NA             1             2            NA\n2         27 1996-06-22         NA         NA             2            NA            NA\n3         26 2002-07-11 2004-04-05 2007-09-02             2             2             1\n4         32 2004-10-10 2009-08-27 2012-07-21             1             1             1\n5         29 2000-12-05 2005-02-28         NA             2             1            NA\"\nDT &lt;- fread(s2)\nDT\n\n   family_id age_mother dob_child1 dob_child2 dob_child3 gender_child1\n       &lt;int&gt;      &lt;int&gt;     &lt;IDat&gt;     &lt;IDat&gt;     &lt;IDat&gt;         &lt;int&gt;\n1:         1         30 1998-11-26 2000-01-29       &lt;NA&gt;             1\n2:         2         27 1996-06-22       &lt;NA&gt;       &lt;NA&gt;             2\n3:         3         26 2002-07-11 2004-04-05 2007-09-02             2\n4:         4         32 2004-10-10 2009-08-27 2012-07-21             1\n5:         5         29 2000-12-05 2005-02-28       &lt;NA&gt;             2\n   gender_child2 gender_child3\n           &lt;int&gt;         &lt;int&gt;\n1:             2            NA\n2:            NA            NA\n3:             2             1\n4:             1             1\n5:             1            NA\n\n\n这个数据集中，包含了家庭编号（family_id）、母亲的年龄（age_mother）和家庭中每个孩子出生的日期和性别。dob是“date of birth”的缩写；而gender代表性别，其中1为女性，2为男性。这个数据集来自于官方的介绍案例，链接为：https://cloud.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html。 这显然是一个宽数据，我们首先尝试把孩子的出生日期进行宽转长的变化。\n\n#构造要转化的列变量名称的向量\ncolA = paste(\"dob_child\", 1:3, sep = \"\")\n\n#宽数据转化为长数据，只把dob_child1/dob_child2/dob_child3进行了转化\nDT.m = melt(DT, measure.vars = list(colA), value.name = c(\"dob\"))\nDT.m\n\n    family_id age_mother gender_child1 gender_child2 gender_child3   variable\n        &lt;int&gt;      &lt;int&gt;         &lt;int&gt;         &lt;int&gt;         &lt;int&gt;     &lt;fctr&gt;\n 1:         1         30             1             2            NA dob_child1\n 2:         2         27             2            NA            NA dob_child1\n 3:         3         26             2             2             1 dob_child1\n 4:         4         32             1             1             1 dob_child1\n 5:         5         29             2             1            NA dob_child1\n 6:         1         30             1             2            NA dob_child2\n 7:         2         27             2            NA            NA dob_child2\n 8:         3         26             2             2             1 dob_child2\n 9:         4         32             1             1             1 dob_child2\n10:         5         29             2             1            NA dob_child2\n11:         1         30             1             2            NA dob_child3\n12:         2         27             2            NA            NA dob_child3\n13:         3         26             2             2             1 dob_child3\n14:         4         32             1             1             1 dob_child3\n15:         5         29             2             1            NA dob_child3\n           dob\n        &lt;IDat&gt;\n 1: 1998-11-26\n 2: 1996-06-22\n 3: 2002-07-11\n 4: 2004-10-10\n 5: 2000-12-05\n 6: 2000-01-29\n 7:       &lt;NA&gt;\n 8: 2004-04-05\n 9: 2009-08-27\n10: 2005-02-28\n11:       &lt;NA&gt;\n12:       &lt;NA&gt;\n13: 2007-09-02\n14: 2012-07-21\n15:       &lt;NA&gt;\n\n\nmeasure.vars可以接收一个或一组变量，即我们需要聚合的列，聚合的变量名可以用variable.name来定义（否则会使用默认的“variable”作为变量名称）。聚合后的值的列名称可以用value.name进行设置，否则会默认使用“value”作为列名（本例中使用了“dob”作为列名）。 如果要把这个长数据还原，可以使用dcast函数，代码如下：\n\nDT.c = dcast(DT.m, ...~ variable, value.var = c(\"dob\"))\nDT.c\n\nKey: &lt;family_id, age_mother, gender_child1, gender_child2, gender_child3&gt;\n   family_id age_mother gender_child1 gender_child2 gender_child3 dob_child1\n       &lt;int&gt;      &lt;int&gt;         &lt;int&gt;         &lt;int&gt;         &lt;int&gt;     &lt;IDat&gt;\n1:         1         30             1             2            NA 1998-11-26\n2:         2         27             2            NA            NA 1996-06-22\n3:         3         26             2             2             1 2002-07-11\n4:         4         32             1             1             1 2004-10-10\n5:         5         29             2             1            NA 2000-12-05\n   dob_child2 dob_child3\n       &lt;IDat&gt;     &lt;IDat&gt;\n1: 2000-01-29       &lt;NA&gt;\n2:       &lt;NA&gt;       &lt;NA&gt;\n3: 2004-04-05 2007-09-02\n4: 2009-08-27 2012-07-21\n5: 2005-02-28       &lt;NA&gt;\n\n\n这里我们要还原的列为variable，而值放在dob列中，其他变量则统统用...来表示。我们看到这个函数的第二个参数必须要用方程式来表示。 下面我们同时进行两个转换（dob和gender）。知道单个转化如何进行，多个的话，只要在后面加一个变量即可。\n\ncolA = paste0(\"dob_child\", 1:3)\ncolB = paste0(\"gender_child\", 1:3)\nDT.m2 = melt(DT, measure.vars = list(colA, colB), value.name = c(\"dob\", \"gender\"))\nDT.m2\n\n    family_id age_mother variable        dob gender\n        &lt;int&gt;      &lt;int&gt;   &lt;fctr&gt;     &lt;IDat&gt;  &lt;int&gt;\n 1:         1         30        1 1998-11-26      1\n 2:         2         27        1 1996-06-22      2\n 3:         3         26        1 2002-07-11      2\n 4:         4         32        1 2004-10-10      1\n 5:         5         29        1 2000-12-05      2\n 6:         1         30        2 2000-01-29      2\n 7:         2         27        2       &lt;NA&gt;     NA\n 8:         3         26        2 2004-04-05      2\n 9:         4         32        2 2009-08-27      1\n10:         5         29        2 2005-02-28      1\n11:         1         30        3       &lt;NA&gt;     NA\n12:         2         27        3       &lt;NA&gt;     NA\n13:         3         26        3 2007-09-02      1\n14:         4         32        3 2012-07-21      1\n15:         5         29        3       &lt;NA&gt;     NA\n\n\n它的逆运算也非常简便：\n\nDT.c2 = dcast(DT.m2, family_id + age_mother ~ variable, value.var = c(\"dob\", \"gender\"))\nDT.c2\n\nKey: &lt;family_id, age_mother&gt;\n   family_id age_mother      dob_1      dob_2      dob_3 gender_1 gender_2\n       &lt;int&gt;      &lt;int&gt;     &lt;IDat&gt;     &lt;IDat&gt;     &lt;IDat&gt;    &lt;int&gt;    &lt;int&gt;\n1:         1         30 1998-11-26 2000-01-29       &lt;NA&gt;        1        2\n2:         2         27 1996-06-22       &lt;NA&gt;       &lt;NA&gt;        2       NA\n3:         3         26 2002-07-11 2004-04-05 2007-09-02        2        2\n4:         4         32 2004-10-10 2009-08-27 2012-07-21        1        1\n5:         5         29 2000-12-05 2005-02-28       &lt;NA&gt;        2        1\n   gender_3\n      &lt;int&gt;\n1:       NA\n2:       NA\n3:        1\n4:        1\n5:       NA\n\n\n有一个小问题就是，总是需要构造变量名称，然后放在colA和colB中，这样不是特别方便。我们可以用pattern函数来识别列名称的模式，从而简化这个工作：\n\nDT.m2 = melt(DT, measure.vars = patterns(\"^dob\", \"^gender\"), value.name = c(\"dob\", \"gender\"))\nDT.m2\n\n    family_id age_mother variable        dob gender\n        &lt;int&gt;      &lt;int&gt;   &lt;fctr&gt;     &lt;IDat&gt;  &lt;int&gt;\n 1:         1         30        1 1998-11-26      1\n 2:         2         27        1 1996-06-22      2\n 3:         3         26        1 2002-07-11      2\n 4:         4         32        1 2004-10-10      1\n 5:         5         29        1 2000-12-05      2\n 6:         1         30        2 2000-01-29      2\n 7:         2         27        2       &lt;NA&gt;     NA\n 8:         3         26        2 2004-04-05      2\n 9:         4         32        2 2009-08-27      1\n10:         5         29        2 2005-02-28      1\n11:         1         30        3       &lt;NA&gt;     NA\n12:         2         27        3       &lt;NA&gt;     NA\n13:         3         26        3 2007-09-02      1\n14:         4         32        3 2012-07-21      1\n15:         5         29        3       &lt;NA&gt;     NA\n\n\n这样一来，我们就把以“dob”开头和“gender”开头的变量分为两组，作为需要进行变换的列。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>快速整理：基于data.table的数据处理工具</span>"
    ]
  },
  {
    "objectID": "快速整理：基于data.table的数据处理工具.html#tidyfst数据处理",
    "href": "快速整理：基于data.table的数据处理工具.html#tidyfst数据处理",
    "title": "5  快速整理：基于data.table的数据处理工具",
    "section": "5.3 tidyfst数据处理",
    "text": "5.3 tidyfst数据处理\n\n5.3.1 tidyfst简介\ntidyfst是一个R语言包，结合了data.table的高性能和dplyr的简洁语法，提供高效易用的数据处理工具。它支持管道操作和丰富的操作函数，如过滤、选择、分组、汇总、排序和变形，使用户能够以简洁的方式进行复杂的数据处理。tidyfst的设计师法tidyverse，使用户能够无缝地与dplyr、ggplot2等包结合使用，从而利用tidyverse的丰富生态系统，同时享受data.table带来的性能提升。\ntidyfst 包利用了 data.table高效的数据处理能力，在处理大规模数据集时具有出色的表现。data.table的内部优化确保了数据操作的速度和内存效率，使得数据清洗、分析和导出更加便捷和高效。这特别适合处理大数据和复杂的分析任务，为大数据操作提供了便捷的高性能处理工具。\n总体而言，tidyfst包为数据科学家和分析师提供了一个强大而灵活的工具。它在简洁的语法和高效性能之间找到了良好的平衡，是处理大规模数据的理想选择。无论是数据清洗、分析还是结果导出，tidyfst 都能显著提升工作效率和效果。\n\n\n5.3.2 基本操作实现\n由于tidyfst包是基于data.table所构建的，因此很多操作与上一节讲述data.table的内容相似。在本部分只聚焦于tidyfst包的特色函数进行介绍，以让读者能够快速掌握大数据便捷操作的方法。\n\n5.3.2.1 单表操作\n所谓单表操作，也就是基于单个表格进行的数据操作，包括检索、筛选、排序、汇总等。下面，我们将会以tidyfst包作为主要工具，介绍如何在R中完成这些单表操作。\n\n5.3.2.1.1 检索\n检索是针对用户的需求，来提取总数据集一部分进行查阅，一般可以分为行检索与列检索。 在行检索中，一般是根据数据条目所在位置进行检索。比如我们想要查看iris数据表的第3行，可以这样操作：\n\nlibrary(pacman)\np_load(tidyfst)\n\niris %&gt;%  slice_dt(3)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.7         3.2          1.3         0.2  setosa\n\n\n如果想要查看多列，可以使用向量作为检索内容。\n\n# 查看第4和第6列\niris %&gt;% slice_dt(c(4,6))\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.6         3.1          1.5         0.2  setosa\n2:          5.4         3.9          1.7         0.4  setosa\n\n# 查看第4到第6列\niris %&gt;% slice_dt(4:6)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.6         3.1          1.5         0.2  setosa\n2:          5.0         3.6          1.4         0.2  setosa\n3:          5.4         3.9          1.7         0.4  setosa\n\n\n对列进行检索，则具有更多灵活的选择。首先，与行检索类似，可以根据列所在的位置，来对列进行检索。\n\n# 查看第1列\niris %&gt;% select_dt(1)\n\n     Sepal.Length\n            &lt;num&gt;\n  1:          5.1\n  2:          4.9\n ---             \n149:          6.2\n150:          5.9\n\n# 查看第1和第3列\niris %&gt;% select_dt(1,3)\n\n     Sepal.Length Petal.Length\n            &lt;num&gt;        &lt;num&gt;\n  1:          5.1          1.4\n  2:          4.9          1.4\n ---                          \n149:          6.2          5.4\n150:          5.9          5.1\n\n# 查看第1到3列\niris %&gt;% select_dt(1:3)\n\n     Sepal.Length Sepal.Width Petal.Length\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n  1:          5.1         3.5          1.4\n  2:          4.9         3.0          1.4\n ---                                      \n149:          6.2         3.4          5.4\n150:          5.9         3.0          5.1\n\n\n其次，还可以通过变量的名称，直接对其中的一个或多个变量进行检索。\n\n# 选择Species列\niris %&gt;% select_dt(Species)\n\n       Species\n        &lt;fctr&gt;\n  1:    setosa\n  2:    setosa\n ---          \n149: virginica\n150: virginica\n\n# 选择Sepal.Length和Sepal.Width列\niris %&gt;% select_dt(Sepal.Length,Sepal.Width)\n\n     Sepal.Length Sepal.Width\n            &lt;num&gt;       &lt;num&gt;\n  1:          5.1         3.5\n  2:          4.9         3.0\n ---                         \n149:          6.2         3.4\n150:          5.9         3.0\n\n# 选择从Sepal.Length列到Petal.Length列中的所有列\niris %&gt;% select_dt(Sepal.Length:Petal.Length)\n\n     Sepal.Length Sepal.Width Petal.Length\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n  1:          5.1         3.5          1.4\n  2:          4.9         3.0          1.4\n ---                                      \n149:          6.2         3.4          5.4\n150:          5.9         3.0          5.1\n\n\n如果要按照名称选择多列，还可以使用正则表达式的方法。比如我们要选择列名称中包含“Pe”的列，可以这样操作：\n\niris %&gt;% select_dt(\"Pe\")\n\n     Petal.Length Petal.Width\n            &lt;num&gt;       &lt;num&gt;\n  1:          1.4         0.2\n  2:          1.4         0.2\n ---                         \n149:          5.4         2.3\n150:          5.1         1.8\n\n\n与此同时，我们还可以根据数据类型来选择列，比如我们如果需要选择所有的因子变量，可以这样操作：\n\niris %&gt;% select_dt(is.factor)\n\n       Species\n        &lt;fctr&gt;\n  1:    setosa\n  2:    setosa\n ---          \n149: virginica\n150: virginica\n\n\n如果要去除某些列，在前面加负号（“-“）即可：\n\n# 去除Sepal.Length列\niris %&gt;% select_dt(-Sepal.Length)\n\n     Sepal.Width Petal.Length Petal.Width   Species\n           &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:         3.5          1.4         0.2    setosa\n  2:         3.0          1.4         0.2    setosa\n ---                                               \n149:         3.4          5.4         2.3 virginica\n150:         3.0          5.1         1.8 virginica\n\n# 去除第1列\niris %&gt;% select_dt(-1)\n\n     Sepal.Width Petal.Length Petal.Width   Species\n           &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:         3.5          1.4         0.2    setosa\n  2:         3.0          1.4         0.2    setosa\n ---                                               \n149:         3.4          5.4         2.3 virginica\n150:         3.0          5.1         1.8 virginica\n\n# 去除列名称包含“Se”的列\niris %&gt;% select_dt(-\"Se\")\n\n     Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          1.4         0.2    setosa\n  2:          1.4         0.2    setosa\n ---                                   \n149:          5.4         2.3 virginica\n150:          5.1         1.8 virginica\n\n# 去除数据类型为因子型的列\niris %&gt;% select_dt(-is.factor)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;\n  1:          5.1         3.5          1.4         0.2\n  2:          4.9         3.0          1.4         0.2\n ---                                                  \n149:          6.2         3.4          5.4         2.3\n150:          5.9         3.0          5.1         1.8\n\n\n\n\n5.3.2.1.2 筛选\n筛选操作就是要把数据框中符合条件的行筛选出来，在tidyfst包中可以使用filter_dt函数进行实现。比如我们要筛选iris数据框中Sepal.Length列大于7的条目，可以这样操作：\n\niris %&gt;% filter_dt(Sepal.Length &gt; 7)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n           &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n 1:          7.1         3.0          5.9         2.1 virginica\n 2:          7.6         3.0          6.6         2.1 virginica\n 3:          7.3         2.9          6.3         1.8 virginica\n 4:          7.2         3.6          6.1         2.5 virginica\n 5:          7.7         3.8          6.7         2.2 virginica\n 6:          7.7         2.6          6.9         2.3 virginica\n 7:          7.7         2.8          6.7         2.0 virginica\n 8:          7.2         3.2          6.0         1.8 virginica\n 9:          7.2         3.0          5.8         1.6 virginica\n10:          7.4         2.8          6.1         1.9 virginica\n11:          7.9         3.8          6.4         2.0 virginica\n12:          7.7         3.0          6.1         2.3 virginica\n\n\n在筛选条件中，可以使用且（&）、或（|）和非（!）三种逻辑运算符，来表达复杂的条件关系。举个例子，比如我们想要筛选Sepal.Length大于7且Sepal.Width大于3的条目,可以这样操作：\n\niris %&gt;% filter_dt(Sepal.Length &gt; 7 & Sepal.Width &gt; 3)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n1:          7.2         3.6          6.1         2.5 virginica\n2:          7.7         3.8          6.7         2.2 virginica\n3:          7.2         3.2          6.0         1.8 virginica\n4:          7.9         3.8          6.4         2.0 virginica\n\n\n\n\n5.3.2.1.3 排序\n在数据框的操作中，可以根据一个或多个变量对行进行排序。tidyfst包中，可以利用arrange_dt函数对排序进行实现。比如，如果我们想要根据Sepal.Length进行排序，可以这样操作：\n\niris %&gt;% arrange_dt(Sepal.Length)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          4.3         3.0          1.1         0.1    setosa\n  2:          4.4         2.9          1.4         0.2    setosa\n ---                                                            \n149:          7.7         3.0          6.1         2.3 virginica\n150:          7.9         3.8          6.4         2.0 virginica\n\n\n从结果中我们可以获知，默认的排序是升序排列。如果需要降序排列，那么需要在变量前面加上负号：\n\niris %&gt;% arrange_dt(-Sepal.Length)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          7.9         3.8          6.4         2.0 virginica\n  2:          7.7         3.8          6.7         2.2 virginica\n ---                                                            \n149:          4.4         3.2          1.3         0.2    setosa\n150:          4.3         3.0          1.1         0.1    setosa\n\n\n同时，可以加入多个变量，从而在第一个变量相同的情况下，根据第二个变量进行排列：\n\niris %&gt;% arrange_dt(Sepal.Length,Sepal.Width) %&gt;% \n  head() # 只观察结果的前6行\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.3         3.0          1.1         0.1  setosa\n2:          4.4         2.9          1.4         0.2  setosa\n3:          4.4         3.0          1.3         0.2  setosa\n4:          4.4         3.2          1.3         0.2  setosa\n5:          4.5         2.3          1.3         0.3  setosa\n6:          4.6         3.1          1.5         0.2  setosa\n\n\n我们可以看到，当Sepal.Length都等于4.4的时候，条目是根据Sepal.Width进行升序排列的。\n\n\n5.3.2.1.4 更新\n本节提到的更新，具体是指对某一列进行数据的更新，或者通过计算获得一个新的数据列。在tidyfst包中，可以使用mutate_dt函数来对列进行更新。举例来说，比如我们想要让iris数据框中的Sepal.Length列全部加1，可以这样操作：\n\niris %&gt;% \n  mutate_dt(Sepal.Length = Sepal.Length + 1)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;\n  1:          6.1         3.5          1.4         0.2    setosa\n  2:          5.9         3.0          1.4         0.2    setosa\n ---                                                            \n149:          7.2         3.4          5.4         2.3 virginica\n150:          6.9         3.0          5.1         1.8 virginica\n\n\n我们也可以新增一列，比如我们想要新增一个名称为one的列，这一列的数据为常数1。\n\niris %&gt;% mutate_dt(one = 1)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species   one\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt; &lt;num&gt;\n  1:          5.1         3.5          1.4         0.2    setosa     1\n  2:          4.9         3.0          1.4         0.2    setosa     1\n ---                                                                  \n149:          6.2         3.4          5.4         2.3 virginica     1\n150:          5.9         3.0          5.1         1.8 virginica     1\n\n\n如果我们在更新之后，只想保留更新的那些列，可以使用transmute_dt函数。\n\niris %&gt;% \n  transmute_dt(one = 1,\n               Sepal.Length = Sepal.Length + 1)\n\n       one Sepal.Length\n     &lt;num&gt;        &lt;num&gt;\n  1:     1          6.1\n  2:     1          5.9\n ---                   \n149:     1          7.2\n150:     1          6.9\n\n\n上面的例子中，我们就仅保留了更新后的两列。 如果需要分组更新，可以使用by参数定义分组信息。比如，我们想要把iris数据框中，根据物种进行分组，然后把Sepal.Length的平均值求出来，附在名为“sp_avg_sl”列中。操作方法如下：\n\niris %&gt;% mutate_dt(sp_avg_sl = mean(Sepal.Length),by = Species)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species sp_avg_sl\n            &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;    &lt;fctr&gt;     &lt;num&gt;\n  1:          5.1         3.5          1.4         0.2    setosa     5.006\n  2:          4.9         3.0          1.4         0.2    setosa     5.006\n ---                                                                      \n149:          6.2         3.4          5.4         2.3 virginica     6.588\n150:          5.9         3.0          5.1         1.8 virginica     6.588\n\n\n\n\n5.3.2.1.5 汇总\n汇总，即对一系列数据进行概括的数据操作。求和、求均值、最大值、最小值，均可以视为汇总操作。比如，我们想求iris数据框中Sepal.Length的均值，可以使用tidyfst包的summarise_dt函数。\n\niris %&gt;% summarise_dt(avg = mean(Sepal.Length))\n\n        avg\n      &lt;num&gt;\n1: 5.843333\n\n\n在上面的操作中，我们把最终输出的列名称设定为“avg”。在实际应用中，我们往往需要进行分组汇总操作，这可以通过设定by参数进行实现。比如我们想知道每个物种Sepal.Length的均值，可以这样操作：\n\niris %&gt;% summarise_dt(avg = mean(Sepal.Length),by = Species)\n\n      Species   avg\n       &lt;fctr&gt; &lt;num&gt;\n1:     setosa 5.006\n2: versicolor 5.936\n3:  virginica 6.588\n\n\n\n\n\n5.3.2.2 多表操作\n在实际工作中，很多时候我们不仅是要对一个表格进行操作，而是要进行多个表格数据的整合归并。在tidyfst的工作流中，有三种处理多表操作的模式，包括更新型连接、过滤型连接和集合运算操作，下面将一一进行介绍。\n\n5.3.2.2.1 更新型连接\n更新型连接（Mutating joins）是根据两个表格中的共有列进行匹配，然后完成合并的过程，可以分为内连接、外连接、左连接和右连接四种。下面，我们将会构造一个数据集来对四种连接进行说明。\n\nlibrary(pacman)\np_load(tidyfst)\n\ndf1 = data.frame(CustomerId = c(1:6), Product = c(rep(\"洗衣机\", 3), rep(\"微波炉\", 3)))\ndf1\n\n  CustomerId Product\n1          1  洗衣机\n2          2  洗衣机\n3          3  洗衣机\n4          4  微波炉\n5          5  微波炉\n6          6  微波炉\n\ndf2 = data.frame(CustomerId = c(2, 4, 6), Province = c(rep(\"广东\", 2), rep(\"北京\", 1)))\ndf2\n\n  CustomerId Province\n1          2     广东\n2          4     广东\n3          6     北京\n\n\n在上面的代码中，我们构造了两个数据框（df1和df2）。其中，df1中有消费者的ID号和他们买了什么产品；df2中则包含了消费者ID号和他们所在的地点（省份）。 内连接又称为自然连接，是根据两个表格某一列或多列共有的部分，进行连接的过程。下面，我们来对之前构造的两个表格进行内连接，这可以使用inner_join_dt函数完成。\n\ndf1 %&gt;% inner_join_dt(df2)\n\nJoining by: CustomerId\n\n\nKey: &lt;CustomerId&gt;\n   CustomerId Product Province\n        &lt;int&gt;  &lt;char&gt;   &lt;char&gt;\n1:          2  洗衣机     广东\n2:          4  微波炉     广东\n3:          6  微波炉     北京\n\n\n通过上面的结果，我们可以看到，如果没有设定连接的列，那么inner_join_dt会自动识别两个数据框中的同名列进行匹配。在内连接中，会找到df1和df2同名列CustomerId中完全匹配的条目，进行连接。如果希望直接设定合并的列，可以使用by参数来特殊指定。\n\ndf1 %&gt;% inner_join_dt(df2,by = \"CustomerId\")\n\nKey: &lt;CustomerId&gt;\n   CustomerId Product Province\n        &lt;int&gt;  &lt;char&gt;   &lt;char&gt;\n1:          2  洗衣机     广东\n2:          4  微波炉     广东\n3:          6  微波炉     北京\n\n\n在进行内连接的时候，我们可以看到，如果不匹配的条目，会全部消失。如果想要保留这些条目，可以使用全连接，它会保留两个表格中所有的条目。而没有数值的地方，会自动填充缺失值。\n\ndf1 %&gt;% full_join_dt(df2)\n\nJoining by: CustomerId\n\n\nKey: &lt;CustomerId&gt;\n   CustomerId Product Province\n        &lt;int&gt;  &lt;char&gt;   &lt;char&gt;\n1:          1  洗衣机     &lt;NA&gt;\n2:          2  洗衣机     广东\n3:          3  洗衣机     &lt;NA&gt;\n4:          4  微波炉     广东\n5:          5  微波炉     &lt;NA&gt;\n6:          6  微波炉     北京\n\n\n在上面的结果中，我们可以看到，在df2中没有消费者1、3、5的地区数据，因此填充了缺失值NA。 左连接和右连接是互为逆运算的两个操作，左连接会保留左边数据框的所有信息，但是对于右边的数据框，则只有匹配的数据得以保留，不匹配的部分会填入缺失值。下面我们来进行演示：\n\ndf1 %&gt;% left_join_dt(df2)\n\nJoining by: CustomerId\n\n\nKey: &lt;CustomerId&gt;\n   CustomerId Product Province\n        &lt;int&gt;  &lt;char&gt;   &lt;char&gt;\n1:          1  洗衣机     &lt;NA&gt;\n2:          2  洗衣机     广东\n3:          3  洗衣机     &lt;NA&gt;\n4:          4  微波炉     广东\n5:          5  微波炉     &lt;NA&gt;\n6:          6  微波炉     北京\n\ndf1 %&gt;% right_join_dt(df2)\n\nJoining by: CustomerId\n\n\nKey: &lt;CustomerId&gt;\n   CustomerId Product Province\n        &lt;int&gt;  &lt;char&gt;   &lt;char&gt;\n1:          2  洗衣机     广东\n2:          4  微波炉     广东\n3:          6  微波炉     北京\n\n\n有的时候，我们需要根据两个或以上的列进行连接，那么将需要通过设置by参数来完成。下面我们举个例子：\n\nworkers = fread(\"\n    name company\n    Nick Acme\n    John Ajax\n    Daniela Ajax\n\")\n\npositions = fread(\"\n    name position\n    John designer\n    Daniela engineer\n    Cathie manager\n\")\n\npositions2 = setNames(positions, c(\"worker\", \"position\")) \n\nworkers\n\n      name company\n    &lt;char&gt;  &lt;char&gt;\n1:    Nick    Acme\n2:    John    Ajax\n3: Daniela    Ajax\n\npositions2\n\n    worker position\n    &lt;char&gt;   &lt;char&gt;\n1:    John designer\n2: Daniela engineer\n3:  Cathie  manager\n\n\n在上面的代码中，我们获得了workers和position2两个数据框。其中，workers数据框中的name为工人名称，而position2数据框中的worker列为数据名称，因此两者合并的时候需要根据名称不同的列进行匹配：\n\nworkers %&gt;% inner_join_dt(positions2, by = c(\"name\" = \"worker\"))\n\nKey: &lt;name&gt;\n      name company position\n    &lt;char&gt;  &lt;char&gt;   &lt;char&gt;\n1: Daniela    Ajax engineer\n2:    John    Ajax designer\n\n\n得到的结果会保留第一个出现的数据框的名称，即workers数据框中的name，而第二个数据框position2的worker列则会消失。\n\n\n5.3.2.2.2 过滤型连接\n过滤型连接（Filtering joins）是根据两个表格中是否有匹配内容来决定一个表格中的观测是否得以保留的操作，在tidyfst包中可以使用anti_join_dt和semi_join_dt实现。其中，anti_join_dt会保留第一个表格有而第二个表格中没有匹配的内容，而semi_join_dt则会保留第一个表格有且第二个表格也有的内容。但是，第二个表格中非匹配列的其他数据不会并入生成表格中。\n\nworkers %&gt;% anti_join_dt(positions)\n\nJoining by: name\n\n\n     name company\n   &lt;char&gt;  &lt;char&gt;\n1:   Nick    Acme\n\nworkers %&gt;% semi_join_dt(positions)\n\nJoining by: name\n\n\n      name company\n    &lt;char&gt;  &lt;char&gt;\n1:    John    Ajax\n2: Daniela    Ajax\n\n\n\n\n5.3.2.2.3 集合运算操作\n在R中，每个向量都可以视为一个集合，基本包提供了intersect/union/setdiff来求集合的交集、并集和补集，并可以使用setequal函数来查看两个向量是否全等。我们可以做一个简单的演示：\n\nx = 1:4\ny = 3:6\n\nunion(x, y) #并集\n\n[1] 1 2 3 4 5 6\n\nintersect(x, y) #交集\n\n[1] 3 4\n\nsetdiff(x, y) #补集，x有而y没有部分\n\n[1] 1 2\n\nsetdiff(y, x) #补集，y有而x没有部分\n\n[1] 5 6\n\nsetequal(x, y) # x与y是否相等\n\n[1] FALSE\n\n\n而在tidyfst中，利用了data.table的集合运算函数，可以直接对数据框进行对应的集合运算操作。需要注意的是，所求数据框需要有相同的列名称。下面我们利用iris的前3列来做一个简单的演示：\n\nx = iris[1:2,]\ny = iris[2:3,] \n\nunion_dt(x, y) #并集\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n2:          4.9         3.0          1.4         0.2  setosa\n3:          4.7         3.2          1.3         0.2  setosa\n\nintersect_dt(x, y) #交集\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.9           3          1.4         0.2  setosa\n\nsetdiff_dt(x, y) #补集，x有而y没有部分\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n\nsetdiff_dt(y, x) #补集，y有而x没有部分\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          4.7         3.2          1.3         0.2  setosa\n\nsetequal_dt(x, y) # x与y是否相等\n\n[1] FALSE\n\n\n这些函数都有all参数，可以调节其对重复值的处理。比如，如果我们取并集的时候，不需要去重，那么可以设置“all = TRUE”。\n\nunion_dt(x,y,all = TRUE)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;num&gt;       &lt;num&gt;        &lt;num&gt;       &lt;num&gt;  &lt;fctr&gt;\n1:          5.1         3.5          1.4         0.2  setosa\n2:          4.9         3.0          1.4         0.2  setosa\n3:          4.9         3.0          1.4         0.2  setosa\n4:          4.7         3.2          1.3         0.2  setosa\n\n\n\n\n\n\n5.3.3 便捷工具介绍\n\n5.3.3.1 缺失值处理\n在数据处理的时候，难免会遇到数据集包含缺失值的情况。这有可能是因为人工失误引起的，也可能是系统故障导致的。根据缺失值分布的特征，通常可以把缺失情况分为三类：完全随机缺失（missing completely at random,MCAR）、随机缺失（missing at random,MAR）、非随机缺失（missing not at random,MNAR）。我们常常需要根据数据缺失的分布特征，来推断数据缺失的真实原因，从而考虑如何来处理这些缺失值。一般而言，缺失值的处理有三种手段：1、删除；2、替换；3、插值。下面，我们将会介绍如何利用tidyfst包在R中实现这3种缺失值处理。\n\n5.3.3.1.1 缺失值删除\n删除缺失值可能是缺失值处理中最为简单粗暴的方法，在样本量非常大的时候，直接删除往往对结果影响不大，而实现的成本又较低。在tidyfst包中，有三个函数能够对包含缺失值的数据进行直接删除：\n\ndrop_na_dt：行删除操作，如果一列或多列中包含任意缺失值，将整个观测删除。\ndelete_na_cols：列删除操作，如果数据框中任意列的缺失值比例或数量超过一个阈值，将整个列删除掉。\ndelete_na_rows：行删除操作，如果数据框中任意行的缺失值比例或数量超过一个阈值，将整个列删除掉。\n下面，我们将举个例子对上述操作进行演示。首先我们要构建一个缺失值数据框。\n\nlibrary(pacman)\np_load(tidyfst)\n\ndf &lt;- data.frame(col1 = c(1:3, NA),\n                 col2 = c(\"this\", NA,NA, \"text\"), \n                 col3 = c(TRUE, FALSE, TRUE, TRUE), \n                 col4 = c(NA, NA, 3.2, NA))\n\ndf\n\n  col1 col2  col3 col4\n1    1 this  TRUE   NA\n2    2 &lt;NA&gt; FALSE   NA\n3    3 &lt;NA&gt;  TRUE  3.2\n4   NA text  TRUE   NA\n\n\n所构造的数据框中，第一、二、三、四列分别有1、2、0和3个缺失值。我们如果想要删除col2中包含缺失值的条目，可以使用drop_na_dt函数。\n\ndf %&gt;% drop_na_dt(col2)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:    NA   text   TRUE    NA\n\n\n如果想要删除缺失值大于等于2个或缺失比例大于等于50%的列，则可以这样操作：\n\n# 删除缺失值大于等于2个的列\ndf %&gt;% delete_na_cols(n = 2)\n\n    col1   col3\n   &lt;int&gt; &lt;lgcl&gt;\n1:     1   TRUE\n2:     2  FALSE\n3:     3   TRUE\n4:    NA   TRUE\n\n# 删除缺失比例大于等于50%的列\ndf %&gt;% delete_na_cols(prop = 0.5) \n\n    col1   col3\n   &lt;int&gt; &lt;lgcl&gt;\n1:     1   TRUE\n2:     2  FALSE\n3:     3   TRUE\n4:    NA   TRUE\n\n\n如果想要删除缺失值大于等于2个或缺失比例大于等于50%的行，则可以这样操作：\n\n# 删除缺失值大于等于2个的行\ndf %&gt;% delete_na_rows(n = 2)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:     3   &lt;NA&gt;   TRUE   3.2\n\n# 删除缺失比例大于等于50%的行\ndf %&gt;% delete_na_rows(prop = 0.5)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:     3   &lt;NA&gt;   TRUE   3.2\n\n\n\n\n\n5.3.3.1.2 缺失值替换\n缺失值替换就是把缺失的部分用指定数据进行替代的过程，在tidyfst中可以使用replace_na_dt进行实现。比如，我们要将上面所构造数据框的col1列缺失值替换为-99，可以这样操作：\n\ndf %&gt;% \n  replace_na_dt(col1,to = -99)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:     2   &lt;NA&gt;  FALSE    NA\n3:     3   &lt;NA&gt;   TRUE   3.2\n4:   -99   text   TRUE    NA\n\n\n也可以同时对col1和col4同时进行这项操作：\n\ndf %&gt;% \n  replace_na_dt(col1,col4,to = -99)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE -99.0\n2:     2   &lt;NA&gt;  FALSE -99.0\n3:     3   &lt;NA&gt;   TRUE   3.2\n4:   -99   text   TRUE -99.0\n\n\n如果不设定替换列，默认替换所有的列。但需要注意的是，每一个列的类型都不一样，因此在替换的时候需要保证替换列数据类型的一致性。\n\ndf %&gt;% \n  mutate_vars(.func = as.character) %&gt;% #将有所列转化为字符型\n  replace_na_dt(to = \"missing\") #将缺失值替换为“missing”\n\n      col1    col2   col3    col4\n    &lt;char&gt;  &lt;char&gt; &lt;char&gt;  &lt;char&gt;\n1:       1    this   TRUE missing\n2:       2 missing  FALSE missing\n3:       3 missing   TRUE     3.2\n4: missing    text   TRUE missing\n\n\n\n\n5.3.3.1.3 缺失值插值\n与替换不同，缺失值的插值需要根据列中数据的关系来对要插入的值进行一定的计算，然后再填入到缺失的部分。在tidyfst包中，可以完成插值的函数包括：\n\nfill_na_dt：把缺失值替换为其最临近的上一个或下一个观测值\nimpute_dt：根据列汇总数据（平均值、中位数、众数或其他）来对缺失值进行插补\n我们先对fill_na_dt函数进行讲解，它的原理非常简单，我们直接进行演示。\n\n# 对col2向下进行插补\ndf %&gt;% fill_na_dt(col2)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:     2   this  FALSE    NA\n3:     3   this   TRUE   3.2\n4:    NA   text   TRUE    NA\n\n# 对col2向上进行插补\ndf %&gt;% fill_na_dt(col2,direction = \"up\")\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:     2   text  FALSE    NA\n3:     3   text   TRUE   3.2\n4:    NA   text   TRUE    NA\n\n# 对数值型列进行向下插补\ndf %&gt;% fill_na_dt(is.numeric)\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE    NA\n2:     2   &lt;NA&gt;  FALSE    NA\n3:     3   &lt;NA&gt;   TRUE   3.2\n4:     3   text   TRUE   3.2\n\n# 对所有列进行向上插补\ndf %&gt;% fill_na_dt(direction = \"up\")\n\n    col1   col2   col3  col4\n   &lt;int&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE   3.2\n2:     2   text  FALSE   3.2\n3:     3   text   TRUE   3.2\n4:    NA   text   TRUE    NA\n\n\n有的时候，我们会尝试用均值来对数值型的变量进行插值，那么我们可以使用impute_dt函数进行操作：\n\ndf %&gt;% impute_dt(is.numeric,.func = \"mean\")\n\n    col1   col2   col3  col4\n   &lt;num&gt; &lt;char&gt; &lt;lgcl&gt; &lt;num&gt;\n1:     1   this   TRUE   3.2\n2:     2   &lt;NA&gt;  FALSE   3.2\n3:     3   &lt;NA&gt;   TRUE   3.2\n4:     2   text   TRUE   3.2\n\n\n如果把.func设置为“mode”和“median”，我们就可以利用其众数或中位数进行插值。\n\n\n\n\n5.3.3.2 长宽数据转换\n表格的长宽转换是一个经典的数据操作，它可以自由地改变二维表的结构。比如我们之前熟悉的iris数据框，其实就可以转化为任意的其他结构。举个例子，我们给每一朵花都进行编号，然后直接转为长表格式。\n\nlibrary(pacman)\np_load(tidyfst)\n\niris %&gt;% \n  mutate_vars(.func = as.character) %&gt;%  #所有数据转化为字符型\n  mutate_dt(id = 1:.N) %&gt;%  # 给所有花进行编号\n  longer_dt(id) # 以编号id作为分组变量，转化为长表\n\n        id         name     value\n     &lt;int&gt;       &lt;fctr&gt;    &lt;char&gt;\n  1:     1 Sepal.Length       5.1\n  2:     2 Sepal.Length       4.9\n ---                             \n749:   149      Species virginica\n750:   150      Species virginica\n\n\n在输出的结果中，我们可以看到三列，分别为id、name和value。以第一行为例，它表示id为1的花朵的Sepal.Length属性值为5.1。这种长宽变换能够让我们自如地对数据框结构进行重塑，从而获得满足分析要求的数据结构。\n\n5.3.3.2.1 宽表转长表\n在tidyfst包中，可以使用longer_dt函数来把宽表转化为长表，需要定义的核心参数是数据框和分组列。分组列就是不参与长宽变换的列，而其他列名称将会统统聚合起来成为一列。下面我们进行一个简单的演示。首先，我们先进行数据准备：\n\nstocks = data.frame(\n  time = as.Date('2009-01-01') + 0:9,\n  X = rnorm(10, 0, 1),\n  Y = rnorm(10, 0, 2),\n  Z = rnorm(10, 0, 4)\n)\n\nstocks\n\n         time           X          Y          Z\n1  2009-01-01 -0.41951421 -4.0696697  1.1862528\n2  2009-01-02 -1.95653982  1.2851513 -4.3121715\n3  2009-01-03 -0.18057417  0.2530406  1.4529063\n4  2009-01-04  1.53254389  2.5200584  1.2337196\n5  2009-01-05 -0.40798547 -3.1497638 -2.6830911\n6  2009-01-06  2.09614174  4.4814026  2.1697771\n7  2009-01-07 -0.39973265 -0.4680301  1.6492286\n8  2009-01-08  0.33918122 -2.1085464  1.9916131\n9  2009-01-09  0.25558423  2.7826645  0.3446163\n10 2009-01-10 -0.09429387 -0.9727120 -1.4884806\n\n\n观察数据，我们知道这是一个10行4列的数据框，一列为时间time，其余三列为数值列。现在，我们要把它转化为长表。\n\nstocks %&gt;% \n  longer_dt(time) -&gt; longer_table\n\nlonger_table\n\n          time   name       value\n        &lt;Date&gt; &lt;fctr&gt;       &lt;num&gt;\n 1: 2009-01-01      X -0.41951421\n 2: 2009-01-02      X -1.95653982\n 3: 2009-01-03      X -0.18057417\n 4: 2009-01-04      X  1.53254389\n 5: 2009-01-05      X -0.40798547\n 6: 2009-01-06      X  2.09614174\n 7: 2009-01-07      X -0.39973265\n 8: 2009-01-08      X  0.33918122\n 9: 2009-01-09      X  0.25558423\n10: 2009-01-10      X -0.09429387\n11: 2009-01-01      Y -4.06966965\n12: 2009-01-02      Y  1.28515135\n13: 2009-01-03      Y  0.25304059\n14: 2009-01-04      Y  2.52005836\n15: 2009-01-05      Y -3.14976381\n16: 2009-01-06      Y  4.48140262\n17: 2009-01-07      Y -0.46803013\n18: 2009-01-08      Y -2.10854639\n19: 2009-01-09      Y  2.78266455\n20: 2009-01-10      Y -0.97271195\n21: 2009-01-01      Z  1.18625279\n22: 2009-01-02      Z -4.31217152\n23: 2009-01-03      Z  1.45290630\n24: 2009-01-04      Z  1.23371964\n25: 2009-01-05      Z -2.68309108\n26: 2009-01-06      Z  2.16977710\n27: 2009-01-07      Z  1.64922855\n28: 2009-01-08      Z  1.99161315\n29: 2009-01-09      Z  0.34461627\n30: 2009-01-10      Z -1.48848060\n          time   name       value\n\n\n可以发现，列名称X/Y/Z都在name列中，其值则在value列中。这些列名称可以通过更改name和value参数重新被定义。\n\nstocks %&gt;% \n  longer_dt(time,\n            name = \"var\",\n            value = \"val\") \n\n          time    var         val\n        &lt;Date&gt; &lt;fctr&gt;       &lt;num&gt;\n 1: 2009-01-01      X -0.41951421\n 2: 2009-01-02      X -1.95653982\n 3: 2009-01-03      X -0.18057417\n 4: 2009-01-04      X  1.53254389\n 5: 2009-01-05      X -0.40798547\n 6: 2009-01-06      X  2.09614174\n 7: 2009-01-07      X -0.39973265\n 8: 2009-01-08      X  0.33918122\n 9: 2009-01-09      X  0.25558423\n10: 2009-01-10      X -0.09429387\n11: 2009-01-01      Y -4.06966965\n12: 2009-01-02      Y  1.28515135\n13: 2009-01-03      Y  0.25304059\n14: 2009-01-04      Y  2.52005836\n15: 2009-01-05      Y -3.14976381\n16: 2009-01-06      Y  4.48140262\n17: 2009-01-07      Y -0.46803013\n18: 2009-01-08      Y -2.10854639\n19: 2009-01-09      Y  2.78266455\n20: 2009-01-10      Y -0.97271195\n21: 2009-01-01      Z  1.18625279\n22: 2009-01-02      Z -4.31217152\n23: 2009-01-03      Z  1.45290630\n24: 2009-01-04      Z  1.23371964\n25: 2009-01-05      Z -2.68309108\n26: 2009-01-06      Z  2.16977710\n27: 2009-01-07      Z  1.64922855\n28: 2009-01-08      Z  1.99161315\n29: 2009-01-09      Z  0.34461627\n30: 2009-01-10      Z -1.48848060\n          time    var         val\n\n\n\n\n5.3.3.2.2 长表转宽表\n长表转为宽表是宽表转长表的逆运算，因此需要定义的核心参数也有相仿之处，需要知道数据框、分组列的信息，同时需要知道名称列（name）和数值列（value）分别来自哪里。以上面生成的longer_table为例，我们尝试把它进行还原。\n\nlonger_table\n\n          time   name       value\n        &lt;Date&gt; &lt;fctr&gt;       &lt;num&gt;\n 1: 2009-01-01      X -0.41951421\n 2: 2009-01-02      X -1.95653982\n 3: 2009-01-03      X -0.18057417\n 4: 2009-01-04      X  1.53254389\n 5: 2009-01-05      X -0.40798547\n 6: 2009-01-06      X  2.09614174\n 7: 2009-01-07      X -0.39973265\n 8: 2009-01-08      X  0.33918122\n 9: 2009-01-09      X  0.25558423\n10: 2009-01-10      X -0.09429387\n11: 2009-01-01      Y -4.06966965\n12: 2009-01-02      Y  1.28515135\n13: 2009-01-03      Y  0.25304059\n14: 2009-01-04      Y  2.52005836\n15: 2009-01-05      Y -3.14976381\n16: 2009-01-06      Y  4.48140262\n17: 2009-01-07      Y -0.46803013\n18: 2009-01-08      Y -2.10854639\n19: 2009-01-09      Y  2.78266455\n20: 2009-01-10      Y -0.97271195\n21: 2009-01-01      Z  1.18625279\n22: 2009-01-02      Z -4.31217152\n23: 2009-01-03      Z  1.45290630\n24: 2009-01-04      Z  1.23371964\n25: 2009-01-05      Z -2.68309108\n26: 2009-01-06      Z  2.16977710\n27: 2009-01-07      Z  1.64922855\n28: 2009-01-08      Z  1.99161315\n29: 2009-01-09      Z  0.34461627\n30: 2009-01-10      Z -1.48848060\n          time   name       value\n\nlonger_table %&gt;% \n  wider_dt(time,name = \"name\",value = \"value\")\n\nKey: &lt;time&gt;\n          time           X          Y          Z\n        &lt;Date&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1: 2009-01-01 -0.41951421 -4.0696697  1.1862528\n 2: 2009-01-02 -1.95653982  1.2851513 -4.3121715\n 3: 2009-01-03 -0.18057417  0.2530406  1.4529063\n 4: 2009-01-04  1.53254389  2.5200584  1.2337196\n 5: 2009-01-05 -0.40798547 -3.1497638 -2.6830911\n 6: 2009-01-06  2.09614174  4.4814026  2.1697771\n 7: 2009-01-07 -0.39973265 -0.4680301  1.6492286\n 8: 2009-01-08  0.33918122 -2.1085464  1.9916131\n 9: 2009-01-09  0.25558423  2.7826645  0.3446163\n10: 2009-01-10 -0.09429387 -0.9727120 -1.4884806\n\n\n在上面的操作中，我们把time定义为分组列，然后以字符形式来定义哪一列是名称列，哪一列是数值列。在宽表转长表的时候，name和value如果不自定义，就会自动给名称列命名为“name”，给数值列命名为“value”；但是在长表转为宽表的时候，则必须手动进行定义，否则计算机无法自动识别。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>快速整理：基于data.table的数据处理工具</span>"
    ]
  },
  {
    "objectID": "快速整理：基于data.table的数据处理工具.html#小结",
    "href": "快速整理：基于data.table的数据处理工具.html#小结",
    "title": "5  快速整理：基于data.table的数据处理工具",
    "section": "5.4 小结",
    "text": "5.4 小结\n本章聚焦于如何利用data.table以及衍生工具包tidyfst来对大规模数据进行整理。通过学习，我们知道data.table的数据处理效率非常高，但是语法结构相对来说比较复杂，因此有时候难以记忆，需要多花时间来熟悉。另一方面，tidyfst包底层用data.table来构建，而语法结构相对来说比较简单，很多复杂的操作也被集成在该工具包中，可以简单地进行调用。利用这些工具，我们可以在R环境中对海量数据高效地进行整理和清洗。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>快速整理：基于data.table的数据处理工具</span>"
    ]
  },
  {
    "objectID": "快速绘图：大数据可视化工具.html",
    "href": "快速绘图：大数据可视化工具.html",
    "title": "6  快速绘图：大数据可视化工具",
    "section": "",
    "text": "6.1 可视化基本流程\n有效探索大数据、对变量进行解释，并将大数据分析结果传达给不同受众的最有效方法之一是数据可视化。当我们处理大数据时，数据可视化可以在许多方面为我们提供帮助，例如：\n进行有效的可视化需要明确分析目标，然后再进行设计。有时我们可能已经知道有关数据的一些问题的答案；其他情况下，我们可能希望进一步探索和了解数据，以便为下一步的数据分析提供更好的见解。在此过程中，我们需要考虑许多可视化要素，例如要使用的变量类型、坐标轴、标签、图例、颜色等等。此外，如果我们打算向特定受众展示可视化结果，那么我们还需要考虑可视化对目标受众的可用性和可解释性。\n有效的数据可视化通常包括以下步骤：\n图6.1展示了一些基于变量类型和可视化目的的数据可视化建议。在R中，几乎所有这些可视化都可以很容易地实现，尽管有时对这些可视化所需数据的整理可能相当繁琐。\nFigure 6.1: 面向不同目标的可视化解决方案",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>快速绘图：大数据可视化工具</span>"
    ]
  },
  {
    "objectID": "快速绘图：大数据可视化工具.html#可视化基本流程",
    "href": "快速绘图：大数据可视化工具.html#可视化基本流程",
    "title": "6  快速绘图：大数据可视化工具",
    "section": "",
    "text": "了解变量的分布特征\n检测数据录入问题\n识别数据中的异常值\n了解变量之间的关系\n选择适合的数据分析变量（即特征提取）\n检查预测模型的结果（例如准确性和过拟合）\n向不同的受众报告结果\n\n\n\n\n明确目标：确定数据可视化的目标，如探索数据、揭示关系等\n准备数据：包括数据的整理、清洗与转换等\n工具选择：根据数据可视化的目标选择合适的可视化工具\n图形绘制：生成可视化，即对图形进行绘制\n结果揭示：解释可视化中的信息，并将其展示给目标受众",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>快速绘图：大数据可视化工具</span>"
    ]
  },
  {
    "objectID": "快速绘图：大数据可视化工具.html#大数据可视化面临的挑战",
    "href": "快速绘图：大数据可视化工具.html#大数据可视化面临的挑战",
    "title": "6  快速绘图：大数据可视化工具",
    "section": "6.2 大数据可视化面临的挑战",
    "text": "6.2 大数据可视化面临的挑战\n对大数据集中的某些特征和模式进行可视化主要面临两个挑战：大N问题和大P问题（示意图见图6.2）。大N问题是指一个数据集包含大量的观测值（行），以至于无法使用标准的数据分析技术在计算机上处理。可视化中的大N问题是指，根据绘图的类型，绘制包含大量观测值的原始数据可能需要很长时间，并生成很大的文件。大P问题则是描述一个数据集的变量（列）接近甚至多于观测值，使得使用传统分析技术寻找一个良好的预测模型变得困难或难以实现。在可视化中，由于在图中显示的数据量巨大，变量之间的关系错综复杂，导致模式识别变得更加困难。这两个挑战在探索性数据分析中非常常见，在可视化过程中有多种方案能够应对这些挑战，以下将会分别进行介绍。\n\n\n\n\n\n\n\n\nFigure 6.2: 大N问题和大P问题",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>快速绘图：大数据可视化工具</span>"
    ]
  },
  {
    "objectID": "快速绘图：大数据可视化工具.html#面向大n问题的可视化方案",
    "href": "快速绘图：大数据可视化工具.html#面向大n问题的可视化方案",
    "title": "6  快速绘图：大数据可视化工具",
    "section": "6.3 面向大N问题的可视化方案",
    "text": "6.3 面向大N问题的可视化方案\n一般来说，可视化中的大N问题是因为数据点过多导致可视化过程卡顿，甚至因内存不足而无法实现。下面我们举一个例子，比如我们构造100万个点，它们有横坐标x和纵坐标y。构造代码如下：\n\nlibrary(pacman)\np_load(ggplot2,pryr,bench,fs,tidyst)\n\nx = rnorm(1e6,mean = 6)\ny = rnorm(1e6) + 2 + 1.5 * x\n\ndata = data.frame(x = x, y = y)\n\nobject_size(data)\n\n## 16.00 MB\n\n在上面的代码中，我们知道构造的数据大概占用了16.00 MB的内存空间。 然后，我们要使用ggplot2包来对这些点进行可视化，我们先生成这个图，然后观察这个图究竟占用了多少内存：\n\n# 绘制图片\ndata_plot = ggplot(data, aes(x=x, y=y))+ geom_point()\n\n# 查看内存占用\nobject_size(data_plot)\n\n## 17.69 MB\n\n结果显示可视化的结果大概占用了17.69 MB的内存空间。我们来看一下对这个图进行可视化需要多长时间：\n\npst(print(data_plot))\n\n# Finished in 20.8s elapsed (1.220s cpu)\n\n\n\n\n\n\n\n\n\n\n结果显示，大概需要20.8秒。如果要把这个图存起来，也需要耗费很多时间，而且还要占用比较大的空间（以保存为PDF格式为例，大概需要22秒，占内存约55 MB）。不同配置的计算机也许速度不一样，但是这个速度可以说相当的慢。这个例子给我们展示的就是可视化中的大N问题，要解决这个问题有多种方案，这里我们介绍两种：1、生成基于栅格的图形；2、先对数据进行汇总统计。\n所谓生成基于栅格的图形，经常也叫做生成位图。默认情况下，大多数数据可视化库，包括ggplot2，都会生成基于矢量的图形。一般来说，当绘制的观测值数量较少或中等时，这对于任何类型的图来说都是非常合理的，矢量图形在坐标系中将线条和形状定义为矢量。在散点图中，每个点的x和y坐标都需要被记录下来。相比之下，位图文件以矩阵（如果涉及颜色，则为多个矩阵）的形式包含图像信息，每个矩阵单元代表一个像素，并包含该像素的颜色信息。虽然绘制少量观测值的矢量图表示可能比同一图的高分辨率位图表示更节省内存，但当我们绘制上百万个观测值时，情况很可能会相反。在R中，可以使用scattermore包对这种方法进行实现：\n\np_load(scattermore)\n# 绘制图片\ndata_plot2 = ggplot(data,aes(x,y))+\n     geom_scattermore()\n\n# 测试速度\npst(print(data_plot2))\n\n# Finished in 0.920s elapsed (0.170s cpu)\n\n\n\n\n\n\n\n\n\n\n另一种方案是使用汇总数据，也就是说实际上可能不需要直接绘制所有的观测值，而是先对数据进行汇总，然后对汇总结果进行可视化。比如在我们上面这个例子中，可以将画布分成网格单元（通常是矩形或六边形），计算每个网格单元中将包含的观测值或点的数量，然后通过单元的颜色层次表示每个网格单元中的观测值数量。ggplot2中可以用geom_hex函数对这个功能进行实现：\n\n# 绘制图片\ndata_plot3 = ggplot(data, aes(x=x, y=y))+\n     geom_hex()\n\n# 测试速度\npst(print(data_plot3))\n\n# Finished in 0.440s elapsed (0.060s cpu)\n\n\n\n\n\n\n\n\n\n\n总体而言，先对数据进行汇总，然后在进行可视化是最常见的可视化模式（常用的图形类型包括分布图、箱线图等）。无论数据量大还是小，先对数据进行统计，然后在进行可视化，能够大大提高可视化的效率，同时也让读者能够更加直观地对数据进行审视和探究。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>快速绘图：大数据可视化工具</span>"
    ]
  },
  {
    "objectID": "快速绘图：大数据可视化工具.html#面向大p问题的可视化方案",
    "href": "快速绘图：大数据可视化工具.html#面向大p问题的可视化方案",
    "title": "6  快速绘图：大数据可视化工具",
    "section": "6.4 面向大P问题的可视化方案",
    "text": "6.4 面向大P问题的可视化方案\n在可视化问题中，大P问题往往是指变量较多的情况，需要对变量之间的关系进行探索，因此需要采用合适的可视化手段来清晰地进行表达。这里，我们将会采用R语言工具进行实现，来介绍在R中是如何使用不同的方案来应对可视化中的大P问题。\n\n6.4.1 分面与分组的应用\n理论上来说，低维数据可视化并不算大P问题，但是如果要对多个低维数据进行可视化，并进行比较，那么它就成为一种特殊的大P问题。解决低维数据可视化的大P问题，通常是通过分组或者分面来处理的。\n分组是指以某一个离散型变量作为分组变量，然后对其他变量进行分组可视化的过程，分组后各个组别之间可以使用颜色、形状、大小、线型、透明度等要素加以区分。我们以ggplot2所带的diamond数据集为例，用密度分布图对不同颜色的钻石深度分布进行可视化：\n\nlibrary(pacman)\np_load(tidyverse,ggridges)\n# 选取部分数据\ndf = diamonds[1:100, c(\"color\", \"depth\")]\n\n# 进行可视化\nggplot(df,aes(depth,color)) +\n  geom_density_ridges(aes(fill = color)) # 对不同的分组采用不同的颜色\n\nPicking joint bandwidth of 0.678\n\n\n\n\n\n\n\n\n\n根据上面的图，我们根据color这个离散型变量对密度图用了不同颜色进行填充，这样可以直观地知道不同颜色钻石深度的分布情况。分组允许你在单个图中绘制多个变量，使用颜色、形状和大小等视觉特征。而在分面中，一个图由几个独立的小图组成，相当于先用某离散型变量对数据进行分组，然后再对图形进行分别绘制。沿用上面的例子，如果采用分面绘图，其实现方法如下：\n\nggplot(df,aes(depth,color)) +\n  geom_density_ridges() +\n  facet_wrap(~color,scales = \"free\") +\n  theme(axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n根据上图显示的结果，我们可以看到对于每一个颜色，都绘制了一幅分布图，这种方法可以在没有颜色填充的情况下依然对各个情况进行区分。\n对于低维分布图的可视化，还可以使用包括箱线图、小提琴图、直方图在内的各种图形，这里仅仅强调分组和分面在解决低维数据可视化大P问题中的应用，因此不再对其他类型的图片进行赘述。感兴趣的读者可以尝试使用其他的可视化方法来对其进行实现。\n\n\n6.4.2 高维数据可视化\n试想一下这个问题，一个平面展示的信息图，最多可以表示数据表格中的多少个变量？我们不妨使用iris数据集来做一个试验。如果仅仅是对一个变量的分布进行展示，那么就只展示了一个变量：\n\nggplot(iris,aes(Sepal.Length)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n如果是表示两个连续变量（这里使用Sepal.Length和Petal.Length两个变量）关系的散点图，那么就展示了两个变量：\n\nggplot(iris,aes(Sepal.Length,Petal.Length)) +\n  geom_point()\n\n\n\n\n\n\n\n\n在上面的基础上，在对每一朵花的物种类别加以区分，就表示了三个变量：\n\nggplot(iris,aes(Sepal.Length,Petal.Length)) +\n  geom_point(aes(color = Species))\n\n\n\n\n\n\n\n\n如果我们还想对Sepal.Width这一变量进行展示，可以调节点的大小：\n\nggplot(iris,aes(Sepal.Length,Petal.Length)) +\n  geom_point(aes(color = Species,size = Sepal.Width))\n\n\n\n\n\n\n\n\n如果还想对Petal.Width进行展示，还可以利用透明度这一要素：\n\nggplot(iris,aes(Sepal.Length,Petal.Length)) +\n  geom_point(aes(color = Species,size = Sepal.Width,alpha = Petal.Width))\n\n\n\n\n\n\n\n\n那么我们就用一个图对4个变量都进行了表征。当然上面这类可视化并不一定是值得鼓励的（因为不同变量之间是并列关系，使用不同的可视化要素进行区分是不妥的），仅仅是为了展示一个平面图可以对多个变量的信息进行表征。基于iris数据集的特点，其实还可以使用其他方法展示数据集中的所有变量：\n\n# Libraries\np_load(GGally)\n\nggparcoord(iris,\n    columns = 1:4,\n    groupColumn = 5,\n    scale = \"globalminmax\") \n\n\n\n\n\n\n\n\n看到上图生成密密麻麻的折线，我们其实可以知道，如果观测值非常多的时候，那么它不仅仅是一个大P问题，还是一个大N问题，这会使得可视化的过程非常难以实现。这种情况下，最好能够先对数据进行汇总，然后再进行展示。举例来说，我们可以通过计算变量的均值和标准差，来展示其数据分布情况并进行比较，实现方式如下图所示：\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarise_if(is.numeric,list(mean = mean,sd = sd)) %&gt;% \n  pivot_longer(-Species) %&gt;% \n  separate_wider_delim(name,delim = \"_\",names = c(\"name\",\"class\")) %&gt;% \n  pivot_wider(names_from = class) %&gt;% \n  ggplot(aes(Species,mean)) +\n  geom_pointrange(aes(ymin = mean-sd,ymax = mean+sd,color = Species),\n                  show.legend = F) +\n  facet_wrap(~name,scales = \"free_y\") + \n  labs(y = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\n上面的图中，圆点表示平均值位置，线条表示一倍标准差的范围。这种可视化方法可以减少绘制成本，而且也能够让表达的信息更加集中而清晰。\n有的时候，我们要探索的不是数据的分布状况，而是数据之间的相关性。这可以用散点图来对多个变量的两两关系进行可视化：\n\nggpairs(iris,columns = 1:4)\n\n\n\n\n\n\n\n\n在上面生成的图中，左下角是两两变量之间的散点图，对角线则表示该变量的分布状况，右上角则显示了变量之间的相关系数及其显著性信息。此外，我们还可以对这个信息进行分组计算，实现方法如下：\n\nggpairs(iris, columns = 1:4, aes(color = Species, alpha = 0.5))\n\n\n\n\n\n\n\n\n在生成的结果中，散点图对三个物种用颜色进行了区分，对角线中是三个物种的不同变量的分布状况，而右上角的相关系数也对各个物种进行了分别计算。\n我们知道，当观测点过多的时候，散点图依然无法进行有效的可视化，更好的方法是只取相关系数信息，然后进行可视化。一般会使用相关性热图来对这种信息进行可视化，实现方法如下：\n\np_load(ggcorrplot)\ncorr = round(cor(iris[,-5]),1)\np.mat = cor_pmat(iris[,-5])\n\nggcorrplot(corr,\n           hc.order = TRUE,\n           type = \"lower\",\n           p.mat = p.mat,\n           lab = TRUE)\n\n\n\n\n\n\n\n\n上图显示了两两变量之间的相关性分析可视化热值图，如果方框中有交叉（“×”）表示这两个变量的相关性不显著，而方框中的数字表示两个变量之间的相关系数，其中相关系数越大则颜色越红，越小则越蓝。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>快速绘图：大数据可视化工具</span>"
    ]
  },
  {
    "objectID": "快速绘图：大数据可视化工具.html#小结",
    "href": "快速绘图：大数据可视化工具.html#小结",
    "title": "6  快速绘图：大数据可视化工具",
    "section": "6.5 小结",
    "text": "6.5 小结\n本章我们对大数据可视化的相关主题进行了探讨，首先熟悉了可视化的基本流程，然后点明在对大数据进行可视化的时候会遇到的挑战。最后，针对一些典型的大数据可视化问题，我们描述了如何用合理的方案进行应对。尽管我们的案例数据并不大，但是这些方案在处理大数据的时候会非常奏效，能够为用户节省很多的操作时间和空间。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>快速绘图：大数据可视化工具</span>"
    ]
  },
  {
    "objectID": "快速建模：高性能机器学习工具.html",
    "href": "快速建模：高性能机器学习工具.html",
    "title": "7  快速建模：高性能机器学习工具",
    "section": "",
    "text": "7.1 机器学习基本流程\n机器学习是一种通过算法和统计模型，使计算机系统从数据中自动学习和改进的技术。它包括监督学习、无监督学习、半监督学习和强化学习四种主要类型。机器学习广泛应用于图像识别、自然语言处理、医疗诊断和金融预测等领域。通过从大量数据中提取模式和特征，机器学习在解决复杂问题方面展现了强大的能力。\nFigure 7.1: 机器学习基本流程\n机器学习的基本步骤包括五个基本环节（见图Figure 7.1），分别是明确问题、数据采集、特征工程、模型训练和模型评估。下面我们对这些环节进行简要的介绍：",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>快速建模：高性能机器学习工具</span>"
    ]
  },
  {
    "objectID": "快速建模：高性能机器学习工具.html#机器学习基本流程",
    "href": "快速建模：高性能机器学习工具.html#机器学习基本流程",
    "title": "7  快速建模：高性能机器学习工具",
    "section": "",
    "text": "7.1.1 明确问题\n在机器学习项目的开始阶段，明确问题是至关重要的。这一步涉及以下几个方面：\n\n定义目标：明确要解决的问题和目标，例如预测房价、分类垃圾邮件、推荐商品等。\n确定任务类型：根据问题的性质确定机器学习任务类型，例如分类、回归、聚类等。\n理解业务需求：了解业务背景和需求，确保机器学习解决方案能够实际应用并带来价值。\n设定评价标准：确定评估模型性能的标准和指标，例如准确率、均方误差、F1分数等。\n\n\n\n7.1.2 数据采集\n数据采集是机器学习的基础，数据的质量直接影响模型的性能。这一步包括：\n\n数据来源：确定数据的来源，可以是数据库、文件系统、网络爬虫等。\n数据收集：从各种来源收集所需的数据，确保数据量足够且具有代表性。\n数据整合：将来自不同来源的数据整合在一起，形成一个统一的数据集。\n数据存储：将数据存储在合适的存储系统中，例如数据库、数据仓库等，以便后续处理。\n\n\n\n7.1.3 特征工程\n特征工程是提高模型性能的关键步骤，包括对数据进行处理和转换，以提取有用的特征，这个过程包括：\n\n数据清洗：处理缺失值、异常值和重复数据，确保数据的质量。\n特征选择：选择对预测结果有显著影响的特征，减少特征数量以提高模型效率。\n特征提取：从原始数据中提取新的特征，例如将日期拆分为年、月、日等。\n特征转换：对特征进行转换，例如标准化、归一化、编码分类变量等，以适应模型输入要求。\n\n\n\n7.1.4 模型训练\n模型训练是机器学习的核心步骤，通过使用训练数据调整模型的参数，使其能够最好地拟合数据，相关的步骤包括：\n\n选择算法：根据任务类型和数据特点选择合适的机器学习算法，例如线性回归、决策树、神经网络等。\n模型构建：使用选定的算法构建初始模型。\n训练模型：将训练数据输入模型，调整参数以最小化误差或最大化某种评估指标。\n超参数调优：通过调整模型的超参数（例如学习率、正则化参数等）进一步优化模型性能。\n\n\n\n7.1.5 模型评估\n模型评估是检测模型性能的关键步骤，通过使用测试数据来评估模型的泛化能力，评估流程包括：\n\n划分数据：将数据集划分为训练集和测试集，确保模型评估的独立性，评估阶段会使用测试集的数据。\n选择评估指标：根据问题和任务类型选择合适的评估指标，例如分类任务中的准确率、精确率、召回率，回归任务中的均方误差、R方等。\n评估模型：使用测试数据计算评估指标，评估模型在未见数据上的表现。\n验证和调整：通过交叉验证等方法进一步验证模型的稳定性，必要时调整模型参数或特征工程。\n\n通过这五个环节，机器学习项目能够系统地进行，从明确问题到最终评估模型，每一步都为构建一个高性能的机器学习模型奠定基础。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>快速建模：高性能机器学习工具</span>"
    ]
  },
  {
    "objectID": "快速建模：高性能机器学习工具.html#机器学习中可加速的环节",
    "href": "快速建模：高性能机器学习工具.html#机器学习中可加速的环节",
    "title": "7  快速建模：高性能机器学习工具",
    "section": "7.2 机器学习中可加速的环节",
    "text": "7.2 机器学习中可加速的环节\n在机器学习过程中，加速某些环节可以显著提高整个机器学习的效率。本节我们会讨论我们能够从哪些环节进行优化，从而让机器学习的实现更加快捷。\n首先，在数据采集和预处理阶段，通过自动化工具和脚本可以大幅减少手动操作的时间。此外，利用多线程或分布式计算框架（如Apache Spark）可以加快大规模数据集的处理和转换。此外，高效的数据存储格式（如Parquet、Feather）和数据库系统（如PostgreSQL、MongoDB）也能显著提高数据读写速度。\n在特征工程阶段，批处理方法可以减少重复计算，自动特征工程工能够自动生成和选择特征，进一步提高效率。缓存特征工程过程中产生的中间结果，可以避免重复计算，从而节省时间。在模型训练过程中，使用多线程、多进程或分布式计算框架）可以并行训练模型，显著缩短训练时间。硬件加速器（如GPU或TPU）在加速深度学习模型训练方面也非常有效。此外，选择高效的优化算法能够加快模型的收敛速度，进而提高训练的整体速度。\n超参数调优是另一个可以加速的环节。并行搜索方法（如并行网格搜索、随机搜索）允许同时评估多个超参数组合，显著节省时间。使用贝叶斯优化、遗传算法等智能优化方法，可以更高效地搜索最佳超参数组合。此外，超参数调优过程中使用早停法（early stopping），当模型性能不再显著提升时停止训练，这样可以节省大量计算资源。\n在模型评估阶段，分布式计算框架可以并行计算评估指标，尤其是在处理大规模测试集时，这一方法非常有效。通过分层K折交叉验证或留一法交叉验证，可以在保证评估可靠性的同时减少计算负担。此外，预先计算一些评估指标，也能减少评估时间。\n在模型部署和推理阶段，可以使用模型剪枝等技术来减少模型大小，提高推理速度，这能够显著提升部署效率。对于重复请求，缓存预测结果可以减少重复计算，进一步提高响应速度。\n通过在这些环节中采用高效的技术和方法，能够显著加速机器学习项目的执行，提高整个流程的效率。上面描述的是我们能够加速的组分，在下一节中我们会采用R语言中mlr3工具包作为机器学习框架，描述如何在R环境高效地开展机器学习过程。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>快速建模：高性能机器学习工具</span>"
    ]
  },
  {
    "objectID": "快速建模：高性能机器学习工具.html#基于mlr3的高性能机器学习",
    "href": "快速建模：高性能机器学习工具.html#基于mlr3的高性能机器学习",
    "title": "7  快速建模：高性能机器学习工具",
    "section": "7.3 基于mlr3的高性能机器学习",
    "text": "7.3 基于mlr3的高性能机器学习\n在现代数据科学和机器学习的领域中，高效处理和分析大规模数据集是关键。为了实现这一目标，R语言中的mlr3框架以其强大的面向对象编程和模块化设计脱颖而出。本节将介绍如何利用mlr3构建高性能的机器学习模型，探索其核心功能和附加包如何协同工作，以支持并行计算和大数据处理。通过学习这一内容，您将了解如何使用mlr3优化机器学习任务的执行效率，提升模型的预测能力和可靠性，从而在实际应用中更好地应对复杂的数据分析挑战。\n\n7.3.1 基本实现\n下面我们举一个简单的例子，来看如何使用mlr3工具包来完成一个基本的机器学习任务。 首先，我们加载必须的包：\n\nlibrary(pacman)\np_load(mlr3verse)\n\n然后，我们来定义一个任务。这里我们提及的任务在mlr3的体系中是一个数据对象，它包含对机器学习任务进行定义的数据（即需要进行数据分析的原始数据）和元数据（比如谁是响应变量、谁是解释变量、是一个分类还是回归问题等）。在mlr3中，有一系列已经定义好的任务，可以直接进行取用。这里我们会取用“penguins”任务，这个任务试图通过企鹅的一系列特征来对企鹅的品种进行分类。任务定义代码如下：\n\ntask = tsk(\"penguins\")\ntask\n\n&lt;TaskClassif:penguins&gt; (344 x 8): Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (7):\n  - int (3): body_mass, flipper_length, year\n  - dbl (2): bill_depth, bill_length\n  - fct (2): island, sex\n\n\n随后，我们需要使用partition函数对数据进行划分：\n\nsplit = partition(task)\nsplit\n\n$train\n  [1]   2   3   4   5   6   8   9  10  12  13  14  20  21  23  24  25  26  27\n [19]  29  32  34  35  36  37  38  40  42  45  46  47  48  49  50  51  53  59\n [37]  60  62  63  64  65  67  70  71  74  77  78  79  81  82  83  84  86  89\n [55]  91  92  94  95  96  97  98 100 101 102 103 104 105 106 107 109 110 111\n [73] 112 113 114 118 119 120 121 122 123 127 128 129 130 132 133 134 135 136\n [91] 138 140 141 143 144 145 146 147 149 150 151 152 154 156 158 159 160 162\n[109] 163 165 167 169 170 171 172 174 175 178 179 180 181 182 184 186 187 189\n[127] 190 192 193 196 197 198 200 201 202 204 205 206 207 208 210 211 214 215\n[145] 216 217 222 223 225 227 229 230 231 232 234 236 237 240 241 244 245 246\n[163] 247 248 250 251 252 253 254 256 258 259 260 261 262 264 266 267 268 269\n[181] 271 272 273 275 276 277 278 279 280 281 282 285 286 287 288 289 290 291\n[199] 294 299 300 301 302 303 305 307 308 309 310 311 312 313 314 316 317 318\n[217] 319 320 321 322 323 325 327 329 330 337 338 339 341 342 343\n\n$test\n  [1]   1   7  11  15  16  17  18  19  22  28  30  31  33  39  41  43  44  52\n [19]  54  55  56  57  58  61  66  68  69  72  73  75  76  80  85  87  88  90\n [37]  93  99 108 115 116 117 124 125 126 131 137 139 142 148 153 155 157 161\n [55] 164 166 168 173 176 177 183 185 188 191 194 195 199 203 209 212 213 218\n [73] 219 220 221 224 226 228 233 235 238 239 242 243 249 255 257 263 265 270\n [91] 274 283 284 292 293 295 296 297 298 304 306 315 324 326 328 331 332 333\n[109] 334 335 336 340 344\n\n\n上面这一步操作，将数据分为了两份，一份是训练数据，一份是测试数据。split变量是一个列表，放着的是训练集和测试集所在的行号。下一步，我们将选择机器学习的模型，制定一个学习器。我们将选用决策树算法进行训练，让其进行分类，实现方法如下：\n\nlearner = lrn(\"classif.rpart\")\nlearner\n\n&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\n如果想知道还有哪些学习器可以选择，可以这样操作：\n\nmlr_learners\n\n&lt;DictionaryLearner&gt; with 49 stored values\nKeys: classif.cv_glmnet, classif.debug, classif.featureless,\n  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n  classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n  clust.dbscan_fpc, clust.diana, clust.em, clust.fanny,\n  clust.featureless, clust.ff, clust.hclust, clust.hdbscan,\n  clust.kkmeans, clust.kmeans, clust.MBatchKMeans, clust.mclust,\n  clust.meanshift, clust.optics, clust.pam, clust.SimpleKMeans,\n  clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n  regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n  regr.rpart, regr.svm, regr.xgboost\n\n\n上面给出的是mlr3内置的所有机器学习方法，其中classif作为前缀的是分类算法、clust作为前缀的是聚类算法、regr作为前缀的是回归算法。关于更多的算法参数列表，可以参考官方的文档（https://mlr-org.com/learners.html）。 在下一步，我们需要对模型训练，这里我们只需要对训练集进行学习，测试集留作测试。实现方法如下：\n\nlearner$train(task, row_ids = split$train)\n\n经过训练之后，获得的模型存在learner对象的model中：\n\nlearner$model\n\nn= 231 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 231 129 Adelie (0.441558442 0.199134199 0.359307359)  \n  2) flipper_length&lt; 207 144  44 Adelie (0.694444444 0.298611111 0.006944444)  \n    4) bill_length&lt; 43.35 100   3 Adelie (0.970000000 0.030000000 0.000000000) *\n    5) bill_length&gt;=43.35 44   4 Chinstrap (0.068181818 0.909090909 0.022727273) *\n  3) flipper_length&gt;=207 87   5 Gentoo (0.022988506 0.034482759 0.942528736) *\n\n\n利用这个学习器得到的模型，我们可以对测试集的数据进行预测。我们会调用学习器learner的predict方法，然后对之前task任务数据中行号为test的测试集进行预测：\n\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n\n&lt;PredictionClassif&gt; for 113 observations:\n    row_ids     truth  response\n          1    Adelie    Adelie\n          7    Adelie    Adelie\n         11    Adelie    Adelie\n---                            \n        336 Chinstrap Chinstrap\n        340 Chinstrap    Gentoo\n        344 Chinstrap Chinstrap\n\n\n最后，我们可以观察一些衡量模型好坏的参数来评估其表现，比如我们可以用准确率来进行衡量，实现方法如下：\n\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9557522 \n\n\n上面这些步骤，就是我们利用mlr3框架对一个基本任务进行机器学习的全过程。有了这个基本的概念，我们来看mlr3是如何使用其框架给我们的机器学习过程提供便捷的工具，让整个任务更加高效。\n\n\n7.3.2 特征选择\n大数据中尝尝存在维度灾难的问题。维度灾难指的是在高维数据中，由于数据点变得稀疏和距离度量失效，导致传统的数据分析和机器学习方法变得无效的问题。它会导致模型过拟合、计算复杂度急剧上升和样本需求量增加。通过特征选择、特征提取和正则化等方法，可以有效缓解维度灾难，提高模型的性能和计算效率。本部分将会聚焦在特征选择这一主题，讨论在mlr3框架下如何对数据的特征进行筛选。常见的两种特征选择方法是筛选法和封装法，以下将分述之。\n\n7.3.2.1 筛选法\n筛选法是一种特征选择方法，通过独立评估每个特征的重要性，使用统计测试或评分指标（如信息增益、卡方检验或相关系数）来选择最具代表性的特征。筛选法的主要优点是计算效率高，可以快速处理大规模数据，并且不依赖于特定的机器学习算法，使其广泛适用于各种建模任务。\n这里，我们以信息增益法为例，描述如何在mlr3的框架下实现筛选法。信息增益法通过评估每个特征对目标变量不确定性的减少程度来衡量特征的重要性。它计算出在引入某个特征后，我们对目标变量的预测是否变得更加明确，从而确定特征的重要性。最终，选择那些显著减少目标变量不确定性的特征用于模型训练。具体实现方法如下：\n\n# 加载包\nlibrary(mlr3filters)\n\n# 定义筛选器的类型，使用信息增益法\nflt_gain = flt(\"information_gain\")\n\n# 使用企鹅分类的任务\ntsk_pen = tsk(\"penguins\")\n\n# 对变量重要性的分值进行计算\nflt_gain$calculate(tsk_pen)\n\n# 对结果进行展示\nas.data.table(flt_gain)\n\n          feature       score\n           &lt;char&gt;       &lt;num&gt;\n1: flipper_length 0.581167901\n2:    bill_length 0.544896584\n3:     bill_depth 0.538718879\n4:         island 0.520157171\n5:      body_mass 0.442879511\n6:            sex 0.007244168\n7:           year 0.000000000\n\n\n需要注意的是，有的筛选法只能处理没有缺失值的数据。在这样的情况下，需要先剔除掉缺失值，然后再对重要性分值进行计算。比如如果我们要选用JMIM（ Joint Mutual Information Maximization）方法，该方法通过最大化特征与目标变量之间的联合互信息来选择特征，确保每个新添加的特征与已选特征和目标变量的联合信息量最大。它在考虑特征之间的相互依赖性和与目标变量的关系时，选择最具代表性的特征。这样的方法有效地避免了选择冗余或相关性较低的特征，从而提高模型的性能。具体实现方法如下：\n\n# 定义筛选器的类型，使用信息增益法\nflt_jmim = flt(\"jmim\")\n\n# 使用企鹅分类的任务\ntsk_pen = tsk(\"penguins\")\n\n# 剔除任务数据中包含缺失值的条目\ntsk_pen$filter(tsk_pen$row_ids[complete.cases(tsk_pen$data())])\n\n# 对变量重要性的分值进行计算\nflt_jmim$calculate(tsk_pen)\n\n# 对结果进行展示\nas.data.table(flt_jmim)\n\n          feature     score\n           &lt;char&gt;     &lt;num&gt;\n1: flipper_length 1.0000000\n2:    bill_length 0.8333333\n3:      body_mass 0.6666667\n4:         island 0.5000000\n5:     bill_depth 0.3333333\n6:           year 0.1666667\n7:            sex 0.0000000\n\n\n\n\n7.3.2.2 封装法\n封装法是一种特征筛选方法，通过直接使用特定的机器学习模型来评估和选择特征子集。它使用搜索算法生成不同的特征子集，然后对每个子集训练和评估模型，使用性能指标来衡量模型表现。通过对每个特征子集的评估，封装法能够选择出在指定模型上表现最佳的特征组合。由于这种方法直接针对特定模型进行优化，通常能够获得优越的预测性能。然而，封装法的计算成本较高，尤其是在处理大规模数据集时，因此需要更多的计算资源和时间。\n为了多快好省地获得最佳特征组合（或者至少是表现较为不错的特征组合），在实现封装法的时候需要遵循一定的搜索规则，从而提高搜索的效率。最常见的方法之一就是前向选择，其基本思想是从空特征集开始，逐步向前添加特征，直到达到某个预设的终止条件为止。在每一步中，选择能够带来最大性能提升的特征添加到当前特征集中，直至满足停止条件。这个过程通常会在指定的性能指标下，如准确率或其他评估指标下进行。其实现方法如下：\n\n# 加载包\nlibrary(mlr3fselect)\n\n# 定义任务\ntsk_pen = tsk(\"penguins\")\n\n# 定义学习器，使用决策树\nlrn_rpart = lrn(\"classif.rpart\")\n\n# 对其中的4个特征进行筛选\ntsk_pen$select(c(\"bill_depth\", \"bill_length\", \"body_mass\",\n  \"flipper_length\"))\n\n# 进行特征筛选\ninstance = fselect(\n  fselector = fs(\"sequential\"), # 筛选器设置\n  task =  tsk_pen,  # 声明任务\n  learner = lrn_rpart, # 学习器设置\n  resampling = rsmp(\"cv\", folds = 3), # 使用3折交叉验证来做重采样\n  measure = msr(\"classif.acc\") # 衡量标准是分类准确率\n)\n\n# 结果展示\ninstance$result_feature_set\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\"\n\n\n我们没有给出展示的结果，因为在实际的操作中，如果重复次数不够多，那么不同迭代中出现的结果可能会不一致，这里仅仅展示实现方法。在特征筛选之后，我们可以使用筛选的特征来进行模型训练，方法如下：\n\n# 定义任务\ntsk_pen = tsk(\"penguins\")\n\n# 选择筛选出来的特征\ntsk_pen$select(instance$result_feature_set)\n\n# 决策树模型训练\nlrn_rpart$train(tsk_pen)\n\n在实际操作中，还可以针对多个目标（比如同时优化特异度和敏感度）进行优化，同时还可以加入终止条件（比如只要迭代100次就停止），相关内容可以参考官方文档（https://mlr3book.mlr-org.com/chapters/chapter6/feature_selection.html）。\n\n\n\n7.3.3 模型比较\n很多时候，我们需要对同一个任务使用多个机器学习模型，并对模型进行比较；甚至有的时候，我们还需要看同一个模型在不同任务上的表现。在这种背景下，就需要构造任务组合，从而看在不同条件下创建的模型表现如何。下面我们展示在mlr3框架下如何对此进行实现：\n\n# 选择2个分类任务\ntasks = tsks(c(\"german_credit\", \"sonar\"))\n\n# 选择3个算法，预测类型为概率值\nlearners = lrns(c(\"classif.rpart\", \"classif.ranger\",\n  \"classif.featureless\"), predict_type = \"prob\")\n\n# 设置重采样方法为5折交叉验证\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\n# 创建训练网格\ndesign = benchmark_grid(tasks, learners, rsmp_cv5)\n\n# 对模型进行训练\nbmr = benchmark(design)\n\n# 观察模型效果\nbmr$aggregate() [,.(task_id,learner_id,classif.ce)]\n\n         task_id          learner_id classif.ce\n          &lt;char&gt;              &lt;char&gt;      &lt;num&gt;\n1: german_credit       classif.rpart  0.2880000\n2: german_credit      classif.ranger  0.2370000\n3: german_credit classif.featureless  0.3000000\n4:         sonar       classif.rpart  0.3022067\n5:         sonar      classif.ranger  0.1773519\n6:         sonar classif.featureless  0.4660859\n\n\n这里我们没有对结果进行展示，读者可以自行运行代码来观察，其中需要注意的细节包括：1、在使用算法的时候，“classif.rpart”使用的是决策树算法，“classif.ranger”使用的是随机森林算法，而“classif.featureless”则是一个基线模型，在分类问题上会对盲猜为多数类；2、这里默认观察模型的效果，会计算分类的错误率，错误率月低，代表模型表现越好，详见官方文档（https://mlr3.mlr-org.com/reference/mlr_measures_classif.ce.html）；3、在对bmr对象进行观察的时候，调用了aggregate方法，这个方法能够对不同迭代的表现结果进行汇总；4、mlr3框架使用了data.table作为底层，因此在观察模型效果的时候，我们选择列直接用了data.table中的方法。关于更多模型比较的内容，可以参考官方文档（https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html）。\n\n\n7.3.4 参数调节\n机器学习中的参数调节是优化机器学习模型性能的关键步骤。超参数是模型在训练过程中设置的参数，不会从数据中学习到，需手动设定。例如，决策树的深度、支持向量机的核函数类型或正则化参数等都是超参数。参数调节旨在找到这些超参数的最佳组合，以提高模型在新数据上的表现，常见的方法包括网格搜索、随机搜索和贝叶斯优化等。网格搜索通过遍历预定义参数组合来寻找最佳参数，但计算量大；随机搜索在预定义范围内随机选择参数组合，效率较高；贝叶斯优化利用先前评估结果构建代理模型，预测并选择最优参数组合。参数调节通常结合交叉验证进行，以确保模型的泛化能力。通过优化超参数，模型能在新数据上表现得更好，提高预测准确性和稳健性。\n在mlr3中，如果要对一个学习器中的参数进行调节，可以在设置学习器的时候就进行声明，而我们日常没有特殊声明的时候都会直接使用函数的默认参数来进行设置。对于不同的机器学习算法，有不同的参数可以调节，我们可以去看学习器的参数集来观察有哪些参数可以调节，而且这些参数调节的范围是什么。比如我们想对随机森林方法的参数集进行观察，可以这样操作：\n\nas.data.table(lrn(\"classif.ranger\")$param_set)[,\n  .(id, class, lower, upper, nlevels)]\n\n                              id    class lower upper nlevels\n                          &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:                        alpha ParamDbl  -Inf   Inf     Inf\n 2:       always.split.variables ParamUty    NA    NA     Inf\n 3:                class.weights ParamUty    NA    NA     Inf\n 4:                      holdout ParamLgl    NA    NA       2\n 5:                   importance ParamFct    NA    NA       4\n 6:                   keep.inbag ParamLgl    NA    NA       2\n 7:                    max.depth ParamInt     0   Inf     Inf\n 8:                   min.bucket ParamInt     1   Inf     Inf\n 9:                min.node.size ParamInt     1   Inf     Inf\n10:                      minprop ParamDbl  -Inf   Inf     Inf\n11:                         mtry ParamInt     1   Inf     Inf\n12:                   mtry.ratio ParamDbl     0     1     Inf\n13:            num.random.splits ParamInt     1   Inf     Inf\n14:                   node.stats ParamLgl    NA    NA       2\n15:                  num.threads ParamInt     1   Inf     Inf\n16:                    num.trees ParamInt     1   Inf     Inf\n17:                    oob.error ParamLgl    NA    NA       2\n18:        regularization.factor ParamUty    NA    NA     Inf\n19:      regularization.usedepth ParamLgl    NA    NA       2\n20:                      replace ParamLgl    NA    NA       2\n21:    respect.unordered.factors ParamFct    NA    NA       3\n22:              sample.fraction ParamDbl     0     1     Inf\n23:                  save.memory ParamLgl    NA    NA       2\n24: scale.permutation.importance ParamLgl    NA    NA       2\n25:                    se.method ParamFct    NA    NA       2\n26:                         seed ParamInt  -Inf   Inf     Inf\n27:         split.select.weights ParamUty    NA    NA     Inf\n28:                    splitrule ParamFct    NA    NA       3\n29:                      verbose ParamLgl    NA    NA       2\n30:                 write.forest ParamLgl    NA    NA       2\n                              id    class lower upper nlevels\n\n\n在展示结果中我们发现一共有30种参数可以进行调节，其中id存放的是参数的名称，class告诉我们参数的数据类型，而lower和upper分别界定的了参数的下限和上限，nlevels则告诉我们参数有几种可以选择（一般适用于离散型变量，如逻辑型和因子型）。这里，如果我们要对随机森林中树的数量进行调节，分别尝试100、200和400棵树，可以这样进行设置：\n\nlearner = lrn(\"classif.ranger\", num.trees = to_tune(c(100, 200, 400)))\n\n如果只是想测试一个范围，比如100到400课树之间，则可以这样操作：\n\nlearner = lrn(\"classif.ranger\", num.trees = to_tune(100,400))\n\n不妨看一下to_tune函数是如何对搜索空间进行设定的：\n\n# 放入一个向量时，会进行遍历\n to_tune(c(100, 200, 400))\n\nTuning over:\np_fct(levels = c(`100` = 100, `200` = 200, `400` = 400))\n\n# 放入两个数字时，会生成一个范围\n to_tune(100,400)        \n\nTuning over:\nrange [100, 400]\n\n\n要对函数进行更多的了解，可以通过键入?to_tune来参阅帮助文档进行学习。 当我们界定了搜索空间的范围时，很多时候我们没有办法进行地毯式的全面训练搜索，这时候就需要设定终止条件，比如训练一段时间后结束，或者进行100次迭代后结束等。在mlr3框架中，可以使用trm函数来定义一个终止条件。在知道上面我们提及的背景信息后，我们来尝试创建一个调参的对象：\n\n# 定义任务\ntsk_sonar = tsk(\"sonar\")\n\n# 定义学习器（随机森林模型），设定需要调节的参数\nlearner = lrn(\"classif.ranger\", num.trees = to_tune(c(100, 200, 400)))\n\n# 创建调参的环境\ninstance = ti(\n  task = tsk_sonar, # 设置任务\n  learner = learner, # 设置学习器\n  resampling = rsmp(\"cv\", folds = 3), # 设置重采样方法：3折交叉验证\n  measures = msr(\"classif.ce\"), # 设置评估指标：分类错误率\n  terminator = trm(\"none\") # 设置终止调节：不采用任何终止条件，全部测试\n)\n\n# 设定调参方法为网格搜索\ntuner = tnr(\"grid_search\")\n\n# 进行调参\ntuner$optimize(instance)\n\n   num.trees learner_param_vals  x_domain classif.ce\n      &lt;char&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1:       400          &lt;list[2]&gt; &lt;list[1]&gt;  0.1971705\n\n# 显示调参结果\ninstance$result\n\n   num.trees learner_param_vals  x_domain classif.ce\n      &lt;char&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1:       400          &lt;list[2]&gt; &lt;list[1]&gt;  0.1971705\n\n\n此外，mlr3框架中实现同样的方法，还有其他更加明晰的实现方法：\n\ntuner = tnr(\"grid_search\")\n\nlearner = lrn(\"classif.ranger\", num.trees = to_tune(c(100, 200, 400)))\n\nrsmp_cv3 = rsmp(\"cv\", folds = 3)\n\nmsr_ce = msr(\"classif.ce\")\n\nterminator = trm(\"none\")\n\ninstance = tune(\n  tuner = tuner,\n  task = tsk_sonar,\n  learner = learner,\n  resampling = rsmp_cv3,\n  measures = msr_ce,\n  terminator = terminator\n)\n\ninstance$result\n\n   num.trees learner_param_vals  x_domain classif.ce\n      &lt;char&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1:       400          &lt;list[2]&gt; &lt;list[1]&gt;  0.1731539\n\n\n最后返回的结果就是本次测试中获得的最佳参数（这里没有给出，请读者自行运行尝试）。关于如何在mlr3框架中灵活地对各种参数组合进行调节，可以参考官方文档（https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html）。\n\n\n7.3.5 综合实践\n在本节中，将会给出一个综合的例子，描述如何在mlr3框架中进行高效的机器学习。首先我们将加载包：\n\nlibrary(mlr3verse)\n\n随后我们会挑选2个内置任务，这两个任务的共同点是它们都属于二分类问题：\n\ntasks = tsks(c(\"german_credit\", \"sonar\"))\n\n而后，我们来定义学习器，并为需要调整的参数进行设置：\n\n# 定义随机森林学习器\nglrn_rf_tuned = as_learner(ppl(\"robustify\") %&gt;&gt;% auto_tuner(\n    tnr(\"grid_search\", resolution = 5), \n    lrn(\"classif.ranger\", num.trees = to_tune(200, 500)),\n    rsmp(\"holdout\")\n))\n# 为学习器进行命名，命名为RF\nglrn_rf_tuned$id = \"RF\"\n\n# 定义集成学习的学习器\nglrn_stack = as_learner(ppl(\"robustify\") %&gt;&gt;% ppl(\"stacking\",\n    lrns(c(\"classif.rpart\", \"classif.kknn\")),\n    lrn(\"classif.log_reg\")\n))\n# 为学习器进行命名，命名为Stack\nglrn_stack$id = \"Stack\"\n\n上面的代码虽然很短，但是其实细节非常多：1、%&gt;&gt;%是mlr3框架中独特的管道操作符，用于将多个数据处理和建模步骤组合在一起，以创建一个完整的机器学习工作流（详见https://mlr3book.mlr-org.com/chapters/chapter7/sequential_pipelines.html）；2、在定义网格搜索的时候，声明resolution参数为5，即分辨率为5，这样会把搜索空间分为5等分，然后在在空间内进行尝试，这个例子中是对随机森林下限为200课树、上限为500棵树这个空间进行划分（详见https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html）；3、在定义名为“Stack”的学习器，使用了集成学习方法，基本思想是使用决策树和KNN算法先训练，然后结合两者的结果再拟合一个逻辑回归对响应变量进行预测。这个方法超越了本书介绍的范畴，感兴趣的读者可以参考官方文档（https://mlr3book.mlr-org.com/chapters/chapter8/non-sequential_pipelines_and_tuning.html）中的相关部分。 定义了两个学习器之后，我们要把两者合并到一起：\n\nlearners = c(glrn_rf_tuned, glrn_stack)\n\n然后，我们要对这些任务-模型组合进行训练，观察其表现。这里统一使用3折交叉验证进行重采样，实现代码如下：\n\nbmr = benchmark(benchmark_grid(tasks, learners, rsmp(\"cv\", folds = 3)))\n\n最后通过对所得结果的准确率进行计算并汇总，来对不同任务下不同算法效果的对比，实现代码如下：\n\nbmr$aggregate(msr(\"classif.acc\"))\n#       nr       task_id learner_id resampling_id iters classif.acc\n#    &lt;int&gt;        &lt;char&gt;     &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n# 1:     1 german_credit         RF            cv     3   0.7550245\n# 2:     2 german_credit      Stack            cv     3   0.7390235\n# 3:     3         sonar         RF            cv     3   0.8221532\n# 4:     4         sonar      Stack            cv     3   0.7213941\n# Hidden columns: resample_result\n\n这里我们没有给出结果，请读者自行尝试。我们可以看到，mlr3框架可以让我们用短短的数行代码来实现非常复杂的机器学习操作，大大提高了我们完成机器学习任务的效率。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>快速建模：高性能机器学习工具</span>"
    ]
  },
  {
    "objectID": "快速建模：高性能机器学习工具.html#小结",
    "href": "快速建模：高性能机器学习工具.html#小结",
    "title": "7  快速建模：高性能机器学习工具",
    "section": "7.4 小结",
    "text": "7.4 小结\n本章针对机器学习问题，描述了如果在R语言中应用mlr3框架来高效完成机器学习的各个环节。事实上，针对大数据问题，mlr3框架还提供了很多高级工具，如并行化、异常处理等（参见https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html）。由于本部分只是对大数据机器学习进行简单介绍，因此没有对各方面的细节进行一一解释，感兴趣的读者可以尝试系统学习mlr3框架的图书，电子版见https://mlr3book.mlr-org.com/。希望读者在学习完本章后，能够熟悉机器学习的各个环节，并利用相关工具对每个环节进行加速，结合优秀的算法和实现工具，减轻建模者和机器可能承受的负担，进而提高机器学习的效率。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>快速建模：高性能机器学习工具</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html",
    "href": "化整为零：对文件进行批处理.html",
    "title": "8  化整为零：对文件进行批处理",
    "section": "",
    "text": "8.1 文件系统操作\n文件系统操作涉及对计算机存储系统中的文件和目录进行管理和操控，包括创建、删除、复制、移动和重命名文件和目录，以及获取文件的属性信息和列出目录内容等。文件系统操作是数据管理和处理过程中不可或缺的一部分，通过有效地组织和维护文件，可以提高工作效率和数据处理的可靠性。\n在R语言中，fs包提供了一组功能强大且易于使用的函数，用于执行各种文件系统操作。fs包的设计注重简洁性和一致性，使得用户可以更直观和高效地进行文件和目录管理。其核心功能包括：\n通过使用 fs包，R语言用户可以高效地执行各种文件系统操作，从而简化文件管理任务，提升数据处理的效率和可靠性。fs包的函数设计简洁一致，使得文件和目录的创建、复制、移动、删除等操作变得直观和容易实现。无论是进行简单的文件操作还是复杂的批量处理，fs 包都提供了强大的支持，使用户能够更加专注于数据分析和处理本身。本介绍只提及了部分fs包的功能，更多相关信息可以参考官方文档（https://fs.r-lib.org/），我们将会在后续的实践中对其中实用的功能进行运用和讲解。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html#文件系统操作",
    "href": "化整为零：对文件进行批处理.html#文件系统操作",
    "title": "8  化整为零：对文件进行批处理",
    "section": "",
    "text": "创建目录： dir_create函数可以方便地创建新目录。这在组织和分类数据文件时非常有用。\n创建文件： 使用 file_create 函数可以创建新的文件。这对于初始化新的数据文件或日志文件非常实用。\n复制文件： file_copy 函数用于复制文件，将文件从一个位置复制到另一个位置。这在备份文件或将文件分发到不同目录时非常有用。\n移动或重命名文件： file_move 函数用于移动文件或重命名文件。移动文件可以改变文件的存储位置，而重命名文件可以使文件名更加有意义或符合命名规范。\n删除文件或目录： file_delete/dir_delete 函数用于删除不需要的文件或目录，帮助用户清理和管理存储空间。\n列出目录内容： 使用 dir_ls 函数可以列出目录中的所有文件和子目录。这对于检查目录内容或批量处理文件时非常有用。\n获取文件信息： file_info函数提供文件的详细信息，如文件大小、创建时间、修改时间等。这些信息在文件管理和分析时非常重要。\n检查文件或目录是否存在： 使用 file_exists/dir_exists 函数可以方便地检查文件或目录是否存在，帮助用户避免文件操作中的错误。\n文件路径操作： fs包还提供了处理文件路径的功能，包括路径组合、路径拆分和获取路径组件等。这些功能使得文件路径操作更加灵活和便捷。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html#文件压缩操作",
    "href": "化整为零：对文件进行批处理.html#文件压缩操作",
    "title": "8  化整为零：对文件进行批处理",
    "section": "8.2 文件压缩操作",
    "text": "8.2 文件压缩操作\n文件压缩操作是一种通过减少文件大小来优化存储和传输效率的技术。它涉及将数据重新编码以占用更少的空间，从而节省存储资源、提高传输速度并增强数据的安全性。压缩文件在数据备份、传输和归档过程中尤为重要，因为它能够显著减少文件的存储需求，并加快通过网络传输的速度。常见的压缩格式包括 zip、tar、rar、gzip、bzip2 和 7-zip 等，每种格式都有其特定的应用场景和优势。\narchive 包是 R 语言中的一个强大工具，用于处理多种压缩格式的文件。这个包提供了统一的接口，可以轻松地对各种类型的压缩文件进行创建、解压和查看操作。与其他专注于单一压缩格式的包不同，archive包支持多种格式，这使其在处理不同类型的压缩文件时更加灵活和强大。archive 包的主要特点包括：\n\n多格式支持：能够处理常见的压缩格式，如 zip、tar、rar、gzip、bzip2、xz 和 7-zip，适用于不同需求的压缩和解压操作。\n统一接口：提供了一套统一的函数接口，使用户能够方便地对不同格式的压缩文件进行操作，而无需为每种格式学习不同的命令。\n高效处理：能够高效地创建和解压大文件，适用于大规模数据处理和备份任务。\n灵活操作：支持列出压缩文件中的内容，方便用户查看压缩包内的文件结构和信息。\n\n通过 archive 包，用户可以轻松地进行压缩和解压操作。例如，使用 archive_write_files/archive_write_dir 函数可以创建一个新的压缩文件，将多个文件压缩到一起；使用 archive 函数可以列出压缩文件中的内容，查看其中包含的文件列表和结构；使用 archive_extract 函数可以将压缩文件解压到指定的目录，方便后续的文件处理和使用。\n总的来说，archive 包为 R 用户提供了一个灵活、高效的工具，能够处理多种压缩格式的文件，简化了文件压缩和解压的工作流程，使数据存储和传输更加方便和高效。无论是在数据备份、传输还是归档过程中，archive 包都能提供强大的支持，更多相关的信息可参考官方文档（https://archive.r-lib.org/）。\n值得一提的是，当我们把多个文件进行打包的时候，并不总是必须对文件进行压缩（即把文件的总容量减小），有的时候我们甚至会把文件变大，但是压缩依然有意义。我们可以把这个过程看做是把很多小物件放进一个容器中（容器可以是塑料袋、收纳盒或集装箱）。有的物件具有弹性，因此我们可以用力压，让它的体积更小；有的东西则弹性不大，只能直接放进容器中。但是无论如何，只要我们把东西归置到一起，就会便于我们管理，而且在对这一群东西进行移动的时候会更加方便。举一个通俗的例子，如果现在我们有6瓶矿泉水，一个人一双手要把这些水移动到操场将会非常困难。但是如果我们再提供一个背包，把6瓶矿泉水都放进去，尽管背包和矿泉水的容积可能没有变小，但是移动这些矿泉水就会非常便捷。因此压缩文件格式其实就是一定组织形式的容器，在大数据传输中还会使用hdf5等其他形式的打包管理工具，而且在R中也有便捷的接口可以调用（比如hdf5可以使用hdf5r包进行调用），感兴趣的读者可以根据需要自行学习和了解。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html#excel文件批处理",
    "href": "化整为零：对文件进行批处理.html#excel文件批处理",
    "title": "8  化整为零：对文件进行批处理",
    "section": "8.3 Excel文件批处理",
    "text": "8.3 Excel文件批处理\nExcel 文件批处理是指对多个Excel文件进行一系列自动化操作，以提高效率和减少手动工作，这些操作包括创建、读取、修改、复制、移动和删除Excel文件。在数据处理和分析过程中，批处理操作可以显著节省时间，尤其是当需要处理大量结构类似的文件时。\nopenxlsx2是一个现代化的R包，用于处理Excel文件，提供了简洁的接口和丰富的功能，可以轻松创建、读取和编辑Excel工作表，支持多种Excel文件格式，包括xlsx、xlsm和xlsb格式，使得处理Excel文件变得更加高效和灵活。openxlsx2是对openxlsx（出现得更早的Excel数据处理包）的改进，通过整合pugixml库解决了在解析XML文件方面的问题，并提供了更强大的样式管理、图表支持和数据处理功能，使得创建、编辑和修改xlsx文件变得更加简单和可靠。\nopenxlsx2的核心功能包括： 1. XML解析改进：通过整合pugixml库，解决了在解析XML文件方面的问题。 2. 样式管理：提供了更强大的样式管理功能，包括对字体、背景颜色、数字格式等的灵活控制。 3. 图表支持：支持原生图表的创建和编辑，使得在xlsx文件中插入图表变得更加简单。 4. 数据处理：提供了丰富的数据处理功能，包括数据验证、条件格式、小图标等，提高了数据处理的灵活性和效率。\n其中，常用的函数包括：\n\nwb_load：加载xlsx文件为工作簿对象。\nwb_add_worksheet：添加工作表到工作簿。\nwb_add_data：将数据添加到工作表中。\nwb_save：保存工作簿为xlsx文件。\nwb_to_df：将工作表转换为数据框。\n\n这里我们不会浪费篇幅来介绍工具包的技术细节，感兴趣的读者可以参考官方文档（https://janmarvin.github.io/openxlsx2/）进行学习。下面我们将会直接在实践中对综合运用本章介绍的工具，从而对文件进行高效的批处理。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html#综合实践",
    "href": "化整为零：对文件进行批处理.html#综合实践",
    "title": "8  化整为零：对文件进行批处理",
    "section": "8.4 综合实践",
    "text": "8.4 综合实践\n在本部分，会首先生成一个数据集，然后模拟现实情况的需求，对数据进行各式各样的批处理，从而展示如何利用R语言各种工具包对各种文件进行高效的操作。\n\n8.4.1 环境配置与数据生成\n首先，我们会加载必要的工具包：\n\nlibrary(pacman)\np_load(fs,openxlsx2,archive,tidyfst,tidyverse)\n\n然后，我们来生成相关的数据集：\n\nnr_of_rows &lt;- 1e7\n\ndf &lt;- data.table(\n    Logical = sample(c(TRUE, FALSE, NA), prob = c(0.85, 0.1, 0.05), nr_of_rows, replace = TRUE),\n    Integer = sample(1L:100L, nr_of_rows, replace = TRUE),\n    Real = sample(sample(1:10000, 20) / 100, nr_of_rows, replace = TRUE),\n    Factor = as.factor(sample(labels(UScitiesD), nr_of_rows, replace = TRUE))\n  )\n\ndf\n\n#           Logical Integer  Real       Factor\n#            &lt;lgcl&gt;   &lt;int&gt; &lt;num&gt;       &lt;fctr&gt;\n#        1:    TRUE      75 89.32      NewYork\n#        2:    TRUE      39  6.99      NewYork\n#        3:    TRUE      76 62.89        Miami\n#        4:    TRUE      41 41.42      Atlanta\n#        5:    TRUE      36 53.85 SanFrancisco\n#       ---                                   \n#  9999996:    TRUE      37 66.42      Seattle\n#  9999997:    TRUE      46  6.99        Miami\n#  9999998:    TRUE      78 53.85      NewYork\n#  9999999:   FALSE      85 62.89   LosAngeles\n# 10000000:    TRUE      20 46.48      Atlanta\n\n由于数据集生成具有一定的随机性，因此读者生成的数据框不会与上面展示的数据框完全一致，但是不影响试验的开展。经过观察，我们知道数据集一共有4列，分别是逻辑型、整数型、数值型和因子型数据。我们不妨来检测一下数据的大小：\n\nobject_size(df)\n# 190.7 Mb\n\n\n\n8.4.2 数据的保存\n在这一部分，我们需要根据需求来对目标数据进行保存。我们的诉求和实现代码如下所示。 1. 在根目录下创建temp文件夹，实现代码如下：\n\ndir_create(\"temp\")\n\n\n在temp文件夹下创建csv文件夹，把数据根据Integer分组，保存在不同的csv文件中，实现代码如下：\n\n\ndir_create(\"temp/csv\")\ndf %&gt;% \n  split(by = \"Integer\") %&gt;%  # 根据Integer列进行分组\n  walk(\\(x){ # 使用walk函数进行迭代\n    fn = x %&gt;% head(1) %&gt;% pull(Integer)  # 根据Integer创建文件名\n    fwrite(x,path(\"temp\",\"csv\",fn,ext = \"csv\")) # 写出文件\n  })\n\n\n在temp文件夹下创建fst文件夹，把数据根据Integer分组，保存在不同的fst文件中，实现代码如下：\n\n\ndir_create(\"temp/fst\")\ndf %&gt;% \n  split(by = \"Integer\") %&gt;%  # 根据Integer列进行分组\n  walk(\\(x){ # 使用walk函数进行迭代\n    fn = x %&gt;% head(1) %&gt;% pull(Integer)  # 根据Integer创建文件名\n    export_fst(x,path(\"temp\",\"fst\",fn,ext = \"fst\")) # 写出文件\n  })\n\n\n\n8.4.3 文件的压缩\n在本部分中，我们会对先前生成的数据进行打包压缩。由于Excel文件已经是一整个文件，因此不需要再进行压缩操作。我们首先把csv文件都打包为zip文件：\n\narchive_write_dir(archive = \"temp/csv.zip\",dir = \"temp/csv\")\n\n另一方面，我们把fst文件都打包为tar文件：\n\narchive_write_dir(archive = \"temp/fst.tar\",dir = \"temp/fst\")\n\n\n\n8.4.4 文件的移动\n我们知道，文件打包后，移动会更加快。我们不妨来进行尝试，在temp中再建立一个dest文件夹，然后分别把csv文件夹和csv压缩包移动进去，并测试移动时间：\n\ndir_create(\"temp/dest\")\npst(file_move(\"temp/csv\",\"temp/dest/csv\"))\npst(file_move(\"temp/csv.zip\",\"temp/dest/csv.zip\"))\n\n如果效果不明显，可以尝试增加文件数量。\n\n\n8.4.5 保存为Excel文件\n在这一步中，我们会把fst中的1到3号文件转存在一个Excel文件中（命名为“1-3.xlsx”），分成不同的工作簿进行保存，实现方法如下：\n\nmap(path(\"temp\",\"fst\",1:3,ext = \"fst\"),import_fst) %&gt;% \n  write_xlsx(path(\"temp\",\"1-3\",ext = \"xlsx\"))\n\n\n\n8.4.6 文件的删除\n下面我们尝试删除temp文件夹，操作方法如下：\n\nfile_delete(\"temp\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html#小结",
    "href": "化整为零：对文件进行批处理.html#小结",
    "title": "8  化整为零：对文件进行批处理",
    "section": "8.5 小结",
    "text": "8.5 小结\n本章介绍了如何在R语言中利用fs、archive和openxlsx2这些工具包，从而对文件进行批处理操作。当我们有海量文件的时候，我们就能够利用命令行对其进行移动、删除、复制等操作，并可以对多个文件进行压缩打包以便于传输、存储和管理。尽管本章没有对各个工具包的特性细节进行介绍（比如openxlsx2能够进行图片插入、表格格式设置、样式的更改等），这些内容笔者认为应该在读者需要的时候自行查询帮助文档，进而进行实现。通过本章的学习，相信读者可以了解并熟悉在R语言中如何利用脚本对多个文件进行批处理。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "化整为零：对文件进行批处理.html#练习",
    "href": "化整为零：对文件进行批处理.html#练习",
    "title": "8  化整为零：对文件进行批处理",
    "section": "8.6 练习",
    "text": "8.6 练习\n\n请设计一套代码，来测试各种压缩方法的特点（压缩比率、压缩时间），并对压缩后文件传输的速度进行测试。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>化整为零：对文件进行批处理</span>"
    ]
  },
  {
    "objectID": "跨语言召唤术：在R中调用其他编程工具.html",
    "href": "跨语言召唤术：在R中调用其他编程工具.html",
    "title": "9  跨语言召唤术：在R中调用其他编程工具",
    "section": "",
    "text": "9.1 调用C++\n在R语言中调用C++主要是为了提升性能和效率。R适合数据分析和统计建模，但在处理计算密集型任务、大规模数据集或复杂算法时可能表现较慢。通过调用C++，可以利用其高效的计算能力和更好的内存管理，显著加快计算速度，满足实时处理和低延迟要求，进而优化整个工作流程。\nRcpp包是一个强大且流行的R扩展包，用于在R中集成和调用C++代码。它提供了一个简洁的接口，使得在R中编写和执行C++代码变得更加容易，从而结合了R的简便性和C++的高效性。通过Rcpp，用户可以在R环境中执行高性能计算任务，并且可以轻松地在两种语言之间传递数据，极大地提高了计算效率和代码执行速度。\n一般来说，有两种方案能够利用Rcpp包来在R环境中对C++进行调用。一种方法是使用cppFunction函数，在R中直接对C++函数进行定义，然后调用。比如我们想要判断一个数字是否为基数，可以这样进行操作：\n# 加载包\nlibrary(Rcpp)\n\n# 函数定义\ncppFunction(\"bool isOddCpp(int num) {\n   bool result = (num % 2 == 1);\n   return result;\n}\")\n\n# 函数调用\nisOddCpp(42L)\n需要注意的是，在C++中，每个变量都需要进行定义，声明其属于什么数据类型（比如我们这里声明函数接受一个整数，返回一个布尔变量，即逻辑变量）。\n另一种调用方法是先把函数保存为cpp文件 （文件名为“fibonacci.cpp”，放在根目录下的data文件夹中），然后通过sourceCpp函数进行调用。比如我们先把以下C++函数保存为cpp文件，我们想要计算斐波那契序列第n个数字：\n然后，我们利用sourceCpp函数在 R 环境中直接编译和加载 C++ 代码：\nsourceCpp(\"data/fibonacci.cpp\")\n现在，我们就可以直接在R中对其进行调用了：\nfibonacci(20)\n# 6765\n在上面的例子中，我们展示了如何简单地在R中调用C++。在实际应用中，当我们发现某一个步骤非常消耗时间，而又可以使用C++进行实现，就可以尝试是否能够通过Rcpp来突破这个瓶颈。关于如何更好地充分利用Rcpp工具，可以参考开源图书Rcpp for everyone，该书全面地介绍了Rcpp包的使用方法。另外，更多最新的讯息，可以参考官方文档https://www.rcpp.org/。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>跨语言召唤术：在R中调用其他编程工具</span>"
    ]
  },
  {
    "objectID": "跨语言召唤术：在R中调用其他编程工具.html#调用c",
    "href": "跨语言召唤术：在R中调用其他编程工具.html#调用c",
    "title": "9  跨语言召唤术：在R中调用其他编程工具",
    "section": "",
    "text": "#include \"Rcpp.h\"\n\n// [[Rcpp::export]]\nint fibonacci(const int x) {\n   if (x &lt; 2) return(x);\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>跨语言召唤术：在R中调用其他编程工具</span>"
    ]
  },
  {
    "objectID": "跨语言召唤术：在R中调用其他编程工具.html#调用sql",
    "href": "跨语言召唤术：在R中调用其他编程工具.html#调用sql",
    "title": "9  跨语言召唤术：在R中调用其他编程工具",
    "section": "9.2 调用SQL",
    "text": "9.2 调用SQL\n在R中调用SQL是为了高效地访问和处理存储在关系型数据库中的大规模数据。SQL擅长数据查询、过滤、排序和聚合操作，而R则强于数据分析和可视化。通过在R中调用SQL，用户可以结合两者的优势，实现数据的高效管理、清洗和预处理，简化复杂的数据操作流程，并提升整体数据处理和分析的效率。这样，用户可以在R中方便地进行端到端的数据分析，从数据库提取数据到分析和可视化，均在一个环境中完成。\n尽管当前R也已经用强大的数据管理和操作能力，但是有的时候我们仍然需要与传统的关系型数据库进行协作，复用一些SQL代码，那么就必须利用相应的工具进行实现。当前R几乎可以与任意数据库进行连接，这里我们将会以SQLite数据库为例，看看如何在R中对数据库进行一些基本操作。\n\n9.2.1 数据库的连接\n如果已经有一个sqlite数据库（假设数据库名为“my-db.sqlite”），需要进行连接，可以这样操作：\n\nlibrary(pacman)\np_load(DBI,RSQLite)\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"my-db.sqlite\")\n\n需要注意的是，如果当前没有该名称的数据库，那么系统会自动在根目录下创建一个数据库。如果我们仅仅需要构建一个临时数据库做实验，可以不指定数据库的名称，操作如下：\n\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"\")\n\n如果需要关闭对数据库的连接，可以这样操作：\n\ndbDisconnect(mydb)\n\n\n\n9.2.2 载入数据\n如果我们要把R中的数据集载入到数据库中，可以使用dbWriteTable函数，操作方法如下：\n\n# 创建临时数据库连接\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"\")\n\n# 写出mtcars数据集\ndbWriteTable(mydb, \"mtcars\", mtcars)\n\n# 写出iris数据集\ndbWriteTable(mydb, \"iris\", iris)\n\n# 观察数据库中有哪些表格\ndbListTables(mydb)\n#&gt; [1] \"iris\"   \"mtcars\"\n\n\n\n9.2.3 数据查询\n如果需要对数据进行查询，可以使用dbGetQuery函数，比如我们想要取mtcars表格中的前5行数据，可以这样操作：\n\ndbGetQuery(mydb, 'SELECT * FROM mtcars LIMIT 5')\n#&gt;    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n关于如何在R中调用数据库进行各式操作，可以参考R for Data Science (2e) 中的相关章节。由于数据库是大数据分析的重要技术方案之一，在本教程后续章节将会对其进行更加详尽的讲解。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>跨语言召唤术：在R中调用其他编程工具</span>"
    ]
  },
  {
    "objectID": "跨语言召唤术：在R中调用其他编程工具.html#小结",
    "href": "跨语言召唤术：在R中调用其他编程工具.html#小结",
    "title": "9  跨语言召唤术：在R中调用其他编程工具",
    "section": "9.3 小结",
    "text": "9.3 小结\n本章以在R语言中调用C++和SQL为例，介绍了如何在一门语言中利用其它的编程工具来提高数据分析的效率。实际上，R语言还可以调用很多别的编程工具，包括Python（reticulate）、Java（rJava）、Rust（rextendr）等。一般来说，调用C++有利于我们在递归、循环等操作中提高代码的运行效率，而调用Java和Python等工具则是希望能够直接把现成的工具包集成到R环境中，使得工作能够在R环境中一并完成，从而提高工作效率。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>跨语言召唤术：在R中调用其他编程工具</span>"
    ]
  },
  {
    "objectID": "跨语言召唤术：在R中调用其他编程工具.html#练习",
    "href": "跨语言召唤术：在R中调用其他编程工具.html#练习",
    "title": "9  跨语言召唤术：在R中调用其他编程工具",
    "section": "9.4 练习",
    "text": "9.4 练习\n\n请使用Rcpp来实现冒泡排序法，并利用R语言也写一套方法，比较两者之间的运行速度。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>跨语言召唤术：在R中调用其他编程工具</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html",
    "href": "时间换空间：大数据流式计算.html",
    "title": "10  时间换空间：大数据流式计算",
    "section": "",
    "text": "10.1 基本思想\n在计算机内存有限的情况下，处理大型文件的一种有效方法是文件的分块操作。分块操作是将大型文件分割成多个较小的块，逐块进行处理，而不是一次性将整个文件加载到内存中。具体步骤如下：\n这种分块操作方法可以有效利用有限的内存资源，避免内存溢出，适用于需要处理大规模数据的场景，如数据分析、机器学习预处理和日志处理等。下面我们将以R语言为例，介绍如何在R中完成大数据流式计算（主要使用分块方法），针对不同的文件类型（如csv、fst等），我们会给出不同的方案。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html#基本思想",
    "href": "时间换空间：大数据流式计算.html#基本思想",
    "title": "10  时间换空间：大数据流式计算",
    "section": "",
    "text": "读取文件的一部分：利用文件指针或分块读取方法，逐块读取文件中的数据到内存中。\n处理当前块：对读入内存的这一部分数据进行处理，例如计算统计信息、数据转换等。\n保存处理结果：将处理后的数据或中间结果保存到磁盘或汇总到一个结果集合中。\n继续读取下一块：重复上述过程，直到文件的所有部分都处理完毕。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html#csv分块计算",
    "href": "时间换空间：大数据流式计算.html#csv分块计算",
    "title": "10  时间换空间：大数据流式计算",
    "section": "10.2 csv分块计算",
    "text": "10.2 csv分块计算\n如果我们现在有一份4G的csv文件需要处理，但是我们的计算机内存只有2G，在不扩容的基础上，我们无法一次性把整个文件载入到环境中，那么就必须分块计算。在R语言中，最佳的csv分块计算可以使用readr包的read_csv_chunked函数进行实现。函数中除了指定文件所在路径外，至少还需要设置两个参数：1、callback：对每次分块要做什么处理；2、chunk_size：每次分块处理多少行数据。下面我们用官方文档的例子进行演示：\n\n# 定义每次分块的处理\nf &lt;- function(x, pos) subset(x, gear == 3) # 筛选出gear为3的行\n\n# 执行分块读取\nread_csv_chunked(\n  readr_example(\"mtcars.csv\"),  # 文件所在路径\n  DataFrameCallback$new(f),  # 设置每次读取后的操作\n  chunk_size = 5) # 设置每次读5行\n\n需要注意的是，在定义函数的时候，我们总是需要在函数中设定两个参数，其中x是分块的数据框，而pos是位置，我们只需要放在那里即可。在上面的操作中，我们相当于是把符合一定条件的行筛选出来，只要筛选出来的数据小于我们的最大内存，那么我们就可以成功进行操作。\n除了分块读取之外，其实我们在写出csv的时候，也可以使用readr包中的write_csv函数进行分块写出操作，只需要把append参数设置为TRUE即可。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html#fst分块计算",
    "href": "时间换空间：大数据流式计算.html#fst分块计算",
    "title": "10  时间换空间：大数据流式计算",
    "section": "10.3 fst分块计算",
    "text": "10.3 fst分块计算\n我们在前面的章节中提到了可以使用fst文件格式对数据框进行快速的存取，事实上，fst还支持随机访问（Random access）。这就意味着，我们可以通过制定要读取数据的位置来定向获取文件的部分数据，这一特性为分块计算创造了有利条件。以下是笔者自定义的函数，能够对任意的fst文件进行分块计算：\n\nlibrary(pacman)\np_load(fst,data.table)\n\nimport_fst_chunked = \\(path,chunk_size = 1e4,chunk_f = identity,combine_f = rbindlist){\n  # 计算文件总行数\n  parse_fst(path) %&gt;% nrow() -&gt; ft_nrow\n  \n  # 计算分块起始位置\n  seq(from = 1,to = ft_nrow,by = chunk_size)-&gt; start_index\n  \n  # 计算分块终结位置\n  c(start_index[-1] - 1,ft_nrow) -&gt; end_index\n  \n  # 执行分块计算操作\n  Map(f = \\(s,e){\n    read_fst(path,from = s,to = e,as.data.table = TRUE) %&gt;%  # 对数据进行分块读取\n      chunk_f # 对每个分块要进行的计算\n  },start_index,end_index) %&gt;% \n    combine_f # 对分块计算结果进行汇总\n}\n\n在上面定义的函数中，path代表文件所在路径，chunk_size代表每块读入行数（默认值为1万行），chunk_f是需要对每个块进行的函数操作（默认为返回数据库本身，但是一般来说在分块计算后应该获得的结果要比原来的数据量少得多才有意义），而combine_f则负责对每个分块最后获得的结果进行汇总（获得的结果是一个装着所有分块结果的列表，因此需要对列表中的所有元素进行合并）。\n下面，我们会应用这个函数把文件中Integer为7的记录都筛选出来，操作方法如下：\n\n# 构造数据框\nnr_of_rows &lt;- 1e7 # 构造1千万行数据\ndf &lt;- data.frame(\n  Logical = sample(c(TRUE, FALSE, NA), prob = c(0.85, 0.1, 0.05), nr_of_rows, replace = TRUE),\n  Integer = sample(1L:100L, nr_of_rows, replace = TRUE),\n  Real = sample(sample(1:10000, 20) / 100, nr_of_rows, replace = TRUE),\n  Factor = as.factor(sample(labels(UScitiesD), nr_of_rows, replace = TRUE))\n)\n\n# 写出fst文件\nfst_file &lt;- tempfile(fileext = \".fst\")\nwrite_fst(df, fst_file)\n\n# 分块筛选出Integer为7的记录，赋值给res\nres = import_fst_chunked(fst_file,chunk_f = \\(x) x[Integer==7])\n\n这里我们仅仅是进行了筛选操作，如果操作是可以分开执行并汇总的，理论上都可以使用该函数进行实现，比如求和、求最大值、求最小值等。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html#数据库分块计算",
    "href": "时间换空间：大数据流式计算.html#数据库分块计算",
    "title": "10  时间换空间：大数据流式计算",
    "section": "10.4 数据库分块计算",
    "text": "10.4 数据库分块计算\n有的时候，我们的数据保存在数据库中，需要导入到R环境中进行处理，那么我们就需要从数据库中分块导出，然后每次导出的时候进行处理并保存结果，最后再进行结果的汇总统计。这里我们还是以SQLite数据库为例，看看如何对数据库中的结果进行分块。首先我们先配置好环境：\n\n# 加载包\nlibrary(RSQLite)\n\n# 建立临时数据库\nmydb &lt;- dbConnect(RSQLite::SQLite(), \"\")\n\n# 写出mtcars表格到数据库中\ndbWriteTable(mydb, \"mtcars\", mtcars)\n\n然后，我们尝试对存入的数据分块进行读出，并显示每一块的行数量是多少：\n\n# 记录查询\nrs &lt;- dbSendQuery(mydb, 'SELECT * FROM mtcars')\n\n# 分块处理\nwhile (!dbHasCompleted(rs)) { # 当查询没结束的时候，继续执行循环\n  df &lt;- dbFetch(rs, n = 10)  # 每次取出查询的10条记录\n  print(nrow(df))  # 打印查询记录的行数量\n}\n#&gt; [1] 10\n#&gt; [1] 10\n#&gt; [1] 10\n#&gt; [1] 2\n\n我们可以看到，使用dbSendQuery函数不会马上执行，只有采用dbFetch函数的时候才会将数据取出，而其中的n参数可以控制每次取出块的大小。这样，我们就可以对数据库中的数据进行分块操作了。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html#小结",
    "href": "时间换空间：大数据流式计算.html#小结",
    "title": "10  时间换空间：大数据流式计算",
    "section": "10.5 小结",
    "text": "10.5 小结\n在本章中，我们探讨了单机内存有限条件下如何突破空间限制进行大数据分析。对于大于内存的数据，只能每次从文件中提取小于计算机内存的数据进行处理，然后最后再把每次分块的结果汇总到一起。这样，原来有限内存无法解决的问题，就可以通过步步为营的思路，循序渐进地解决掉。这种方法会花费更多的时间，但是会让本来有限内存无法完成的数据处理变为可能。另一种情况下，很多文件可能本身就是分块存储的（比如一个大数据可以按照某一列进行分组，存成多个小文件），这样我们就可以先对每一个块的数据进行处理，然后再对处理结果进行汇总。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "时间换空间：大数据流式计算.html#练习",
    "href": "时间换空间：大数据流式计算.html#练习",
    "title": "10  时间换空间：大数据流式计算",
    "section": "10.6 练习",
    "text": "10.6 练习\n\n生成一个比本机内存要大的数据框，然后对数据进行求最大值和最小值的操作。在生成过程中，尝试使用readr::write_csv函数，再读取处理过程中尝试使用read_csv_chunked函数。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>时间换空间：大数据流式计算</span>"
    ]
  },
  {
    "objectID": "空间换时间：大数据并行计算.html",
    "href": "空间换时间：大数据并行计算.html",
    "title": "11  空间换时间：大数据并行计算",
    "section": "",
    "text": "11.1 基本思想\n并行计算的基本思想是通过将一个大型计算任务分解为多个小任务，并同时在多个处理单元上执行这些小任务，从而加速计算过程。与串行计算不同，串行计算依赖于单个处理器依次完成所有任务，而并行计算通过多处理器协同工作，可以显著提高计算效率和处理速度。这种方法充分利用了现代计算机多核、多处理器架构的优势，适用于大数据处理、高性能计算和实时数据分析等领域。在并行计算中，任务的分解、负载均衡、数据通信和同步是关键技术，通过合理的设计和优化，可以实现高效的并行处理。\n与前面所讲的流式计算类似，并行计算也对数据进行了分块。但是在流式计算中，分块的意义在于本机容量只够分析一块的内容因此不得不分块，而在并行计算中计算机容量远远大于数据总量，因此可以先把数据进行分块，然后利用多核心、多线程同时对多块数据同时进行处理，从而减少任务完成的时间（见图11.1）。\nFigure 11.1: 流式计算与并行计算概念示意图\n在R语言中，很多包（如data.table、xgboost等）在内部设计的时候已经设置了并行，因此我们不需要额外进行设置就可以享受并行带来的效率提升，我们一般把这种方式使用并行的手段称为隐式并行计算。隐式计算对用户隐藏了大部分细节，用户不需要知道具体数据分配方式、算法的实现或者底层的硬件资源分配，系统会根据当前的硬件资源来自动启动计算核心。而相对应的还有显示并行计算，显式计算要求用户能够自己处理算例中数据划分，任务分配，计算以及最后的结果收集。因此，显式计算模式对用户的要求更高，用户不仅需要理解自己的算法，还需要对并行计算和硬件有一定的理解。由于隐式计算并不需要我们进行学习就可以使用，因此在后面的部分，我们将会聚焦在显式并行计算，介绍如何在R中实现这种显式并行计算方法。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>空间换时间：大数据并行计算</span>"
    ]
  },
  {
    "objectID": "空间换时间：大数据并行计算.html#实现方案",
    "href": "空间换时间：大数据并行计算.html#实现方案",
    "title": "11  空间换时间：大数据并行计算",
    "section": "11.2 实现方案",
    "text": "11.2 实现方案\n\n11.2.1 基于future的并行框架简介\n无论是在高性能计算集群、服务器还是在个人笔记本或台式机上，都可以实现并行计算。总的来说，施行并行计算是为了加快计算、节省时间。在当前的R生态中，future框架是实现并行计算的最佳选择之一，本部门将会选取该框架的例子进行介绍。future包是R语言中一个灵活且强大的并行计算包，旨在简化并行和分布式计算的实现。它通过提供一致的接口和多种并行计划，允许用户轻松地编写并行代码。在这个体系中，提出了若干核心概念：\n\nfuture对象：future是一个抽象概念，表示一个异步计算任务。用户可以创建一个future对象，并将计算任务提交给它。任务会在后台执行，用户可以在以后获取其结果。\n计划（Plan）：future框架支持多种执行计划，定义了计算任务的执行方式。常见的计划包括：\n\n\nSequential（串行）：默认计划，按顺序执行任务。\nMultisession（多进程）：在多个R进程中并行执行任务。\nMulticore（多核）：在多个CPU核心上并行执行任务（不适用于Windows）。\nCluster（集群）：在计算机集群上分布式执行任务。\nRemote（远程）：在远程机器上执行任务。\n\n\n异步执行：future框架支持异步计算，即任务提交后，用户可以继续进行其他操作，直到需要获取结果时再等待任务完成。\n\nfuture框架提供简洁的接口和灵活的执行计划，简化了并行和分布式计算的实现，且能与其他包结合使用以扩展功能。它适用于多核并行计算、分布式计算和异步编程，且兼容不同操作系统和计算环境。总体而言，future框架为各种计算密集型和大规模数据处理应用提供了一致且高效的解决方案。\nFutureverse是一个围绕future包构建的并行和分布式计算生态系统，旨在简化这些复杂计算任务的实现。通过提供一套统一且灵活的接口，Futureverse使得用户能够在各种计算环境中轻松执行并行任务。这个生态系统包含了多个相互协作的包，每个包都专注于不同的功能领域，从而形成一个强大的并行计算工具集。核心包future提供了基础的并行和分布式计算功能，而Futureverse中的其他包则在此基础上，提供了更高层次和更具体的功能。例如，future.apply扩展了apply家族函数（如lapply、sapply等）的功能，使其能够在并行计算环境中运行；doFuture与foreach包集成，使得foreach循环可以利用future包的并行计算功能；furrr则与purrr包集成，允许用户使用purrr风格的函数式编程方式进行并行计算。还有future.batchtools，它与batchtools包集成，使用户能够在高性能计算集群中提交并管理并行任务。关于整个框架所包含的工具包，可以参考官方的介绍文档（https://www.futureverse.org/packages-overview.html）。Futureverse通过这些扩展包的无缝集成，极大地提升了future包的功能和应用范围。它不仅提供了简洁的接口和灵活的执行计划，还保证了极高的可扩展性和平台兼容性，适用于不同操作系统和计算环境。无论是多核并行计算、分布式计算，还是异步编程，Futureverse都能简化复杂计算任务的实现，大大提升了R语言在计算密集型和大规模数据处理中的应用能力。\n\n\n11.2.2 基于future.apply的并行操作实现\nFutureverse从底层到顶层的设计都非常强大而精妙，这使得用户在使用的时候也极其便利，几乎不需要做过多的额外设置就能够对数据处理使用并行操作。以future.apply包为例，如果在基本包中我们使用的操作lapply操作：\n\n# 对mtcars的每一列求均值，赋值给y\ny = lapply(mtcars,mean)\n\n如果要对这个操作进行并行化，可以这样操作：\n\n# 加载包\nlibrary(future.apply)\n\n# 设置并行计划：多进程\nplan(multisession)\n\n# 执行操作\ny = future_lapply(mtcars,mean)\n\n我们可以看到，我们只是多加载了future.apply包，然后设置了一个策略，最后把日常使用的lapply函数改为future_apply即可。future_apply包支持并行化的基本包向量化函数除了lapply之外，还包括sapply、apply、tapply、vapply等，感兴趣的读者可以在官网（https://future.apply.futureverse.org/index.html）进行查阅。\n\n\n11.2.3 异步计算\n异步计算是一种编程技术，通过并行或分布式处理来同时执行多个任务，而不必等待每个任务顺序完成。这种方法使得程序能够在处理耗时任务时保持响应性，从而提高整体性能。在R语言中，future包提供了一种简单而强大的方式来实现异步计算。通过创建future对象，可以在后台执行计算任务，同时主线程可以继续进行其他操作。下面我们举一个例子：\n\n# 加载包\nlibrary(future)\n\n# 定义一个耗时求和任务\nslow_sum &lt;- function(x) {\n  sum &lt;- 0\n  \n  for (value in x) {\n    Sys.sleep(1.0)  ## 每次迭代都要暂停1秒\n    sum &lt;- sum + value\n  }\n  \n  sum\n}\n\n# 创建future指令，让任务在后台执行\nf = future(slow_sum(1:5))\n\n# 观察任务是否执行成功\nr0 = resolved(f)\n\n# 获取任务返回值\nv = value(f)\n\n# 观察任务是否执行成功\nr1 = resolved(f)\n\n在上面的代码中，我们首先定义了一个耗时的任务，只要求和数字的数量多1个，那么我们需要的时间就多1秒。然后，我们先使用future函数创建一个指令并进行赋值，它会在后台运行，可以使用resolved函数观察指令是否运行完毕。我们可以看到，r0的值为FALSE，而r1的值为TRUE。可以看到，取值前其实表达式还没有运行完毕，但是在取值必定需要等程序运行完毕才能返回结果，因此取值之后再去检查是否运行完毕，必定是已经完成了。\n实际应用中，我们往往会直接进行赋值，而不需要总是去看前面的任务是否完成，那么可以使用future中给出的特殊赋值符号%&lt;-%，它会对右边的表达式进行评估，然后在完成后第一时间赋值给左边的变量。举例入下：\n\nx %&lt;-% slow_sum(1:3) # 马上执行完毕，计算在后台运行，完成后马上完成赋值\ny %&lt;-% slow_sum(2:4)\nz = x + y\n\n看到上面的代码，我们就可以知道，我们以后在进行代码运行的时候，可以先让一个代码进行推进，然后继续写代码，等到后面代码写好了，前面代码也运行完了，这样可以提高效率。否则的话，我们每次都必须等待前面的代码运行完毕，才能够进行下一步的操作。关于异步操作更多的特性，推荐参考设计者提供的教程（https://henrikbengtsson.github.io/course-stanford-futureverse-2023/future-api.html）。在实践中，异步计算特别适用于需要同时处理多个独立任务的场景，例如网络请求、文件I/O操作和复杂计算任务。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>空间换时间：大数据并行计算</span>"
    ]
  },
  {
    "objectID": "空间换时间：大数据并行计算.html#注意事项",
    "href": "空间换时间：大数据并行计算.html#注意事项",
    "title": "11  空间换时间：大数据并行计算",
    "section": "11.3 注意事项",
    "text": "11.3 注意事项\n\n在使用并行计算之前，第一要务是确认计算是否可以并行化。并行操作的一个重要特性就是符合map-reduce的逻辑，能够进行划分-操作-合并（split-apply-combine）。比如计算斐波那契数列，这个任务每次计算一个数字都需要前面两个数字，那么这就是一个串行的任务，无法利用并行进行计算。在确定采用并行计算之前，必须确保不同任务之间具有独立性，不会相互依赖，才可以。一个简单的方法是，尝试把即将要并行化的序列打乱，看看得到的结果是否与原始一致，如果是一致的，那么一般都可以进行并行化操作。\n使用并行计算要关心异常处理。如果在并行操作中，其中一个分支出现了错误，有时候会导致所有计算都化为泡影，这样浪费了大量的时间和资源，最后却没有任何结果。正确的做法是在设计程序的时候就使用tryCatch等异常处理函数，对出错的情况进行标志，那么当异常情况出现的时候，我们可以对它记录下来，同时保证已经运行的结果得到保存。最后可以锚定异常出现的位置，然后对其进行修正。\n在future框架中使用并行计算之前，总是先要配置并行计划。一般情况下，选用多线程（multisession）比较稳健，但是如果在条件允许的情况下，往往使用其他的计划会得到更高的性能。比如计算机有多个核心，而且操作系统不是Windows系统的时候，使用多核（multicore）配置往往会得到更佳的效果，因为这种配置能够通过派生（forked）进程来减少资源的复制与传输。更多关于并行计划的配置，可以参考官方提供的资料（https://www.futureverse.org/backends.html）。\nFutureverse生态中，很多顶层设计都是换汤不换药的表达，实质上都是让任务分而治之，然后汇总。因此笔者建议用户选用一种简单的即可。如果是习惯使用tidyverse的用户可以选用furrr包，而对基本包比较熟悉的用户则推荐使用future.apply包。还有一些用户可能已经使用过foreach包，那么可以尝试一下doFuture包。这些顶层设计只是为了用户习惯设计不同的调用函数，实际上进行的并行操作都是统一的。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>空间换时间：大数据并行计算</span>"
    ]
  },
  {
    "objectID": "空间换时间：大数据并行计算.html#小结",
    "href": "空间换时间：大数据并行计算.html#小结",
    "title": "11  空间换时间：大数据并行计算",
    "section": "11.4 小结",
    "text": "11.4 小结\n并行计算通过将大任务分解为多个独立的小任务，并在多个处理器上同时运行，从而提高数据处理速度和资源利用率。本章介绍了并行计算的基本概念，并讲述了如何在R语言中利用future框架来便捷地实现并行计算。通过这种方式，可以显著加快计算密集型任务的处理速度，适用于各种大规模数据处理和复杂计算场景。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>空间换时间：大数据并行计算</span>"
    ]
  },
  {
    "objectID": "空间换时间：大数据并行计算.html#练习",
    "href": "空间换时间：大数据并行计算.html#练习",
    "title": "11  空间换时间：大数据并行计算",
    "section": "11.5 练习",
    "text": "11.5 练习\n\n列举可以使用并行的情况。\n设计一个任务，看看并行计算能给你的任务节省多少时间。\n运行以下代码，体会一下异步计算的特性（推荐参考材料）。\n\n\nf = future(slow_sum(1:60))\n\nwhile(!resolved(f)){\n  message(\"等一下，我还在跑...\")\n  Sys.sleep(1.0)\n}\n\nmessage(\"搞定！\")\n\nv = value(f)\n\nv",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>空间换时间：大数据并行计算</span>"
    ]
  },
  {
    "objectID": "从内存到外存：用数据库管理数据.html",
    "href": "从内存到外存：用数据库管理数据.html",
    "title": "12  从内存到外存：用数据库管理数据",
    "section": "",
    "text": "12.1 磁盘数据处理\n磁盘数据处理（On-Disk Data Processing）是一种在数据处理过程中主要依赖磁盘等外部存储设备来存储和处理数据的技术。当数据集的规模超出内存容量时，这种方法尤其有效，因为它能够利用磁盘的大容量来存储大量数据。磁盘数据处理通过将数据分块存储在磁盘上，并在需要时逐块读取和处理，从而避免了内存不足的问题。\n这种方法的一个显著优势是其可扩展性，能够处理比内存容量大得多的数据集，非常适合大数据分析和处理任务。例如，数据库管理系统（如MySQL、PostgreSQL）、分布式文件系统（如Hadoop HDFS）以及流式处理框架（如Apache Kafka）都广泛使用磁盘数据处理技术。然而，由于磁盘的读写速度相对较慢，磁盘数据处理可能会面临较高的I/O延迟。为了优化性能，通常会采用高速缓存、数据预取和并行I/O操作等技术。\n总的来说，磁盘数据处理通过有效利用外部存储设备，为大规模数据处理提供了一种解决方案。尽管存在I/O速度较慢的挑战，但通过适当的优化，可以在大数据环境中实现高效的数据处理和分析。在本章中，我们会描述如何在R环境中调用数据库资源，同时还会介绍另一类新兴的大数据处理系统（Arrow和Polars），这些大数据处理方案允许用户把数据先存储为Parquet格式，实际处理的时候不需要把数据载入环境就能够对大数据进行分析。通过对这些工具的介绍，我们可以有效地利用计算机的磁盘资源来对比内存大的数据进行高效处理。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>从内存到外存：用数据库管理数据</span>"
    ]
  },
  {
    "objectID": "从内存到外存：用数据库管理数据.html#数据库操作以duckdb为例",
    "href": "从内存到外存：用数据库管理数据.html#数据库操作以duckdb为例",
    "title": "12  从内存到外存：用数据库管理数据",
    "section": "12.2 数据库操作——以duckdb为例",
    "text": "12.2 数据库操作——以duckdb为例\n在实际应用中，大量数据存储在数据库，因此掌握如何访问这些数据至关重要。如果每次访问数据都需要请求数据库管理人员，这样会非常麻烦，因此在保障数据安全的前提下，最佳的方案是我们能够直接对数据库的数据进行自由访问。本部分会介绍如何使用 DBI 包连接到数据库，并通过SQL查询检索数据。SQL（Structured Query Language），即结构化查询语言，是数据库的通用语言，是所有数据科学家都需要掌握的重要工具。然而，在R语言中对数据库进行访问可以跳过对SQL的学习（如果你尚未掌握SQL的话），直接使用dplyr的核心函数来直接对数据进行筛选、排序、分组汇总等各式操作，因为底层能够借助dbplyr工具包把dplyr的代码转为SQL代码，从而完成对数据库的控制。下面我们将会循序渐进地介绍如何在R中对数据库的资源进行访问和处理。\n\n12.2.1 基本环境配置\n在本部分，我们会加载需要的R包。其中，DBI包负责对数据库进行连接并执行SQL语句，dbplyr负责把dplyr语句转换为SQL语句，而tidyverse包则包含了各种数据处理的基本操作函数。这里我们会以控制DuckDB 数据库为例，因此同时会加载duckdb包。执行代码如下：\n\nlibrary(pacman)\np_load(DBI,dbplyr,tidyverse,duckdb)\n\n这里我们稍微对DuckDB 数据库进行一个介绍（标识见图12.1），DuckDB 是一个嵌入式的 SQL 数据库管理系统，旨在提供高效的数据查询和处理功能。它设计用于数据分析和应用程序中的嵌入式数据库需求，支持标准的 SQL 查询语言，同时具备优秀的性能和低延迟。DuckDB 的特点包括内存友好型设计，支持在内存中处理大规模数据集，同时具备与多核处理器和并行计算环境的良好集成能力。它还提供了与许多流行数据分析工具的集成接口，如 R和Python，使得用户可以轻松地在其数据分析工作流中使用 DuckDB 进行快速和高效的数据查询与处理。\n\n\n\n\n\n\n\n\nFigure 12.1: DuckDB数据库Logo\n\n\n\n\n\n\n\n12.2.2 数据库的连接\n完事开头难，对数据库操作的第一步就是必须让R环境与数据库连接起来。在R中要与数据库连接，一般需要两个包：其一是DBI，这个包提供了用于数据库连接、数据传输、执行查询的通用函数；其二是针对用户连接数据库系统的定制包，这些包能够把DBI命令转化为特定数据库系统能够解读的命令，比如要使用SQLite就需要RSQLite包，使用PostgreSQL就需要使用PostgreSQL包。对于咱们的试验来说，需要使用duckdb包来完成这个操作，实现方法如下：\n\ncon = dbConnect(duckdb())\n\n需要注意的是，这里我们创建的是一个虚拟临时数据库，因此当我们推出R环境的时候数据库就会自动被清楚，非常适合用来进行一次性的试验。如果需要连接一个已经存在的数据库，或者创建一个新的数据库，只需要对相关的参数（dbdir）进行设置即可。如果要连接不同的数据库，那么连接的时候需要的参数也会有所不同，相关说明可以参阅DBI::dbConnect函数的帮助文档。\n\n\n12.2.3 数据操作基础\n在创建了数据库连接后，首先我们可以对这个数据库载入数据，这可以使用dbWriteTable函数进行实现：\n\n# 把iris数据集载入到数据库中\ndbWriteTable(con, \"iris\", iris)\n\n# 把ggplot2中的diamonds数据集载入到数据库中\ndbWriteTable(con, \"diamonds\", diamonds)\n\n在上面的函数中，我们知道在函数中需要声明3个要素，分别是数据库连接、表名称和数据。载入之后，我们可以观察一下数据库中都有哪些表：\n\ndbListTables(con)\n\n[1] \"diamonds\" \"iris\"    \n\n\n如果要取出里面的表格，比如我们想要取出iris数据集，有两种方法：\n\n# 方法1：使用dbReadTable\ncon %&gt;% \n  dbReadTable(\"iris\") %&gt;% \n  as_tibble()\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n# 方法2：使用tbl\ntbl(con,\"iris\") %&gt;% \n  as_tibble()\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n在上面两种方法中，方法1的as_tibble其实可以去除，我们只是为了显示方便，所以进行这一步操作，但是即使没有这样操作也可以得到传统的数据框结构。在方法2中，则必须使用as_tibble表示对数据进行调用，事实上也可以使用collect函数对数据进行提取。当然， 还有一种方案就是直接写SQL语句对数据进行查询，方法如下：\n\nsql &lt;- \"\n  SELECT *\n  FROM iris\n\"\nas_tibble(dbGetQuery(con, sql))\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n使用dbGetQuery函数能够直接对数据库传SQL语句并进行执行。 现在，我们就可以自由地使用dplyr中的动词对数据进行各式操作。比如我们想要对diamond表进行一系列操作，方法如下：\n\ndiamonds_db &lt;- tbl(con, \"diamonds\")\ndiamonds_db\n\n# Source:   table&lt;diamonds&gt; [?? x 10]\n# Database: DuckDB v1.0.0 [Admin@Windows 10 x64:R 4.4.1/:memory:]\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ more rows\n\nbig_diamonds_db &lt;- diamonds_db %&gt;% \n  filter(price &gt; 15000) %&gt;% \n  select(carat:clarity, price)\nbig_diamonds_db\n\n# Source:   SQL [?? x 5]\n# Database: DuckDB v1.0.0 [Admin@Windows 10 x64:R 4.4.1/:memory:]\n   carat cut       color clarity price\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  1.54 Premium   E     VS2     15002\n 2  1.19 Ideal     F     VVS1    15005\n 3  2.1  Premium   I     SI1     15007\n 4  1.69 Ideal     D     SI1     15011\n 5  1.5  Very Good G     VVS2    15013\n 6  1.73 Very Good G     VS1     15014\n 7  2.02 Premium   G     SI2     15014\n 8  2.05 Very Good F     SI2     15017\n 9  1.5  Very Good F     VS1     15022\n10  1.82 Very Good G     SI1     15025\n# ℹ more rows\n\n\n需要注意的是，我们在这些操作中都没有对数据进行采集，因此这些赋值对象都还是一个数据连接，而不是R中的数据框。如果需要转化为数据框，可以这样操作：\n\ndiamonds_db %&gt;% as_tibble()\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\nbig_diamonds_db %&gt;% collect()\n\n# A tibble: 1,655 × 5\n   carat cut       color clarity price\n   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  1.54 Premium   E     VS2     15002\n 2  1.19 Ideal     F     VVS1    15005\n 3  2.1  Premium   I     SI1     15007\n 4  1.69 Ideal     D     SI1     15011\n 5  1.5  Very Good G     VVS2    15013\n 6  1.73 Very Good G     VS1     15014\n 7  2.02 Premium   G     SI2     15014\n 8  2.05 Very Good F     SI2     15017\n 9  1.5  Very Good F     VS1     15022\n10  1.82 Very Good G     SI1     15025\n# ℹ 1,645 more rows\n\n\n我们还需要知道的是，凡是能够用dplyr方法进行访问的操作，事实上都已经成功地把dplyr操作转化为了相对应的SQL语句，如果我们想看SQL语句转化的情况，可以使用show_query函数，实现方法如下：\n\nbig_diamonds_db %&gt;% \n  show_query()\n\n&lt;SQL&gt;\nSELECT carat, cut, color, clarity, price\nFROM diamonds\nWHERE (price &gt; 15000.0)\n\n\n基于这些操作，我们可以在磁盘上对数据库进行分析，然后把内存能够轻松容纳的结果导入到R环境中，进行进一步的分析和展示。在数据库使用完毕后，可以使用dbDisconnect函数关闭数据库连接：\n\ndbDisconnect(con)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>从内存到外存：用数据库管理数据</span>"
    ]
  },
  {
    "objectID": "从内存到外存：用数据库管理数据.html#基于arrow的大数据处理方案",
    "href": "从内存到外存：用数据库管理数据.html#基于arrow的大数据处理方案",
    "title": "12  从内存到外存：用数据库管理数据",
    "section": "12.3 基于Arrow的大数据处理方案",
    "text": "12.3 基于Arrow的大数据处理方案\nApache Arrow是一个跨语言的开发平台，用于高性能数据分析，提供了一种内存中的数据格式，旨在高效地共享数据而无需额外的序列化和反序列化步骤。它的设计目标是加速大数据处理和分析，使在处理和传输大规模数据集时表现出色。Arrow支持多种编程语言，包括C++, Java, Python, R等，使得不同语言之间的数据交换变得非常高效。其列式内存格式使数据在内存中的表示非常紧凑和高效，不仅减少了内存使用，还提升了CPU缓存命中率，从而加速数据处理。此外，通过Arrow的内存格式，不同进程和系统之间可以实现零拷贝的数据共享，大幅减少数据传输的开销。Arrow还与许多大数据系统（如Apache Parquet、Apache Spark、DuckDB等）无缝集成，支持高效的数据存储和处理。除了基本的数据类型，Arrow还支持复杂的数据结构和操作，如嵌套数据、时间戳和向量化操作。因此，Apache Arrow通过提供高效的内存格式和跨语言支持，为大数据处理和分析提供了一个强大而灵活的基础设施，极大地提升了数据密集型应用的性能。\n本部分聚焦的是如何利用Arrow来进行内存外的计算，在R包arrow中，open_dataset函数能够在不把数据载入到R环境的情况下对数据（可以是一份文件包含的数据，也可以是分散在多个文件中的数据；数据格式可以是CSV，也可以是parquet）进行查询操作，用户可以使用dplyr包提供的函数来对数据自由进行操作。在条件允许的情况下，我们推荐使用parquet来存储数据，然后再利用arrow包对其进行访问，因为Parquet格式有以下优点：\n\n作为一种专门为大数据需求设计的自定义二进制格式，Parquet文件通常比等效的CSV文件更小。Parquet依赖于高效的编码来减少文件大小，并支持文件压缩。这有助于加快parquet文件的速度，因为从磁盘到内存的数据量更少。\nParquet文件是列式存储的，这意味着它们是按列组织的，非常类似于R的数据框。这通常比按行组织的CSV文件在数据分析任务中表现更好。\nParquet文件是分块的，因此支持并行操作。而且，如果分组恰当的话，可以为数据操作节省很多时间。\n\n我们可以尝试把一份大数据集保存为分块的parquet文件，这可以利用arrow包的write_dataset函数进行实现。\n\n# 构造数据框\nnr_of_rows &lt;- 1e7 # 构造1千万行数据\ndf &lt;- data.frame(\n  Logical = sample(c(TRUE, FALSE, NA), prob = c(0.85, 0.1, 0.05), nr_of_rows, replace = TRUE),\n  Integer = sample(1L:100L, nr_of_rows, replace = TRUE),\n  Real = sample(sample(1:10000, 20) / 100, nr_of_rows, replace = TRUE),\n  Factor = as.factor(sample(labels(UScitiesD), nr_of_rows, replace = TRUE))\n)\n\n# 根据Factor进行分组，然后把数据写出到data文件夹中的test_parquet子文件夹\ndf %&gt;% \n  group_by(Factor) %&gt;% \n  write_dataset(\"data/test_parquet\",format = \"parquet\")\n\n我们可以观察一下文件夹中的文件信息：\n\np_load(arrow)\npq_path = \"data/test_parquet\"\n\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n\n# A tibble: 10 × 2\n   files                               size_MB\n   &lt;chr&gt;                                 &lt;dbl&gt;\n 1 Factor=Atlanta/part-0.parquet          2.02\n 2 Factor=Chicago/part-0.parquet          2.03\n 3 Factor=Denver/part-0.parquet           2.03\n 4 Factor=Houston/part-0.parquet          2.02\n 5 Factor=LosAngeles/part-0.parquet       2.02\n 6 Factor=Miami/part-0.parquet            2.03\n 7 Factor=NewYork/part-0.parquet          2.03\n 8 Factor=SanFrancisco/part-0.parquet     2.03\n 9 Factor=Seattle/part-0.parquet          2.03\n10 Factor=Washington.DC/part-0.parquet    2.02\n\n\n可以看到每一个文件大概2 MB左右，文件名是采用键值对方法进行命名的。现在，我们可以把数据从我们的环境中清除掉，然后使用另一种方式对其进行访问：\n\n# 清除构建的数据集\nrm(df)\n\nWarning in rm(df): object 'df' not found\n\n# 对保存的parquet数据集进行连接\ndf_pq = open_dataset(pq_path)\n\n# 观察数据信息\ndf_pq\n\nFileSystemDataset with 10 Parquet files\n4 columns\nLogical: bool\nInteger: int32\nReal: double\nFactor: string\n\n\n通过观察，我们知道open_dataset函数没有返回数据本身，但是能够探知数据每一列是什么类型的。下面让我们使用dplyr的函数来对其进行查询：\n\n# 构建查询\nquery = df_pq %&gt;% \n  filter(Factor == \"Atlanta\",Real &gt; 50) %&gt;% \n  group_by(Logical) %&gt;% \n  summarise(avg = mean(Integer)) %&gt;% \n  arrange(-avg)\n\n# 观察查询\nquery\n\nFileSystemDataset (query)\nLogical: bool\navg: double\n\n* Sorted by negate_checked(avg) [asc]\nSee $.data for the source Arrow object\n\n\n这一步不会直接执行，只会先记录我们需要执行什么内容。如果我们需要把内容收集起来，可以使用collect函数：\n\nquery %&gt;% collect()\n\n# A tibble: 3 × 2\n  Logical   avg\n  &lt;lgl&gt;   &lt;dbl&gt;\n1 NA       50.5\n2 TRUE     50.5\n3 FALSE    50.3\n\n\n这种数据处理的速度相当的快，如果我们使用open_dataset对CSV文件进行操作，也是能够实现的，但是速度会慢很多，读者不妨进行尝试。 最后需要提及的是，Arrow对DuckDB具有很好的支持，因为数据都是列式存储的，因此不需要进行内存赋值就可以直接进行类型转换，方法如下：\n\ndf_pq %&gt;% \n  to_duckdb() %&gt;% \n  filter(Factor == \"Atlanta\",Real &gt; 50) %&gt;% \n  group_by(Logical) %&gt;% \n  summarise(avg = mean(Integer)) %&gt;% \n  arrange(-avg) %&gt;% \n  collect()\n\n# A tibble: 3 × 2\n  Logical   avg\n  &lt;lgl&gt;   &lt;dbl&gt;\n1 NA       50.5\n2 TRUE     50.5\n3 FALSE    50.3\n\n\n在这种背景下，使用parquet对数据进行存储是非常诱人的，因为这样能够让我们轻松地使用dplyr函数来对存储在磁盘的数据进行操作。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>从内存到外存：用数据库管理数据</span>"
    ]
  },
  {
    "objectID": "从内存到外存：用数据库管理数据.html#基于polars的大数据处理方案",
    "href": "从内存到外存：用数据库管理数据.html#基于polars的大数据处理方案",
    "title": "12  从内存到外存：用数据库管理数据",
    "section": "12.4 基于Polars的大数据处理方案",
    "text": "12.4 基于Polars的大数据处理方案\nPolars 是一个高性能的数据框架库，专为数据操作和分析设计。它由 Rust 编写，确保了速度和内存安全，并利用并行处理来最大化性能。Polars 在处理大型数据集时表现出色，设计目的是最小化内存使用，使用高效的数据结构以减少开销。它提供了丰富的数据操作功能，如过滤、排序、聚合和连接等，支持链式操作，使得代码简洁且易读。总的来说，Polars 的特点包括：\n\n高性能：Polars利用并行计算和SIMD（单指令多数据）技术，在执行数据操作时大大提高了处理速度。它在处理大数据集时的性能优于许多传统的数据分析库。\n内存效率：Polars可以使用 Apache Arrow的内存格式（如Parquet），这使得它能够更有效地利用内存。其数据结构经过优化，可以处理更大的数据集，而不会消耗过多的内存资源。\n灵活的表达能力：Polars提供了一系列丰富的操作功能，包括数据选择、过滤、聚合、排序和连接等，支持链式调用，使得数据处理过程更为流畅。\n惰性计算：Polars 采用惰性执行策略，意味着只有在必要时才会执行计算。这种设计可以减少不必要的计算和内存开销，提升整体性能。\n跨平台支持：除了支持多种编程语言外，Polars还可以在不同的操作系统上运行，适用于各种开发环境。\n用户友好：Polars 的 API设计直观，易于学习和使用，适合各种数据分析任务。\n\n尽管核心是用 Rust 编写的，Polars 提供了 R 接口，因此可以在 R 中方便地使用。Polars 能处理复杂的数据查询和操作，包括时间序列数据、缺失值和类别数据等，未来可能还会支持更多编程语言。在R中要安装核心的Polars包，可以这样操作：\n\ninstall.packages(\"polars\", repos = \"https://community.r-multiverse.org\")\ninstall.packages(\n  'tidypolars', \n  repos = c('https://etiennebacher.r-universe.dev', getOption(\"repos\"))\n)\n\n以上代码会安装polars和tidypolars两个R包，前者负责在R中调用Rust所构建的Polars工具，后者则可以把常用的tidyverse代码（特别是dplyr和tidyr包中的函数）直接转译为Polars所支持的代码。下面我们对该工具进行简单的演示，首先我们生成一份数据集，并保存在根目录下的temp文件夹中：\n\nlibrary(pacman)\np_load(tidyfst,arrow)\n\n# 生成一亿行\nnr_of_rows &lt;- 1e8\n\n# 构造数据框\ndf &lt;- data.frame(\n  Logical = sample(c(TRUE, FALSE, NA), prob = c(0.85, 0.1, 0.05), nr_of_rows, replace = TRUE),\n  Integer = sample(1L:100L, nr_of_rows, replace = TRUE),\n  Real = sample(sample(1:10000, 20) / 100, nr_of_rows, replace = TRUE),\n  Factor = as.factor(sample(labels(UScitiesD), nr_of_rows, replace = TRUE))\n)\n\n# 检查大小\nobject_size(df) # 1.9 Gb\n\n# 导出parquet文件\narrow::write_parquet(df,\"temp/df.parquet\") # 209.1 Mb\n\n# 清除环境内的所有变量\nrm(list = ls())\n\n然后，我们利用polars包的scan_parquet方法把数据扫描到R环境中：\n\nlibrary(pacman)\np_load(polars,tidypolars,tidyverse,tidyfst)\n\n# 扫描数据\npl$scan_parquet(\"df.parquet\") -&gt; dat_pl\n\n需要注意的是，在上面的操作中，我们并没有把数据导入到环境里面。我们用了“扫描”一词，其实相当于对数据进行了连接，类似于我们在前一章节中提到的open_dataset操作。在这个背景下，我们可以对这个没有导入环境的数据进行各种操作，并把结果收集到环境中进行展示，操作方法如下：\n\n# 观察前6行\ndat_pl %&gt;% \n  head() %&gt;% \n  compute()\n\n# 看看总共有多少行\ndat_pl %&gt;% count() %&gt;% compute()\n\n# 分组汇总计算\npst(\n  dat_pl %&gt;% \n    group_by(Logical,Factor) %&gt;% \n    summarise(Real_mean = mean(Real),Real_sd = sd(Real),\n              median_Integer = median(Integer)) %&gt;% \n    compute() -&gt; res \n) # Finished in 3.920s elapsed (15.4s cpu)\n\n# 查看结果\nres\n\n# 把结果转化为R中的数据框\nres$to_data_frame()\n\n# 把结果转化为数据框并使用tibble形式进行展示\nres %&gt;% as_tibble()\n\n通过上面的试验，我们可以发现只需要把数据先存为Parquet格式，然后使用scan_parquet方法进行数据连接，就可以利用我们熟悉的dplyr和tidyr函数对保存在磁盘中的数据进行各式的数据操作，这给我们的大数据分析提供了巨大的便利，是解决内存不足计算（Out-of-Memory Computation）的最佳方案之一。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>从内存到外存：用数据库管理数据</span>"
    ]
  },
  {
    "objectID": "从内存到外存：用数据库管理数据.html#小结",
    "href": "从内存到外存：用数据库管理数据.html#小结",
    "title": "12  从内存到外存：用数据库管理数据",
    "section": "12.5 小结",
    "text": "12.5 小结\n本章介绍了如何在数据分析中有效地使用数据库管理数据，并讨论如何使用Arrow来对存在磁盘的数据进行高效处理。通过学习使用DBI包连接数据库、执行SQL查询，以及借助dbplyr包将dplyr代码翻译成SQL，我们能够在R中直接操作和查询数据库中的数据。这种方法不仅提高了数据处理的效率，还减少了对中间文件（如CSV）的依赖，避免了繁琐的数据导入导出步骤。此外，我们还学习了Apache Arrow和Polars， 它们所提供的Parquet内存格式减少了数据在不同系统间转换的开销，这对于需要处理大量数据的应用程序来说尤为重要。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>从内存到外存：用数据库管理数据</span>"
    ]
  },
  {
    "objectID": "从内存到外存：用数据库管理数据.html#练习",
    "href": "从内存到外存：用数据库管理数据.html#练习",
    "title": "12  从内存到外存：用数据库管理数据",
    "section": "12.6 练习",
    "text": "12.6 练习\n\n尝试使用duckdb方法构建一个数据库，然后实现所有数据库的日常操作，比如对某一列创建索引\n请比较一下是数据库操作快，还是使用Arrow/Polars对数据进行操作快，注意使用同样数据进行比较，同时对数据操作的时间和数据占据的内存进行比较。\n请比较一下，在内存允许的情况下，究竟是在内存中对数据进行处理快，还是使用磁盘进行数据处理速度快。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>从内存到外存：用数据库管理数据</span>"
    ]
  },
  {
    "objectID": "从本地到集群：大数据分布式计算.html",
    "href": "从本地到集群：大数据分布式计算.html",
    "title": "13  从本地到集群：大数据分布式计算",
    "section": "",
    "text": "13.1 分布式计算简介\n从本质上来讲，其实分布式计算是我们先前介绍的并行计算的一种特例（见图13.1），但是它的每一个任务不是由单个计算机的多进程或多核心进行处理，而是直接在多个机器中并发处理。分布式系统是一组电脑，透过网络相互连接传递消息并协调它们的行为而形成的系统，组件之间彼此进行交互以实现一个共同的目标。下面我们将会对分布式计算概念的各方面进行简要介绍。\nFigure 13.1: 分布式计算与并行计算的比较",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>从本地到集群：大数据分布式计算</span>"
    ]
  },
  {
    "objectID": "从本地到集群：大数据分布式计算.html#分布式计算简介",
    "href": "从本地到集群：大数据分布式计算.html#分布式计算简介",
    "title": "13  从本地到集群：大数据分布式计算",
    "section": "",
    "text": "13.1.1 基本原理\n分布式计算的基本原理是将任务分解为多个子任务，并将这些子任务分配给多个独立的计算节点进行并行处理。每个节点独立执行分配给它的任务，并将结果返回给一个集中管理的节点进行汇总。这种方式能够充分利用多台计算机的计算资源，达到加速计算和提高效率的目的。\n\n\n13.1.2 关键组成部分\n\n计算节点：分布式系统中的每个独立计算单元。节点可以是物理计算机或虚拟机。\n任务调度器：负责将任务分解并分配给不同计算节点的系统。任务调度器还负责监控任务的执行情况，处理节点故障和任务重试。\n数据存储：用于存储分布式系统中的数据。常见的分布式数据存储系统包括HDFS和Apache Cassandra等。\n通信机制：节点之间通过网络进行通信，交换数据和状态信息。常用的通信协议包括TCP/IP、RPC（远程过程调用）等。\n\n\n\n13.1.3 特性\n\n扩展性：分布式系统可以通过增加更多的计算节点来提升整体性能和处理能力，几乎可以无限制地扩展。\n容错性：分布式系统可以通过冗余和备份机制实现故障容错，即使某个节点出现故障，系统仍然能够继续运行。\n资源利用率高：能够充分利用分布在不同位置的计算资源，提高资源利用效率。\n\n\n\n13.1.4 应用场景\n\n大数据处理：分布式计算广泛应用于大数据分析、数据挖掘和数据清洗等领域。例如，电商平台可以使用分布式计算分析用户行为数据，优化推荐系统。\n科学计算：在基因组学、气象预测、天体物理等需要大量计算资源的科学研究中，分布式计算能够显著加速数据处理和模拟仿真过程。\n金融分析：金融机构使用分布式计算进行实时交易分析、风险管理和市场预测，以应对海量数据和高频交易的需求。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>从本地到集群：大数据分布式计算</span>"
    ]
  },
  {
    "objectID": "从本地到集群：大数据分布式计算.html#分布式工具概览",
    "href": "从本地到集群：大数据分布式计算.html#分布式工具概览",
    "title": "13  从本地到集群：大数据分布式计算",
    "section": "13.2 分布式工具概览",
    "text": "13.2 分布式工具概览\n在大数据处理的过程中，选择合适的分布式计算框架至关重要。不同的框架在架构、功能和应用场景上各具特色，为用户提供了多样化的选择。以下将详细介绍几种常见的分布式计算框架，包括 Apache Hadoop、Apache Spark 和 Apache Flink（图标见图13.2），解析它们的独特优势及应用场景。\n\n\n\n\n\n\n\n\nFigure 13.2: Hadoop、Spark与Flink的标志性图标\n\n\n\n\n\nApache Hadoop 是一个开源的分布式计算框架，被广泛用于处理和存储大规模数据集。Hadoop 的核心组件包括 HDFS (Hadoop Distributed File System) 和 MapReduce。HDFS 是一个分布式文件系统，旨在以高吞吐量和高容错性存储大量数据，通过将数据分成块并分布在集群中的不同节点上来实现数据的冗余和高可用性。MapReduce 则是一种编程模型，用于并行处理大规模数据集，通过将任务分解为“Map”阶段和“Reduce”阶段，使计算能够分布在多个节点上进行。Hadoop 的优势在于其处理大规模数据的能力和高容错性，但其批处理模式在处理实时数据时表现不佳，且任务调度和管理相对复杂。\nApache Spark 是一个快速、通用的分布式计算引擎，支持批处理、流处理和机器学习等多种任务类型。Spark 的内存计算能力使其比 Hadoop 更加快速，特别是在迭代计算任务（如机器学习算法）中表现尤为突出。Spark 提供了丰富的编程接口，包括 Spark Core、Spark SQL、Spark Streaming、MLlib（机器学习库）和 GraphX（图计算库），使得用户可以方便地实现复杂的数据处理和分析任务。Spark 具有很好的扩展性，能够处理从单机到数千节点的大规模集群。Spark 的优势在于其高效的内存计算和广泛的应用场景，但其挑战主要在于对内存的高需求以及需要优化内存管理和任务调度。\nApache Flink 是一个专注于流处理的分布式计算框架，特别适合实时数据处理和复杂事件处理。与 Spark 的微批处理模式不同，Flink 采用了真正的流处理模式，能够实时处理数据流，并提供低延迟的计算结果。Flink 提供强大的状态管理功能，能够高效地管理和恢复计算状态，确保流处理任务的准确性和可靠性。此外，Flink 提供了多种 API，包括 DataStream API、DataSet API 和 Table API，支持流处理和批处理任务的统一编程模型。Flink 的优势在于其强大的流处理能力和状态管理机制，适合实时数据分析和复杂事件处理，但其挑战在于相对复杂的编程模型和高资源需求。\n不同的分布式计算框架各有优劣，选择时应根据具体的应用场景和需求进行评估和选型。在实际应用中，可能需要结合多种技术框架，以最佳方式处理和分析大规模数据。Hadoop 适合处理大规模批处理任务和存储大量数据，Spark 适合多种数据处理任务并在需要高效迭代计算的场景中表现优异，而 Flink 则专注于实时流处理，非常适合实时数据分析和复杂事件处理。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>从本地到集群：大数据分布式计算</span>"
    ]
  },
  {
    "objectID": "从本地到集群：大数据分布式计算.html#基于sparklyr的实现",
    "href": "从本地到集群：大数据分布式计算.html#基于sparklyr的实现",
    "title": "13  从本地到集群：大数据分布式计算",
    "section": "13.3 基于sparklyr的实现",
    "text": "13.3 基于sparklyr的实现\nsparklyr是专为 R 语言设计的包，旨在通过直观的接口简化与 Apache Spark 的集成。它允许用户使用熟悉的 dplyr 语法操作 Spark 数据帧，从而进行高效的数据处理和分析，支持大规模数据集和复杂计算任务，并提供与 Spark SQL、机器学习库的深度集成，是处理大数据的强大工具。本部分将会介绍如何使用该工具来进行分布式计算，不过读者只需要在自己的个人计算机上就可以运行这些代码并进行实现。当你具有Spark集群条件的时候，只需要在配置连接的代码上稍作修改，其他代码就可以原封不动地对大规模数据进行分布式操作。\n\n13.3.1 配置\n首先，我们需要加载必要的包：\n\nlibrary(pacman)\np_load(sparklyr,tidyverse)\n\n此外，由于因为 Spark 是使用 Scala 编程语言构建的，而 Scala 运行在 Java 虚拟机（JVM）上，因此您还需要在系统上安装 Java 8。如果尚未安装，可以到Java官网进行下载。如果已经安装了，可以运行以下代码进行验证：\n\nsystem(\"java -version\")\n\n然后，我们必须在计算机上安装Spark，sparklyr提供了便捷的安装方法：\n\nspark_install()\n\n事实上我们可以可以手动进行解压、安装和路径配置，感兴趣的读者可以键入?spark_install进行查阅。\n\n\n13.3.2 连接\n对数据的连接，可以使用spark_connect函数进行实现，这里我们使用本地连接：\n\nsc &lt;- spark_connect(master = \"local\")\n\n如果需要关闭连接，可以使用spark_disconnect函数：\n\nspark_disconnect(sc)\n\n关于连接到其他集群的各种操作，可以参考官方的配置文档进行查阅。\n\n\n13.3.3 读取\n把数据导入到Spark数据库中有两种方法，一种方法是利用copy_to函数从R环境中导入：\n\ntbl_mtcars &lt;- copy_to(sc, mtcars, \"spark_mtcars\")\n\ntbl_mtcars\n#&gt; # Source: spark&lt;spark_mtcars&gt; [?? x 11]\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160    110  3.9   2.62  16.5     0     1\n#&gt;  2  21       6  160    110  3.9   2.88  17.0     0     1\n#&gt;  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#&gt;  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#&gt;  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#&gt;  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#&gt;  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#&gt;  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#&gt;  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#&gt; 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#&gt; # … with more rows, and 2 more variables: gear &lt;dbl&gt;,\n#&gt; #   carb &lt;dbl&gt;\n\n另一种方法则是从本地的文件直接进行导入，大致形式如下：\n\nspark_read_csv(sc, name = \"test_table\",  path = \"/data/test.csv\")\n\nsparklyr支持直接读入的文件包括CSV、JSON、Parquet等。此外，不仅仅可以对这些格式的文件进行读入，还可以写出不同格式的文件，更多的信息可以参考相关的帮助文档（https://spark.posit.co/packages/sparklyr/latest/reference/index.html#spark-data）。\n\n\n13.3.4 清洗\n在sparklyr中，我们可以直接使用dplyr命令来对数据进行操作：\n\ntbl_mtcars %&gt;% \n  group_by(vs,am) %&gt;% \n  summarise(\n    count = n(),\n    hp = mean(hp,na.rm = TRUE),\n    drat = mean(drat,na.rm = TRUE)\n  )\n# # Source:   SQL [4 x 5]\n# # Database: spark_connection\n# # Groups:   vs\n#      vs    am count    hp  drat\n#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n# 1     1     1     7  80.6  4.15\n# 2     0     1     6 181.   3.94\n# 3     1     0     7 102.   3.57\n# 4     0     0    12 194.   3.12\n\n在底层，其实这些代码都转化为了SQL语句，如果需要观察，可以使用show_query函数，这与我们之前介绍的数据库操作是一致的：\n\ntbl_mtcars %&gt;% \n  group_by(vs,am) %&gt;% \n  summarise(\n    count = n(),\n    hp = mean(hp,na.rm = TRUE),\n    drat = mean(drat,na.rm = TRUE)\n  ) %&gt;% \n  show_query()\n# &lt;SQL&gt;\n# SELECT `vs`, `am`, COUNT(*) AS `count`, AVG(`hp`) AS `hp`, AVG(`drat`) AS `drat`\n# FROM `spark_mtcars`\n# GROUP BY `vs`, `am`\n\n事实上，我们也可以直接使用SQL语句从中调取数据：\n\np_load(DBI)\n\ndbGetQuery(sc,\"SELECT vs, am, hp FROM spark_mtcars LIMIT 5\")\n#   vs am  hp\n# 1  0  1 110\n# 2  0  1 110\n# 3  1  1  93\n# 4  1  0 110\n# 5  0  0 175\n\n本部分与数据库操作基本一致，因此不再进行赘述。\n\n\n13.3.5 建模\n在R中对处于Spark集群的数据进行分析，都是依赖把R命令转化为Spark能够理解的指令来实现的，sparklyr把这些指令转化为见名知意的R代码，下面我们来简单进行展示：\n\n# 只选取其中3列，并划分训练集（70%）和测试集（30%）\npartitions &lt;- mtcars_tbl %&gt;%\n  select(mpg, wt, cyl) %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3)\n\n# 利用训练集拟合线性模型\nfit &lt;- partitions$training %&gt;%\n  ml_linear_regression(mpg ~ .)\n\n# 利用模型对测试集进行预测\npred &lt;- ml_predict(fit, partitions$test)\n\n我们可以观察到，这里的sdf_random_split和ml_predict都是sparklyr提供的机器学习函数，能够协助我们对集群中的数据进行类似在R中开展的机器学习操作。关于更多在Spark中实现机器学习的操作，可以参考官方文档（https://spark.posit.co/guides/mlib.html）。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>从本地到集群：大数据分布式计算</span>"
    ]
  },
  {
    "objectID": "从本地到集群：大数据分布式计算.html#小结",
    "href": "从本地到集群：大数据分布式计算.html#小结",
    "title": "13  从本地到集群：大数据分布式计算",
    "section": "13.4 小结",
    "text": "13.4 小结\n本章我们对分布式计算进行了了解，并学会如何使用sparklyr包来在R中对分布式计算进行实现。我们所介绍的功能只是冰山一角，更多的内容可以参考sparklyr的官方指南。我们相信，随着前沿工具的蓬勃发展，分布式计算的实现工具会越来越便利，这意味着用户在不清楚底层逻辑的前提下就能够自由地对数据进行调度、处理和建模。正因为有这样的愿景，因此本章没有对更多的技术细节进行深究。相信通过对基本概念的了解，未来读者将会用到更加便捷的工具，从而在应对大规模数据集的时候更加从容地利用分布式计算方法开展分析。",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>从本地到集群：大数据分布式计算</span>"
    ]
  },
  {
    "objectID": "参考资料.html",
    "href": "参考资料.html",
    "title": "参考资料",
    "section": "",
    "text": "Matter, Ulrich. Big data analytics: a guide to data science practitioners making the transition to big data. CRC Press, 2023.\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for data science. O’Reilly Media, 2023.\nLuraschi, Javier, Kevin Kuo, and Edgar Ruiz. Mastering Spark with R: the complete guide to large-scale analysis and modeling. O’Reilly Media, 2019.\n刘艺非. R语言高效能实战:更多数据和更快速度. 人民邮电出版社, 2022.\nLarger-Than-Memory Data Workflows with Apache Arrow\nApache Arrow R Cookbook\nR interface to Apache Spark\nAn Introduction to Polars from R\ntidypolars",
    "crumbs": [
      "参考资料"
    ]
  }
]